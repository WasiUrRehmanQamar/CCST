{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Use GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:11:02.583484Z",
     "iopub.status.busy": "2025-10-14T06:11:02.583113Z",
     "iopub.status.idle": "2025-10-14T06:11:02.589554Z",
     "shell.execute_reply": "2025-10-14T06:11:02.588139Z",
     "shell.execute_reply.started": "2025-10-14T06:11:02.583457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hide Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:11:02.591467Z",
     "iopub.status.busy": "2025-10-14T06:11:02.591117Z",
     "iopub.status.idle": "2025-10-14T06:11:02.607714Z",
     "shell.execute_reply": "2025-10-14T06:11:02.606607Z",
     "shell.execute_reply.started": "2025-10-14T06:11:02.591442Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-14T06:11:02.610221Z",
     "iopub.status.busy": "2025-10-14T06:11:02.609858Z",
     "iopub.status.idle": "2025-10-14T06:11:06.651519Z",
     "shell.execute_reply": "2025-10-14T06:11:06.650151Z",
     "shell.execute_reply.started": "2025-10-14T06:11:02.610184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio torchinfo einops scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:11:06.653234Z",
     "iopub.status.busy": "2025-10-14T06:11:06.652905Z",
     "iopub.status.idle": "2025-10-14T06:11:11.650252Z",
     "shell.execute_reply": "2025-10-14T06:11:11.649073Z",
     "shell.execute_reply.started": "2025-10-14T06:11:06.653205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
      "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (3.7.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (4.67.1)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (1.8.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (4.4.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (25.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne==1.0.0) (3.1.6)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne==1.0.0) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne==1.0.0) (2.32.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne==1.0.0) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (1.4.8)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (11.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mne==1.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mne==1.0.0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U mne==1.0.0 scipy==1.13.1 numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:11:11.652711Z",
     "iopub.status.busy": "2025-10-14T06:11:11.652378Z",
     "iopub.status.idle": "2025-10-14T06:11:11.659970Z",
     "shell.execute_reply": "2025-10-14T06:11:11.659092Z",
     "shell.execute_reply.started": "2025-10-14T06:11:11.652685Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import LeaveOneGroupOut, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 133 samples (1.663 s)\n",
      "\n",
      "\n",
      "####################\n",
      "Traininig Started\n",
      "####################\n",
      "\n",
      "===== Fold 1\n",
      "===== Seed : 43\n",
      "===== Training Subject : 2, 3, 4, 5, 6, 7, 8, 9\n",
      "===== Test Subject : 1\n",
      "\n",
      "Epoch 1/100 | Training Loss : 1.4136 | Training Accuracy : 29.28 % | Validation Loss : 1.3783 | Validation Accuracy : 28.14 %\n",
      "Epoch 2/100 | Training Loss : 1.3759 | Training Accuracy : 30.54 % | Validation Loss : 1.3292 | Validation Accuracy : 35.93 %\n",
      "Epoch 3/100 | Training Loss : 1.3524 | Training Accuracy : 32.66 % | Validation Loss : 1.3117 | Validation Accuracy : 36.36 %\n",
      "Epoch 4/100 | Training Loss : 1.3031 | Training Accuracy : 37.48 % | Validation Loss : 1.2556 | Validation Accuracy : 40.69 %\n",
      "Epoch 5/100 | Training Loss : 1.2580 | Training Accuracy : 40.52 % | Validation Loss : 1.2292 | Validation Accuracy : 41.99 %\n",
      "Epoch 6/100 | Training Loss : 1.2552 | Training Accuracy : 41.39 % | Validation Loss : 1.2362 | Validation Accuracy : 43.72 %\n",
      "Epoch 7/100 | Training Loss : 1.2177 | Training Accuracy : 43.90 % | Validation Loss : 1.2620 | Validation Accuracy : 39.39 %\n",
      "Epoch 8/100 | Training Loss : 1.2033 | Training Accuracy : 42.45 % | Validation Loss : 1.1780 | Validation Accuracy : 44.59 %\n",
      "Epoch 9/100 | Training Loss : 1.1939 | Training Accuracy : 45.54 % | Validation Loss : 1.1857 | Validation Accuracy : 43.72 %\n",
      "Epoch 10/100 | Training Loss : 1.1812 | Training Accuracy : 46.07 % | Validation Loss : 1.1223 | Validation Accuracy : 45.02 %\n",
      "Epoch 11/100 | Training Loss : 1.1499 | Training Accuracy : 46.02 % | Validation Loss : 1.1444 | Validation Accuracy : 47.62 %\n",
      "Epoch 12/100 | Training Loss : 1.1497 | Training Accuracy : 46.60 % | Validation Loss : 1.1571 | Validation Accuracy : 46.75 %\n",
      "Epoch 13/100 | Training Loss : 1.1530 | Training Accuracy : 47.03 % | Validation Loss : 1.1436 | Validation Accuracy : 44.16 %\n",
      "Epoch 14/100 | Training Loss : 1.1407 | Training Accuracy : 47.52 % | Validation Loss : 1.1421 | Validation Accuracy : 51.52 %\n",
      "Epoch 15/100 | Training Loss : 1.1360 | Training Accuracy : 47.76 % | Validation Loss : 1.2271 | Validation Accuracy : 41.99 %\n",
      "Epoch 16/100 | Training Loss : 1.1373 | Training Accuracy : 48.48 % | Validation Loss : 1.1370 | Validation Accuracy : 49.35 %\n",
      "Epoch 17/100 | Training Loss : 1.1039 | Training Accuracy : 49.40 % | Validation Loss : 1.0851 | Validation Accuracy : 48.92 %\n",
      "Epoch 18/100 | Training Loss : 1.0735 | Training Accuracy : 51.57 % | Validation Loss : 1.0583 | Validation Accuracy : 51.52 %\n",
      "Epoch 19/100 | Training Loss : 1.0769 | Training Accuracy : 52.19 % | Validation Loss : 1.0923 | Validation Accuracy : 48.48 %\n",
      "Epoch 20/100 | Training Loss : 1.0735 | Training Accuracy : 51.33 % | Validation Loss : 1.0540 | Validation Accuracy : 55.41 %\n",
      "Epoch 21/100 | Training Loss : 1.0824 | Training Accuracy : 52.19 % | Validation Loss : 1.0790 | Validation Accuracy : 50.65 %\n",
      "Epoch 22/100 | Training Loss : 1.0712 | Training Accuracy : 52.48 % | Validation Loss : 1.0610 | Validation Accuracy : 51.95 %\n",
      "Epoch 23/100 | Training Loss : 1.0625 | Training Accuracy : 51.76 % | Validation Loss : 1.0550 | Validation Accuracy : 52.38 %\n",
      "Epoch 24/100 | Training Loss : 1.0578 | Training Accuracy : 52.29 % | Validation Loss : 1.0846 | Validation Accuracy : 49.78 %\n",
      "Epoch 25/100 | Training Loss : 1.0715 | Training Accuracy : 51.52 % | Validation Loss : 1.1080 | Validation Accuracy : 53.25 %\n",
      "Epoch 26/100 | Training Loss : 1.0619 | Training Accuracy : 51.91 % | Validation Loss : 1.0552 | Validation Accuracy : 52.38 %\n",
      "Epoch 27/100 | Training Loss : 1.0449 | Training Accuracy : 54.41 % | Validation Loss : 1.0479 | Validation Accuracy : 53.25 %\n",
      "Epoch 28/100 | Training Loss : 1.0236 | Training Accuracy : 54.99 % | Validation Loss : 1.0300 | Validation Accuracy : 52.38 %\n",
      "Epoch 29/100 | Training Loss : 1.0097 | Training Accuracy : 55.96 % | Validation Loss : 1.0309 | Validation Accuracy : 54.11 %\n",
      "Epoch 30/100 | Training Loss : 1.0187 | Training Accuracy : 54.90 % | Validation Loss : 1.0148 | Validation Accuracy : 56.28 %\n",
      "Epoch 31/100 | Training Loss : 1.0327 | Training Accuracy : 53.98 % | Validation Loss : 1.0352 | Validation Accuracy : 54.11 %\n",
      "Epoch 32/100 | Training Loss : 1.0256 | Training Accuracy : 54.85 % | Validation Loss : 1.0311 | Validation Accuracy : 54.11 %\n",
      "Epoch 33/100 | Training Loss : 1.0237 | Training Accuracy : 56.30 % | Validation Loss : 1.0405 | Validation Accuracy : 51.08 %\n",
      "Epoch 34/100 | Training Loss : 1.0156 | Training Accuracy : 54.61 % | Validation Loss : 1.0677 | Validation Accuracy : 45.02 %\n",
      "Epoch 35/100 | Training Loss : 1.0039 | Training Accuracy : 55.67 % | Validation Loss : 1.0197 | Validation Accuracy : 51.95 %\n",
      "Epoch 36/100 | Training Loss : 0.9970 | Training Accuracy : 57.98 % | Validation Loss : 1.0386 | Validation Accuracy : 52.38 %\n",
      "Epoch 37/100 | Training Loss : 0.9920 | Training Accuracy : 56.15 % | Validation Loss : 1.0343 | Validation Accuracy : 49.35 %\n",
      "Epoch 38/100 | Training Loss : 1.0055 | Training Accuracy : 55.52 % | Validation Loss : 1.0387 | Validation Accuracy : 52.38 %\n",
      "Epoch 39/100 | Training Loss : 0.9831 | Training Accuracy : 57.94 % | Validation Loss : 1.0178 | Validation Accuracy : 54.11 %\n",
      "Epoch 40/100 | Training Loss : 0.9923 | Training Accuracy : 56.49 % | Validation Loss : 1.0283 | Validation Accuracy : 52.38 %\n",
      "Epoch 41/100 | Training Loss : 0.9785 | Training Accuracy : 58.03 % | Validation Loss : 1.0168 | Validation Accuracy : 51.52 %\n",
      "Epoch 42/100 | Training Loss : 0.9853 | Training Accuracy : 56.92 % | Validation Loss : 1.0067 | Validation Accuracy : 58.01 %\n",
      "Epoch 43/100 | Training Loss : 1.0006 | Training Accuracy : 56.78 % | Validation Loss : 1.0327 | Validation Accuracy : 53.68 %\n",
      "Epoch 44/100 | Training Loss : 0.9814 | Training Accuracy : 57.12 % | Validation Loss : 1.0397 | Validation Accuracy : 49.78 %\n",
      "Epoch 45/100 | Training Loss : 0.9752 | Training Accuracy : 57.94 % | Validation Loss : 1.0121 | Validation Accuracy : 52.81 %\n",
      "Epoch 46/100 | Training Loss : 0.9846 | Training Accuracy : 56.49 % | Validation Loss : 1.0199 | Validation Accuracy : 53.25 %\n",
      "Epoch 47/100 | Training Loss : 0.9814 | Training Accuracy : 56.44 % | Validation Loss : 1.0311 | Validation Accuracy : 53.25 %\n",
      "Epoch 48/100 | Training Loss : 0.9628 | Training Accuracy : 57.89 % | Validation Loss : 1.0300 | Validation Accuracy : 51.95 %\n",
      "Epoch 49/100 | Training Loss : 0.9840 | Training Accuracy : 57.36 % | Validation Loss : 1.0232 | Validation Accuracy : 52.81 %\n",
      "Epoch 50/100 | Training Loss : 0.9760 | Training Accuracy : 57.69 % | Validation Loss : 1.0205 | Validation Accuracy : 52.38 %\n",
      "Epoch 51/100 | Training Loss : 0.9479 | Training Accuracy : 59.82 % | Validation Loss : 1.0149 | Validation Accuracy : 54.11 %\n",
      "Epoch 52/100 | Training Loss : 0.9714 | Training Accuracy : 57.55 % | Validation Loss : 1.0242 | Validation Accuracy : 52.38 %\n",
      "Epoch 53/100 | Training Loss : 0.9676 | Training Accuracy : 58.80 % | Validation Loss : 1.0164 | Validation Accuracy : 53.25 %\n",
      "Epoch 54/100 | Training Loss : 0.9505 | Training Accuracy : 58.90 % | Validation Loss : 1.0183 | Validation Accuracy : 52.81 %\n",
      "Epoch 55/100 | Training Loss : 0.9572 | Training Accuracy : 59.43 % | Validation Loss : 1.0090 | Validation Accuracy : 54.11 %\n",
      "Epoch 56/100 | Training Loss : 0.9580 | Training Accuracy : 59.72 % | Validation Loss : 1.0105 | Validation Accuracy : 53.68 %\n",
      "Epoch 57/100 | Training Loss : 0.9549 | Training Accuracy : 60.15 % | Validation Loss : 1.0085 | Validation Accuracy : 53.68 %\n",
      "Epoch 58/100 | Training Loss : 0.9700 | Training Accuracy : 58.13 % | Validation Loss : 1.0096 | Validation Accuracy : 53.68 %\n",
      "Epoch 59/100 | Training Loss : 0.9455 | Training Accuracy : 59.38 % | Validation Loss : 1.0093 | Validation Accuracy : 54.11 %\n",
      "Epoch 60/100 | Training Loss : 0.9644 | Training Accuracy : 58.51 % | Validation Loss : 1.0145 | Validation Accuracy : 53.25 %\n",
      "Epoch 61/100 | Training Loss : 0.9699 | Training Accuracy : 58.18 % | Validation Loss : 1.0139 | Validation Accuracy : 52.81 %\n",
      "Epoch 62/100 | Training Loss : 0.9551 | Training Accuracy : 58.66 % | Validation Loss : 1.0079 | Validation Accuracy : 54.11 %\n",
      "Epoch 63/100 | Training Loss : 0.9566 | Training Accuracy : 59.14 % | Validation Loss : 1.0089 | Validation Accuracy : 54.11 %\n",
      "Epoch 64/100 | Training Loss : 0.9485 | Training Accuracy : 59.38 % | Validation Loss : 1.0024 | Validation Accuracy : 55.84 %\n",
      "Epoch 65/100 | Training Loss : 0.9665 | Training Accuracy : 58.90 % | Validation Loss : 1.0076 | Validation Accuracy : 52.38 %\n",
      "Epoch 66/100 | Training Loss : 0.9688 | Training Accuracy : 58.90 % | Validation Loss : 1.0097 | Validation Accuracy : 54.11 %\n",
      "Epoch 67/100 | Training Loss : 0.9696 | Training Accuracy : 58.76 % | Validation Loss : 1.0095 | Validation Accuracy : 53.68 %\n",
      "Epoch 68/100 | Training Loss : 0.9397 | Training Accuracy : 59.48 % | Validation Loss : 1.0106 | Validation Accuracy : 52.81 %\n",
      "Epoch 69/100 | Training Loss : 0.9576 | Training Accuracy : 60.06 % | Validation Loss : 1.0048 | Validation Accuracy : 54.98 %\n",
      "Epoch 70/100 | Training Loss : 0.9526 | Training Accuracy : 60.01 % | Validation Loss : 1.0070 | Validation Accuracy : 55.41 %\n",
      "Epoch 71/100 | Training Loss : 0.9569 | Training Accuracy : 57.84 % | Validation Loss : 1.0067 | Validation Accuracy : 53.25 %\n",
      "Epoch 72/100 | Training Loss : 0.9617 | Training Accuracy : 58.90 % | Validation Loss : 1.0023 | Validation Accuracy : 55.84 %\n",
      "Epoch 73/100 | Training Loss : 0.9521 | Training Accuracy : 59.62 % | Validation Loss : 1.0013 | Validation Accuracy : 54.98 %\n",
      "Epoch 74/100 | Training Loss : 0.9477 | Training Accuracy : 59.48 % | Validation Loss : 1.0058 | Validation Accuracy : 53.68 %\n",
      "Epoch 75/100 | Training Loss : 0.9572 | Training Accuracy : 58.76 % | Validation Loss : 1.0045 | Validation Accuracy : 54.11 %\n",
      "Epoch 76/100 | Training Loss : 0.9426 | Training Accuracy : 59.96 % | Validation Loss : 1.0085 | Validation Accuracy : 53.68 %\n",
      "Epoch 77/100 | Training Loss : 0.9647 | Training Accuracy : 57.55 % | Validation Loss : 1.0051 | Validation Accuracy : 54.11 %\n",
      "Epoch 78/100 | Training Loss : 0.9512 | Training Accuracy : 58.37 % | Validation Loss : 1.0035 | Validation Accuracy : 54.98 %\n",
      "Epoch 79/100 | Training Loss : 0.9609 | Training Accuracy : 59.58 % | Validation Loss : 1.0088 | Validation Accuracy : 54.11 %\n",
      "Epoch 80/100 | Training Loss : 0.9476 | Training Accuracy : 60.59 % | Validation Loss : 1.0086 | Validation Accuracy : 54.11 %\n",
      "Epoch 81/100 | Training Loss : 0.9370 | Training Accuracy : 59.62 % | Validation Loss : 1.0026 | Validation Accuracy : 54.98 %\n",
      "Epoch 82/100 | Training Loss : 0.9575 | Training Accuracy : 57.79 % | Validation Loss : 1.0113 | Validation Accuracy : 53.25 %\n",
      "Epoch 83/100 | Training Loss : 0.9349 | Training Accuracy : 60.73 % | Validation Loss : 1.0036 | Validation Accuracy : 55.41 %\n",
      "Epoch 84/100 | Training Loss : 0.9570 | Training Accuracy : 58.18 % | Validation Loss : 1.0078 | Validation Accuracy : 53.25 %\n",
      "Epoch 85/100 | Training Loss : 0.9519 | Training Accuracy : 60.11 % | Validation Loss : 1.0034 | Validation Accuracy : 54.98 %\n",
      "Epoch 86/100 | Training Loss : 0.9739 | Training Accuracy : 58.22 % | Validation Loss : 1.0011 | Validation Accuracy : 55.41 %\n",
      "Epoch 87/100 | Training Loss : 0.9580 | Training Accuracy : 59.14 % | Validation Loss : 1.0058 | Validation Accuracy : 54.11 %\n",
      "Epoch 88/100 | Training Loss : 0.9671 | Training Accuracy : 59.04 % | Validation Loss : 1.0126 | Validation Accuracy : 51.95 %\n",
      "Epoch 89/100 | Training Loss : 0.9382 | Training Accuracy : 60.40 % | Validation Loss : 1.0085 | Validation Accuracy : 53.25 %\n",
      "Epoch 90/100 | Training Loss : 0.9688 | Training Accuracy : 58.13 % | Validation Loss : 1.0084 | Validation Accuracy : 52.81 %\n",
      "Epoch 91/100 | Training Loss : 0.9570 | Training Accuracy : 59.14 % | Validation Loss : 1.0123 | Validation Accuracy : 52.38 %\n",
      "Epoch 92/100 | Training Loss : 0.9584 | Training Accuracy : 58.42 % | Validation Loss : 1.0074 | Validation Accuracy : 54.55 %\n",
      "Epoch 93/100 | Training Loss : 0.9347 | Training Accuracy : 60.30 % | Validation Loss : 1.0062 | Validation Accuracy : 54.11 %\n",
      "Epoch 94/100 | Training Loss : 0.9699 | Training Accuracy : 58.18 % | Validation Loss : 1.0099 | Validation Accuracy : 53.25 %\n",
      "Epoch 95/100 | Training Loss : 0.9573 | Training Accuracy : 59.86 % | Validation Loss : 1.0060 | Validation Accuracy : 53.68 %\n",
      "Epoch 96/100 | Training Loss : 0.9428 | Training Accuracy : 59.86 % | Validation Loss : 1.0101 | Validation Accuracy : 54.11 %\n",
      "Epoch 97/100 | Training Loss : 0.9525 | Training Accuracy : 59.38 % | Validation Loss : 1.0070 | Validation Accuracy : 54.98 %\n",
      "Epoch 98/100 | Training Loss : 0.9471 | Training Accuracy : 59.43 % | Validation Loss : 1.0074 | Validation Accuracy : 54.55 %\n",
      "Epoch 99/100 | Training Loss : 0.9583 | Training Accuracy : 57.60 % | Validation Loss : 1.0125 | Validation Accuracy : 52.38 %\n",
      "Epoch 100/100 | Training Loss : 0.9467 | Training Accuracy : 60.20 % | Validation Loss : 1.0052 | Validation Accuracy : 53.25 %\n",
      "Test Subject : 1 | Test Loss : 1.0644 | Test Accuracy : 56.94 %\n",
      "\n",
      "################################\n",
      "\n",
      "===== Fold 2\n",
      "===== Seed : 44\n",
      "===== Training Subject : 1, 3, 4, 5, 6, 7, 8, 9\n",
      "===== Test Subject : 2\n",
      "\n",
      "Epoch 1/100 | Training Loss : 1.0462 | Training Accuracy : 55.57 % | Validation Loss : 1.0076 | Validation Accuracy : 55.84 %\n",
      "Epoch 2/100 | Training Loss : 0.9884 | Training Accuracy : 57.65 % | Validation Loss : 0.9082 | Validation Accuracy : 57.58 %\n",
      "Epoch 3/100 | Training Loss : 0.9772 | Training Accuracy : 59.33 % | Validation Loss : 0.9957 | Validation Accuracy : 57.14 %\n",
      "Epoch 4/100 | Training Loss : 0.9751 | Training Accuracy : 58.80 % | Validation Loss : 0.9379 | Validation Accuracy : 55.84 %\n",
      "Epoch 5/100 | Training Loss : 0.9696 | Training Accuracy : 58.76 % | Validation Loss : 0.9154 | Validation Accuracy : 60.61 %\n",
      "Epoch 6/100 | Training Loss : 0.9444 | Training Accuracy : 58.66 % | Validation Loss : 0.9800 | Validation Accuracy : 58.01 %\n",
      "Epoch 7/100 | Training Loss : 0.9716 | Training Accuracy : 57.74 % | Validation Loss : 1.0537 | Validation Accuracy : 53.68 %\n",
      "Epoch 8/100 | Training Loss : 0.9543 | Training Accuracy : 59.24 % | Validation Loss : 0.9093 | Validation Accuracy : 58.87 %\n",
      "Epoch 9/100 | Training Loss : 0.9125 | Training Accuracy : 62.13 % | Validation Loss : 0.8813 | Validation Accuracy : 61.47 %\n",
      "Epoch 10/100 | Training Loss : 0.9161 | Training Accuracy : 61.41 % | Validation Loss : 0.8893 | Validation Accuracy : 64.07 %\n",
      "Epoch 11/100 | Training Loss : 0.8858 | Training Accuracy : 62.42 % | Validation Loss : 0.9090 | Validation Accuracy : 61.90 %\n",
      "Epoch 12/100 | Training Loss : 0.8923 | Training Accuracy : 62.57 % | Validation Loss : 0.8795 | Validation Accuracy : 64.50 %\n",
      "Epoch 13/100 | Training Loss : 0.8746 | Training Accuracy : 62.42 % | Validation Loss : 0.8973 | Validation Accuracy : 61.47 %\n",
      "Epoch 14/100 | Training Loss : 0.8991 | Training Accuracy : 61.31 % | Validation Loss : 0.8937 | Validation Accuracy : 59.74 %\n",
      "Epoch 15/100 | Training Loss : 0.8709 | Training Accuracy : 63.39 % | Validation Loss : 0.8720 | Validation Accuracy : 65.37 %\n",
      "Epoch 16/100 | Training Loss : 0.8898 | Training Accuracy : 62.71 % | Validation Loss : 0.8936 | Validation Accuracy : 61.04 %\n",
      "Epoch 17/100 | Training Loss : 0.8760 | Training Accuracy : 63.29 % | Validation Loss : 0.8881 | Validation Accuracy : 58.87 %\n",
      "Epoch 18/100 | Training Loss : 0.8543 | Training Accuracy : 64.11 % | Validation Loss : 0.8749 | Validation Accuracy : 62.34 %\n",
      "Epoch 19/100 | Training Loss : 0.8487 | Training Accuracy : 64.21 % | Validation Loss : 0.8826 | Validation Accuracy : 60.17 %\n",
      "Epoch 20/100 | Training Loss : 0.8574 | Training Accuracy : 64.50 % | Validation Loss : 0.8743 | Validation Accuracy : 61.90 %\n",
      "Epoch 21/100 | Training Loss : 0.8387 | Training Accuracy : 64.06 % | Validation Loss : 0.8955 | Validation Accuracy : 59.74 %\n",
      "Epoch 22/100 | Training Loss : 0.8175 | Training Accuracy : 65.32 % | Validation Loss : 0.8549 | Validation Accuracy : 62.77 %\n",
      "Epoch 23/100 | Training Loss : 0.8044 | Training Accuracy : 67.49 % | Validation Loss : 0.8536 | Validation Accuracy : 63.20 %\n",
      "Epoch 24/100 | Training Loss : 0.8080 | Training Accuracy : 66.33 % | Validation Loss : 0.8483 | Validation Accuracy : 63.64 %\n",
      "Epoch 25/100 | Training Loss : 0.8161 | Training Accuracy : 66.09 % | Validation Loss : 0.8418 | Validation Accuracy : 64.50 %\n",
      "Epoch 26/100 | Training Loss : 0.8014 | Training Accuracy : 67.58 % | Validation Loss : 0.8432 | Validation Accuracy : 64.94 %\n",
      "Epoch 27/100 | Training Loss : 0.8100 | Training Accuracy : 66.52 % | Validation Loss : 0.8599 | Validation Accuracy : 64.07 %\n",
      "Epoch 28/100 | Training Loss : 0.8131 | Training Accuracy : 65.89 % | Validation Loss : 0.8456 | Validation Accuracy : 64.94 %\n",
      "Epoch 29/100 | Training Loss : 0.8072 | Training Accuracy : 67.05 % | Validation Loss : 0.8435 | Validation Accuracy : 65.37 %\n",
      "Epoch 30/100 | Training Loss : 0.7887 | Training Accuracy : 67.49 % | Validation Loss : 0.8508 | Validation Accuracy : 64.94 %\n",
      "Epoch 31/100 | Training Loss : 0.8062 | Training Accuracy : 67.25 % | Validation Loss : 0.8411 | Validation Accuracy : 61.90 %\n",
      "Epoch 32/100 | Training Loss : 0.8093 | Training Accuracy : 65.75 % | Validation Loss : 0.8339 | Validation Accuracy : 64.50 %\n",
      "Epoch 33/100 | Training Loss : 0.7719 | Training Accuracy : 68.02 % | Validation Loss : 0.8655 | Validation Accuracy : 63.64 %\n",
      "Epoch 34/100 | Training Loss : 0.8086 | Training Accuracy : 66.57 % | Validation Loss : 0.8710 | Validation Accuracy : 62.77 %\n",
      "Epoch 35/100 | Training Loss : 0.7905 | Training Accuracy : 67.05 % | Validation Loss : 0.8419 | Validation Accuracy : 61.04 %\n",
      "Epoch 36/100 | Training Loss : 0.7780 | Training Accuracy : 68.16 % | Validation Loss : 0.8375 | Validation Accuracy : 64.07 %\n",
      "Epoch 37/100 | Training Loss : 0.7963 | Training Accuracy : 66.43 % | Validation Loss : 0.8458 | Validation Accuracy : 64.50 %\n",
      "Epoch 38/100 | Training Loss : 0.8057 | Training Accuracy : 66.71 % | Validation Loss : 0.8760 | Validation Accuracy : 66.23 %\n",
      "Epoch 39/100 | Training Loss : 0.7675 | Training Accuracy : 68.69 % | Validation Loss : 0.8444 | Validation Accuracy : 63.64 %\n",
      "Epoch 40/100 | Training Loss : 0.7598 | Training Accuracy : 69.03 % | Validation Loss : 0.8336 | Validation Accuracy : 63.20 %\n",
      "Epoch 41/100 | Training Loss : 0.7550 | Training Accuracy : 70.00 % | Validation Loss : 0.8348 | Validation Accuracy : 64.50 %\n",
      "Epoch 42/100 | Training Loss : 0.7354 | Training Accuracy : 70.09 % | Validation Loss : 0.8407 | Validation Accuracy : 62.77 %\n",
      "Epoch 43/100 | Training Loss : 0.7426 | Training Accuracy : 68.84 % | Validation Loss : 0.8452 | Validation Accuracy : 63.64 %\n",
      "Epoch 44/100 | Training Loss : 0.7350 | Training Accuracy : 70.19 % | Validation Loss : 0.8357 | Validation Accuracy : 66.67 %\n",
      "Epoch 45/100 | Training Loss : 0.7513 | Training Accuracy : 69.22 % | Validation Loss : 0.8450 | Validation Accuracy : 64.07 %\n",
      "Epoch 46/100 | Training Loss : 0.7498 | Training Accuracy : 69.51 % | Validation Loss : 0.8411 | Validation Accuracy : 65.37 %\n",
      "Epoch 47/100 | Training Loss : 0.7325 | Training Accuracy : 70.62 % | Validation Loss : 0.8400 | Validation Accuracy : 64.50 %\n",
      "Epoch 48/100 | Training Loss : 0.7408 | Training Accuracy : 69.66 % | Validation Loss : 0.8273 | Validation Accuracy : 64.94 %\n",
      "Epoch 49/100 | Training Loss : 0.7536 | Training Accuracy : 68.55 % | Validation Loss : 0.8324 | Validation Accuracy : 65.80 %\n",
      "Epoch 50/100 | Training Loss : 0.7355 | Training Accuracy : 70.33 % | Validation Loss : 0.8287 | Validation Accuracy : 65.37 %\n",
      "Epoch 51/100 | Training Loss : 0.7336 | Training Accuracy : 70.24 % | Validation Loss : 0.8316 | Validation Accuracy : 65.80 %\n",
      "Epoch 52/100 | Training Loss : 0.7320 | Training Accuracy : 70.53 % | Validation Loss : 0.8270 | Validation Accuracy : 64.07 %\n",
      "Epoch 53/100 | Training Loss : 0.7366 | Training Accuracy : 70.53 % | Validation Loss : 0.8377 | Validation Accuracy : 63.20 %\n",
      "Epoch 54/100 | Training Loss : 0.7312 | Training Accuracy : 70.82 % | Validation Loss : 0.8356 | Validation Accuracy : 64.07 %\n",
      "Epoch 55/100 | Training Loss : 0.7452 | Training Accuracy : 69.75 % | Validation Loss : 0.8254 | Validation Accuracy : 66.67 %\n",
      "Epoch 56/100 | Training Loss : 0.7331 | Training Accuracy : 69.90 % | Validation Loss : 0.8222 | Validation Accuracy : 64.94 %\n",
      "Epoch 57/100 | Training Loss : 0.7216 | Training Accuracy : 70.82 % | Validation Loss : 0.8240 | Validation Accuracy : 64.07 %\n",
      "Epoch 58/100 | Training Loss : 0.7416 | Training Accuracy : 70.43 % | Validation Loss : 0.8371 | Validation Accuracy : 64.07 %\n",
      "Epoch 59/100 | Training Loss : 0.7176 | Training Accuracy : 71.64 % | Validation Loss : 0.8267 | Validation Accuracy : 65.80 %\n",
      "Epoch 60/100 | Training Loss : 0.7404 | Training Accuracy : 69.61 % | Validation Loss : 0.8245 | Validation Accuracy : 64.07 %\n",
      "Epoch 61/100 | Training Loss : 0.7352 | Training Accuracy : 70.77 % | Validation Loss : 0.8278 | Validation Accuracy : 66.23 %\n",
      "Epoch 62/100 | Training Loss : 0.7399 | Training Accuracy : 70.62 % | Validation Loss : 0.8363 | Validation Accuracy : 64.50 %\n",
      "Epoch 63/100 | Training Loss : 0.7321 | Training Accuracy : 70.67 % | Validation Loss : 0.8343 | Validation Accuracy : 64.50 %\n",
      "Epoch 64/100 | Training Loss : 0.7357 | Training Accuracy : 69.51 % | Validation Loss : 0.8278 | Validation Accuracy : 64.50 %\n",
      "Epoch 65/100 | Training Loss : 0.7161 | Training Accuracy : 71.35 % | Validation Loss : 0.8354 | Validation Accuracy : 64.50 %\n",
      "Epoch 66/100 | Training Loss : 0.6961 | Training Accuracy : 73.13 % | Validation Loss : 0.8306 | Validation Accuracy : 64.94 %\n",
      "Epoch 67/100 | Training Loss : 0.7128 | Training Accuracy : 71.10 % | Validation Loss : 0.8282 | Validation Accuracy : 64.94 %\n",
      "Epoch 68/100 | Training Loss : 0.7284 | Training Accuracy : 70.09 % | Validation Loss : 0.8292 | Validation Accuracy : 65.80 %\n",
      "Epoch 69/100 | Training Loss : 0.7085 | Training Accuracy : 71.20 % | Validation Loss : 0.8295 | Validation Accuracy : 64.94 %\n",
      "Epoch 70/100 | Training Loss : 0.7188 | Training Accuracy : 71.64 % | Validation Loss : 0.8298 | Validation Accuracy : 66.23 %\n",
      "Epoch 71/100 | Training Loss : 0.7190 | Training Accuracy : 71.49 % | Validation Loss : 0.8260 | Validation Accuracy : 64.94 %\n",
      "Epoch 72/100 | Training Loss : 0.7429 | Training Accuracy : 69.90 % | Validation Loss : 0.8379 | Validation Accuracy : 65.37 %\n",
      "Epoch 73/100 | Training Loss : 0.7098 | Training Accuracy : 71.10 % | Validation Loss : 0.8348 | Validation Accuracy : 65.37 %\n",
      "Epoch 74/100 | Training Loss : 0.7193 | Training Accuracy : 70.38 % | Validation Loss : 0.8255 | Validation Accuracy : 65.37 %\n",
      "Epoch 75/100 | Training Loss : 0.7163 | Training Accuracy : 71.83 % | Validation Loss : 0.8319 | Validation Accuracy : 66.23 %\n",
      "Epoch 76/100 | Training Loss : 0.7359 | Training Accuracy : 70.72 % | Validation Loss : 0.8255 | Validation Accuracy : 65.37 %\n",
      "Epoch 77/100 | Training Loss : 0.7192 | Training Accuracy : 71.35 % | Validation Loss : 0.8349 | Validation Accuracy : 64.94 %\n",
      "Epoch 78/100 | Training Loss : 0.7219 | Training Accuracy : 70.86 % | Validation Loss : 0.8313 | Validation Accuracy : 65.37 %\n",
      "Epoch 79/100 | Training Loss : 0.7106 | Training Accuracy : 71.97 % | Validation Loss : 0.8318 | Validation Accuracy : 66.23 %\n",
      "Epoch 80/100 | Training Loss : 0.7321 | Training Accuracy : 70.82 % | Validation Loss : 0.8270 | Validation Accuracy : 65.37 %\n",
      "Epoch 81/100 | Training Loss : 0.7230 | Training Accuracy : 71.30 % | Validation Loss : 0.8256 | Validation Accuracy : 64.94 %\n",
      "Epoch 82/100 | Training Loss : 0.7172 | Training Accuracy : 71.10 % | Validation Loss : 0.8354 | Validation Accuracy : 65.37 %\n",
      "Epoch 83/100 | Training Loss : 0.7080 | Training Accuracy : 71.35 % | Validation Loss : 0.8296 | Validation Accuracy : 64.94 %\n",
      "Epoch 84/100 | Training Loss : 0.7261 | Training Accuracy : 71.97 % | Validation Loss : 0.8359 | Validation Accuracy : 65.37 %\n",
      "Epoch 85/100 | Training Loss : 0.7550 | Training Accuracy : 69.85 % | Validation Loss : 0.8352 | Validation Accuracy : 66.23 %\n",
      "Epoch 86/100 | Training Loss : 0.7234 | Training Accuracy : 71.92 % | Validation Loss : 0.8312 | Validation Accuracy : 65.37 %\n",
      "Epoch 87/100 | Training Loss : 0.7052 | Training Accuracy : 71.68 % | Validation Loss : 0.8294 | Validation Accuracy : 65.80 %\n",
      "Epoch 88/100 | Training Loss : 0.7268 | Training Accuracy : 70.62 % | Validation Loss : 0.8339 | Validation Accuracy : 64.94 %\n",
      "Epoch 89/100 | Training Loss : 0.7114 | Training Accuracy : 72.60 % | Validation Loss : 0.8313 | Validation Accuracy : 64.94 %\n",
      "Epoch 90/100 | Training Loss : 0.7227 | Training Accuracy : 70.72 % | Validation Loss : 0.8336 | Validation Accuracy : 66.23 %\n",
      "Epoch 91/100 | Training Loss : 0.7175 | Training Accuracy : 70.91 % | Validation Loss : 0.8259 | Validation Accuracy : 63.64 %\n",
      "Epoch 92/100 | Training Loss : 0.7236 | Training Accuracy : 71.39 % | Validation Loss : 0.8268 | Validation Accuracy : 65.37 %\n",
      "Epoch 93/100 | Training Loss : 0.7335 | Training Accuracy : 69.46 % | Validation Loss : 0.8297 | Validation Accuracy : 64.50 %\n",
      "Epoch 94/100 | Training Loss : 0.7233 | Training Accuracy : 71.39 % | Validation Loss : 0.8313 | Validation Accuracy : 64.07 %\n",
      "Epoch 95/100 | Training Loss : 0.7101 | Training Accuracy : 71.35 % | Validation Loss : 0.8332 | Validation Accuracy : 65.80 %\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1) Set seeds for reproducibility\n",
    "###############################################################################\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from einops.layers.torch import Rearrange\n",
    "import mne\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "###############################################################################\n",
    "# 2) Load & preprocess data from the pickle file\n",
    "###############################################################################\n",
    "def pad_and_resize_eeg(eeg_2d, target_h=22, target_w=320):\n",
    "    \"\"\"\n",
    "    Zero-pad or resize EEG data to ensure consistent dimensions\n",
    "    \n",
    "    Args:\n",
    "        eeg_2d (np.ndarray): EEG data of shape (channels, time_samples)\n",
    "        target_h (int): Target number of EEG channels\n",
    "        target_w (int): Target number of time samples\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Zero-padded EEG data of shape (target_h, target_w)\n",
    "    \"\"\"\n",
    "    out = np.zeros((target_h, target_w), dtype=eeg_2d.dtype)\n",
    "    h, w = eeg_2d.shape\n",
    "    used_h = min(h, target_h)\n",
    "    used_w = min(w, target_w)\n",
    "    out[:used_h, :used_w] = eeg_2d[:used_h, :used_w]\n",
    "    return out\n",
    "\n",
    "def load_and_preprocess_data(pickle_path, target_h=22, target_w=320, l_freq=7, h_freq=30):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses EEG data from a pickle file\n",
    "\n",
    "    Args:\n",
    "        pickle_path (str): Path to the pickle file\n",
    "        target_h (int): Target number of EEG channels\n",
    "        target_w (int): Target number of time samples\n",
    "        l_freq (float): Low cutoff frequency for band-pass filter\n",
    "        h_freq (float): High cutoff frequency for band-pass filter\n",
    "\n",
    "    Returns:\n",
    "        all_X (np.ndarray): Preprocessed EEG data of shape (trials, 1, channels, time_samples)\n",
    "        all_y (np.ndarray): Labels of shape (trials,)\n",
    "        all_subjects (np.ndarray): Subject indices for each trial\n",
    "    \"\"\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        raw_data = pickle.load(f)  # List of MNE Epochs, one per subject\n",
    "\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    all_subjects = []\n",
    "\n",
    "    for subj_idx, epochs in enumerate(raw_data):\n",
    "        # Apply band-pass filter\n",
    "        epochs = epochs.copy().filter(l_freq=l_freq, h_freq=h_freq)\n",
    "        X = epochs.get_data()  # (trials, channels, time_samples)\n",
    "        y = epochs.events[:, 2]  # (trials,)\n",
    "\n",
    "        # Mapping: Left Hand = 0, Right Hand = 1\n",
    "        # Convert 1-based event IDs to 0-based class labels\n",
    "        y = y - 1  # labels are 0,1,2,3\n",
    "\n",
    "        # Resize each trial\n",
    "        padded = [pad_and_resize_eeg(trial, target_h, target_w) for trial in X]\n",
    "        padded_array = np.array(padded)  # (trials, channels, time_samples)\n",
    "\n",
    "        # Expand dimensions to match (B, 1, C, T)\n",
    "        padded_array = np.expand_dims(padded_array, axis=1)  # (trials, 1, channels, time_samples)\n",
    "\n",
    "        all_X.append(padded_array)\n",
    "        all_y.append(y)\n",
    "        all_subjects += [subj_idx] * len(y)\n",
    "\n",
    "    all_X = np.concatenate(all_X, axis=0)  # (total_trials, 1, channels, time_samples)\n",
    "    all_y = np.concatenate(all_y, axis=0)  # (total_trials,)\n",
    "    all_subjects = np.array(all_subjects)   # (total_trials,)\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    # Reshape to (trials, channels * time_samples) for scaling\n",
    "    all_X_reshaped = all_X.reshape(all_X.shape[0], -1)\n",
    "    all_X_scaled = scaler.fit_transform(all_X_reshaped)\n",
    "    all_X = all_X_scaled.reshape(all_X.shape[0], 1, target_h, target_w).astype(np.float32)\n",
    "\n",
    "    return all_X, all_y, all_subjects\n",
    "\n",
    "###############################################################################\n",
    "# 3) Dataset without augmentation\n",
    "###############################################################################\n",
    "class MultiSubjectBCIDataset(Dataset):\n",
    "    def __init__(self, X, y, augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (np.ndarray): EEG data of shape (trials, 1, channels, time_samples)\n",
    "            y (np.ndarray): Labels of shape (trials,)\n",
    "            augment (bool): Whether to apply data augmentation (kept for compatibility, but ignored)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # self.augment = augment\n",
    "        # self.max_shift = 10     # shift range\n",
    "        # self.noise_amp = 0.01   # noise amplitude\n",
    "        # self.dropout_rate = 0.05\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_np = self.X[idx]  # shape: (1, channels, time_samples)\n",
    "        y = self.y[idx]\n",
    "\n",
    "        # Convert to torch without any augmentation\n",
    "        x_t = torch.from_numpy(x_np).float()  # (1, channels, time_samples)\n",
    "        return x_t, torch.tensor(y).long()\n",
    "\n",
    "###############################################################################\n",
    "# 4) Window-partitioning & shift for Swin-1D\n",
    "###############################################################################\n",
    "def pad_sequence_1d(x, window_size):\n",
    "    \"\"\"\n",
    "    x shape: (B, L, C)\n",
    "    Zero-pad or resize length to multiple of window_size\n",
    "    \"\"\"\n",
    "    B, L, C = x.shape\n",
    "    remainder = L % window_size\n",
    "    if remainder == 0:\n",
    "        return x, L, 0\n",
    "    pad_len = window_size - remainder\n",
    "    pad_vec = torch.zeros(B, pad_len, C, dtype=x.dtype, device=x.device)\n",
    "    x_padded = torch.cat([x, pad_vec], dim=1)\n",
    "    return x_padded, L, pad_len\n",
    "\n",
    "def window_partition_1d(x, window_size):\n",
    "    \"\"\"\n",
    "    x shape: (B, L, C) => (B*nW, window_size, C)\n",
    "    \"\"\"\n",
    "    B, L, C = x.shape\n",
    "    x_padded, orig_L, pad_len = pad_sequence_1d(x, window_size)\n",
    "    Bp, Lp, Cp = x_padded.shape\n",
    "    num_windows = Lp // window_size\n",
    "    x_padded = x_padded.view(Bp, num_windows, window_size, Cp)\n",
    "    x_windows = x_padded.reshape(Bp * num_windows, window_size, Cp)\n",
    "    return x_windows, (orig_L, pad_len, num_windows)\n",
    "\n",
    "def window_reverse_1d(x_windows, window_size, pad_info):\n",
    "    \"\"\"\n",
    "    Reconstruct from (B*nW, window_size, C) => (B, L, C)\n",
    "    \"\"\"\n",
    "    orig_L, pad_len, num_windows = pad_info\n",
    "    BnW, WS, C = x_windows.shape\n",
    "    B = BnW // num_windows\n",
    "    x_reshaped = x_windows.view(B, num_windows, WS, C)\n",
    "    x_merged = x_reshaped.reshape(B, num_windows * WS, C)\n",
    "    if pad_len > 0:\n",
    "        x_merged = x_merged[:, :orig_L, :]\n",
    "    return x_merged\n",
    "\n",
    "def cyclic_shift_1d(x, shift_size):\n",
    "    \"\"\"\n",
    "    Negative roll along dimension=1\n",
    "    \"\"\"\n",
    "    return torch.roll(x, shifts=-shift_size, dims=1)\n",
    "\n",
    "def cyclic_shift_back_1d(x, shift_size):\n",
    "    return torch.roll(x, shifts=shift_size, dims=1)\n",
    "\n",
    "###############################################################################\n",
    "# 5) Custom QKV Attention & MLP\n",
    "###############################################################################\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, L, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, num_heads, L, L)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, L, C)  # (B, L, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "###############################################################################\n",
    "# 6) Swin1DBlock & Swin1DTransformer\n",
    "###############################################################################\n",
    "class Swin1DBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size=4,\n",
    "        shift_size=2,\n",
    "        mlp_hidden=128,\n",
    "        attn_dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        # LayerNorm & Attention\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim=dim, num_heads=num_heads, attn_dropout=attn_dropout)\n",
    "\n",
    "        # LayerNorm & MLP\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim=dim, hidden_dim=mlp_hidden, dropout=attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, dim)\n",
    "        if self.shift_size > 0:\n",
    "            x = cyclic_shift_1d(x, self.shift_size)\n",
    "\n",
    "        x_windows, pad_info = window_partition_1d(x, self.window_size)\n",
    "\n",
    "        # Attention + residual\n",
    "        shortcut = x_windows\n",
    "        x_windows = self.ln1(x_windows)\n",
    "        x_windows = self.attn(x_windows)\n",
    "        x_windows = shortcut + x_windows\n",
    "\n",
    "        # MLP + residual\n",
    "        shortcut = x_windows\n",
    "        x_windows = self.ln2(x_windows)\n",
    "        x_windows = self.mlp(x_windows)\n",
    "        x_windows = shortcut + x_windows\n",
    "\n",
    "        x_merged = window_reverse_1d(x_windows, self.window_size, pad_info)\n",
    "        if self.shift_size > 0:\n",
    "            x_merged = cyclic_shift_back_1d(x_merged, self.shift_size)\n",
    "        return x_merged\n",
    "\n",
    "class Swin1DTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=64,           # Embedding dimension\n",
    "        num_layers=3,     # Number of Swin1DBlock layers\n",
    "        num_heads=4,      # Number of attention heads\n",
    "        mlp_hidden=128,   # Hidden dimension in MLP\n",
    "        window_size=4,    # Window size for attention\n",
    "        attn_dropout=0.1, # Dropout rate for attention\n",
    "        fc_dropout=0.3    # Dropout rate before final FC\n",
    "    ):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        for i in range(num_layers):\n",
    "            shift = window_size // 2 if (i % 2 == 1) else 0\n",
    "            block = Swin1DBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=shift,\n",
    "                mlp_hidden=mlp_hidden,\n",
    "                attn_dropout=attn_dropout\n",
    "            )\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, dim)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        # average pooling over the L dimension\n",
    "        x = x.mean(dim=1)  # (B, dim)\n",
    "        return x  # Feature Vector: (B, dim)\n",
    "\n",
    "###############################################################################\n",
    "# 7) Positional Embeddings: learnable, sine, or none\n",
    "###############################################################################\n",
    "def create_positional_embedding(mode, seq_len, dim):\n",
    "    \"\"\"\n",
    "    Creates positional embeddings\n",
    "\n",
    "    Args:\n",
    "        mode (str): 'learnable', 'sine', or 'none'\n",
    "        seq_len (int): Sequence length\n",
    "        dim (int): Embedding dimension\n",
    "\n",
    "    Returns:\n",
    "        nn.Parameter or None: Positional embedding tensor\n",
    "    \"\"\"\n",
    "    if mode == 'none':\n",
    "        return None\n",
    "    elif mode == 'learnable':\n",
    "        pe = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
    "        nn.init.trunc_normal_(pe, std=0.02)\n",
    "        return pe\n",
    "    elif mode == 'sine':\n",
    "        # Classic sinusoidal\n",
    "        pe_np = np.zeros((seq_len, dim))\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, dim, 2):\n",
    "                theta = pos / (10000 ** ((2 * i) / dim))\n",
    "                pe_np[pos, i]   = np.sin(theta)\n",
    "                if i+1 < dim:\n",
    "                    pe_np[pos, i+1] = np.cos(theta)\n",
    "        pe = torch.from_numpy(pe_np).float().unsqueeze(0)  # shape (1, seq_len, dim)\n",
    "        return nn.Parameter(pe, requires_grad=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported positional embedding mode: {mode}\")\n",
    "\n",
    "###############################################################################\n",
    "# 8) Patch Embedding\n",
    "###############################################################################\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1)),  # Temporal convolution\n",
    "            nn.Conv2d(40, 40, kernel_size=(22, 1), stride=(1, 1)),  # Spatial convolution across electrodes\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 75), stride=(1, 15)),  # Downsample temporal dimension\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(40, emb_size, kernel_size=(1,1), stride=(1,1))  # Projection to emb_size\n",
    "        )\n",
    "        self.rearrange = Rearrange('b e (h) (w) -> b (h w) e')  # Reshape to (B, seq_len, emb_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.shallownet(x)           # [B, emb_size, 1, w]\n",
    "        x = self.rearrange(x)            # [B, seq_len, emb_size]\n",
    "        return x\n",
    "\n",
    "###############################################################################\n",
    "# 9) Classification Head\n",
    "###############################################################################\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim=64, num_classes=4):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)            # [B, dim]\n",
    "        logits = self.linear(x)        # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 10) CCST\n",
    "###############################################################################\n",
    "class CCST(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size=40,\n",
    "        swin_embedding_dim=64,\n",
    "        num_swin_layers=3,\n",
    "        num_heads=4,\n",
    "        mlp_size=128,\n",
    "        fc_dropout=0.3,\n",
    "        pos_emb_mode='learnable',\n",
    "        num_classes=4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(emb_size=emb_size)  # [B, seq_len, emb_size]\n",
    "        self.embedding_projection = nn.Linear(emb_size, swin_embedding_dim)  # [B, seq_len, swin_embedding_dim]\n",
    "        self.pos_encoding = create_positional_embedding(pos_emb_mode, seq_len=15, dim=swin_embedding_dim)\n",
    "        self.transformer = Swin1DTransformer(\n",
    "            dim=swin_embedding_dim,\n",
    "            num_layers=num_swin_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_hidden=mlp_size,\n",
    "            window_size=4,\n",
    "            attn_dropout=0.1\n",
    "        )\n",
    "        self.classification_head = ClassificationHead(input_dim=swin_embedding_dim, num_classes=num_classes)  # [B, num_classes]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, 1, 22, 321)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Logits of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        x = self.patch_embedding(x)         # [B, seq_len, emb_size]\n",
    "        x = self.embedding_projection(x)    # [B, seq_len, swin_embedding_dim]\n",
    "        if self.pos_encoding is not None:\n",
    "            x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        features = self.transformer(x)      # [B, swin_embedding_dim]\n",
    "        logits = self.classification_head(features)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "###############################################################################\n",
    "# 11) Training and Evaluation\n",
    "###############################################################################\n",
    "class HybridExP():\n",
    "    def __init__(self, nsub, pickle_path='BNCI2014_001_TrainingSession_80Hz_Epochs.pkl', device='cuda:0'):\n",
    "        super(HybridExP, self).__init__()\n",
    "        self.batch_size = 72\n",
    "        self.n_epochs = 150\n",
    "        self.patience = 10\n",
    "        self.c_dim = 4  # Number of classes (Left Hand, Right Hand)\n",
    "        self.lr = 3e-4\n",
    "        self.betas = (0.9, 0.999)\n",
    "        self.nSub = nsub\n",
    "        self.pickle_path = pickle_path\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.criterion_cls = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "        self.model = CCST(num_classes=self.c_dim).to(self.device)\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    # Data augmentation method (kept for compatibility, does nothing)\n",
    "    def interaug(self, timg, label):\n",
    "        \"\"\"\n",
    "        Placeholder for additional data augmentation (returns unchanged data)\n",
    "        \"\"\"\n",
    "        return timg, label\n",
    "\n",
    "    # Load data\n",
    "    def get_source_data(self):\n",
    "        # Load and preprocess data\n",
    "        all_X, all_y, all_subjects = load_and_preprocess_data(\n",
    "            pickle_path=self.pickle_path,\n",
    "            target_h=22,\n",
    "            target_w=320,\n",
    "            l_freq=7,\n",
    "            h_freq=30\n",
    "        )\n",
    "\n",
    "        return all_X, all_y, all_subjects\n",
    "\n",
    "    # Training method for one fold\n",
    "    def train_fold(self, train_indices, val_indices, test_indices, train_labels, test_labels):\n",
    "        # Split data into training and validation\n",
    "        X_train = self.X[train_indices]\n",
    "        y_train = self.y[train_indices]\n",
    "        X_val = self.X[val_indices]\n",
    "        y_val = self.y[val_indices]\n",
    "        X_test = self.X[test_indices]\n",
    "        y_test = self.y[test_indices]\n",
    "\n",
    "        # Create datasets without augmentation\n",
    "        train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=False)\n",
    "        val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n",
    "        test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Define optimizer and scheduler\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                         factor=0.5, patience=5)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Training Phase\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for img, label in train_loader:\n",
    "                img = img.to(self.device)  # (B, 1, 22, 321)\n",
    "                label = label.to(self.device)  # (B,)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(img)  # (B, num_classes)\n",
    "                loss = self.criterion_cls(outputs, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * img.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "            train_loss = running_loss / total\n",
    "            train_acc = correct / total\n",
    "\n",
    "            # Validation Phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for img, label in val_loader:\n",
    "                    img = img.to(self.device)\n",
    "                    label = label.to(self.device)\n",
    "\n",
    "                    outputs = self.model(img)\n",
    "                    loss = self.criterion_cls(outputs, label)\n",
    "\n",
    "                    val_loss += loss.item() * img.size(0)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    val_correct += (preds == label).sum().item()\n",
    "                    val_total += label.size(0)\n",
    "\n",
    "            val_loss /= val_total\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            # Step scheduler\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{self.n_epochs} | '\n",
    "                  f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | '\n",
    "                  f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "            # Early Stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = self.model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        # Testing Phase\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img, label in test_loader:\n",
    "                img = img.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                outputs = self.model(img)\n",
    "                loss = self.criterion_cls(outputs, label)\n",
    "\n",
    "                test_loss += loss.item() * img.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_correct += (preds == label).sum().item()\n",
    "                test_total += label.size(0)\n",
    "\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(label.cpu().numpy())\n",
    "\n",
    "        test_loss /= test_total\n",
    "        test_acc = test_correct / test_total\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "        return test_acc, all_labels, all_preds\n",
    "\n",
    "    # Training method with LOSO cross-validation\n",
    "    def train_loso(self):\n",
    "        # Load data\n",
    "        self.X, self.y, self.subjects = self.get_source_data()\n",
    "\n",
    "        logo = LeaveOneGroupOut()\n",
    "        device = self.device\n",
    "\n",
    "        test_accuracies = []\n",
    "        subject_ids = []\n",
    "        fold_count = 0\n",
    "\n",
    "        print(\"\\n####################\")\n",
    "        print(\"Traininig Started\")\n",
    "        print(\"####################\")\n",
    "        \n",
    "        for train_idx, test_idx in logo.split(self.X, self.y, groups=self.subjects):\n",
    "            fold_count += 1\n",
    "            heldout_subj = self.subjects[test_idx[0]]\n",
    "            training_subjects = np.unique(self.subjects[train_idx])\n",
    "            # Convert to 1-based indexing for display\n",
    "            training_subjects_1based = [s + 1 for s in training_subjects]\n",
    "            test_subj_1based = heldout_subj + 1\n",
    "            print(f\"\\n===== Fold {fold_count}\")\n",
    "            print(f\"===== Seed : {42+fold_count}\")\n",
    "            print(f\"===== Training Subject : {', '.join(map(str, training_subjects_1based))}\")\n",
    "            print(f\"===== Test Subject : {test_subj_1based}\\n\")\n",
    "\n",
    "            # Re-seed per fold for reproducibility\n",
    "            set_seed(42 + fold_count)\n",
    "\n",
    "            X_train_full, X_test = self.X[train_idx], self.X[test_idx]\n",
    "            y_train_full, y_test = self.y[train_idx], self.y[test_idx]\n",
    "\n",
    "            # Further split training data into training and validation (e.g., 90-10)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train_full, y_train_full, test_size=0.1,\n",
    "                stratify=y_train_full, random_state=42 + fold_count\n",
    "            )\n",
    "\n",
    "            # Create datasets without augmentation\n",
    "            train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=False)\n",
    "            val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n",
    "            test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n",
    "\n",
    "            # Create dataloaders\n",
    "            train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "            # Define optimizer and scheduler\n",
    "            optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                             factor=0.5, patience=5)\n",
    "\n",
    "            best_val_acc = 0.0\n",
    "            patience_counter = 0\n",
    "            best_model_state = None\n",
    "\n",
    "            for epoch in range(self.n_epochs):\n",
    "                # Training Phase\n",
    "                self.model.train()\n",
    "                running_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "\n",
    "                for img, label in train_loader:\n",
    "                    img = img.to(self.device)  # (B, 1, 22, 321)\n",
    "                    label = label.to(self.device)  # (B,)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.model(img)  # (B, num_classes)\n",
    "                    loss = self.criterion_cls(outputs, label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * img.size(0)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct += (preds == label).sum().item()\n",
    "                    total += label.size(0)\n",
    "\n",
    "                train_loss = running_loss / total\n",
    "                train_acc = correct / total\n",
    "\n",
    "                # Validation Phase\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for img, label in val_loader:\n",
    "                        img = img.to(self.device)\n",
    "                        label = label.to(self.device)\n",
    "\n",
    "                        outputs = self.model(img)\n",
    "                        loss = self.criterion_cls(outputs, label)\n",
    "\n",
    "                        val_loss += loss.item() * img.size(0)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        val_correct += (preds == label).sum().item()\n",
    "                        val_total += label.size(0)\n",
    "\n",
    "                val_loss /= val_total\n",
    "                val_acc = val_correct / val_total\n",
    "\n",
    "                # Step scheduler\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "                print(f'Epoch {epoch+1}/{self.n_epochs} | '\n",
    "                      f'Training Loss : {train_loss:.4f} | Training Accuracy : {train_acc*100:.2f} % | '\n",
    "                      f'Validation Loss : {val_loss:.4f} | Validation Accuracy : {val_acc*100:.2f} %')\n",
    "\n",
    "                # Early Stopping\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = self.model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    # if patience_counter >= self.patience:\n",
    "                    #     print(f\"\\n=== Early Stopping Triggered at Epoch {epoch+1}\\n\")\n",
    "                    #     break\n",
    "\n",
    "            # Load best model\n",
    "            if best_model_state is not None:\n",
    "                self.model.load_state_dict(best_model_state)\n",
    "\n",
    "            # Testing Phase\n",
    "            self.model.eval()\n",
    "            test_loss = 0.0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for img, label in test_loader:\n",
    "                    img = img.to(self.device)\n",
    "                    label = label.to(self.device)\n",
    "\n",
    "                    outputs = self.model(img)\n",
    "                    loss = self.criterion_cls(outputs, label)\n",
    "\n",
    "                    test_loss += loss.item() * img.size(0)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    test_correct += (preds == label).sum().item()\n",
    "                    test_total += label.size(0)\n",
    "\n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(label.cpu().numpy())\n",
    "\n",
    "            test_loss /= test_total\n",
    "            test_acc = test_correct / test_total\n",
    "\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "\n",
    "            print(f'Test Subject : {test_subj_1based} | Test Loss : {test_loss:.4f} | Test Accuracy : {test_acc*100:.2f} %')\n",
    "            print(\"\\n################################\")\n",
    "\n",
    "            test_accuracies.append(test_acc)\n",
    "            subject_ids.append(heldout_subj)\n",
    "\n",
    "        # Summary of LOSO\n",
    "        avg_test_acc = np.mean(test_accuracies) * 100\n",
    "        \n",
    "        print(\"\\n================\")\n",
    "        print(\"LOSO Summary\")\n",
    "        print(\"================\")\n",
    "        for i, sid in enumerate(subject_ids):\n",
    "            print(f\"Subject {sid} -> Test Accuracy : {test_accuracies[i]*100:.2f} %\")\n",
    "        print(\"\\n-----------------------------\")\n",
    "        print(f\"Average Test Accuracy : {avg_test_acc:.2f} %\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "###############################################################################\n",
    "# 12) Main Execution\n",
    "###############################################################################\n",
    "def main():\n",
    "    best = 0\n",
    "    aver = 0\n",
    "\n",
    "    # Path to the pickle file\n",
    "    pickle_path = 'BNCI2014_001_TrainingSession_80Hz_Epochs.pkl'\n",
    "\n",
    "    # Initialize and train the model\n",
    "    exp = HybridExP(nsub=1, pickle_path=pickle_path, device='cuda:0')  # 'nsub' will be managed by LOSO\n",
    "    exp.train_loso()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7205743,
     "sourceId": 13734721,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
