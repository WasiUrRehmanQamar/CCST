{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11494733,"sourceType":"datasetVersion","datasetId":7205743}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Use GPU**","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:11:59.146752Z","iopub.execute_input":"2025-10-14T06:11:59.147502Z","iopub.status.idle":"2025-10-14T06:11:59.154891Z","shell.execute_reply.started":"2025-10-14T06:11:59.147448Z","shell.execute_reply":"2025-10-14T06:11:59.153398Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **Hide Warnings**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:11:59.156423Z","iopub.execute_input":"2025-10-14T06:11:59.156718Z","iopub.status.idle":"2025-10-14T06:11:59.178586Z","shell.execute_reply.started":"2025-10-14T06:11:59.156692Z","shell.execute_reply":"2025-10-14T06:11:59.177408Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **Install Libraries**","metadata":{}},{"cell_type":"code","source":"pip install torch torchvision torchaudio torchinfo einops scikit-learn pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:11:59.180950Z","iopub.execute_input":"2025-10-14T06:11:59.181460Z","iopub.status.idle":"2025-10-14T06:12:03.603057Z","shell.execute_reply.started":"2025-10-14T06:11:59.181409Z","shell.execute_reply":"2025-10-14T06:12:03.601542Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U mne==1.0.0 scipy==1.13.1 numpy==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:12:03.605467Z","iopub.execute_input":"2025-10-14T06:12:03.605972Z","iopub.status.idle":"2025-10-14T06:12:09.383627Z","shell.execute_reply.started":"2025-10-14T06:12:03.605843Z","shell.execute_reply":"2025-10-14T06:12:09.382074Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mne==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (3.7.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (4.67.1)\nRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (1.8.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (4.4.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (24.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (3.1.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne==1.0.0) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne==1.0.0) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne==1.0.0) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (2.8.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mne==1.0.0) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2024.12.14)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport mne\nimport time\nimport math\nimport pickle\nimport random\nimport datetime\nimport scipy.io\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom einops import rearrange\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom einops.layers.torch import Rearrange, Reduce\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import LeaveOneGroupOut, train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:12:09.384958Z","iopub.execute_input":"2025-10-14T06:12:09.385459Z","iopub.status.idle":"2025-10-14T06:12:09.393004Z","shell.execute_reply.started":"2025-10-14T06:12:09.385423Z","shell.execute_reply":"2025-10-14T06:12:09.391513Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"###############################################################################\n# 1) Set seeds for reproducibility\n###############################################################################\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n###############################################################################\n# 2) Load & preprocess data from the pickle file\n###############################################################################\ndef pad_and_resize_eeg(eeg_2d, target_h=3, target_w=321):\n    \"\"\"\n    Zero-pad or resize EEG data to ensure consistent dimensions\n    \n    Args:\n        eeg_2d (np.ndarray): EEG data of shape (channels, time_samples)\n        target_h (int): Target number of EEG channels\n        target_w (int): Target number of time samples\n    \n    Returns:\n        np.ndarray: Zero-padded EEG data of shape (target_h, target_w)\n    \"\"\"\n    out = np.zeros((target_h, target_w), dtype=eeg_2d.dtype)\n    h, w = eeg_2d.shape\n    used_h = min(h, target_h)\n    used_w = min(w, target_w)\n    out[:used_h, :used_w] = eeg_2d[:used_h, :used_w]\n    return out\n\ndef load_and_preprocess_data(pickle_path, target_h=3, target_w=321, l_freq=7, h_freq=30):\n    \"\"\"\n    Loads and preprocesses EEG data from a pickle file\n\n    Args:\n        pickle_path (str): Path to the pickle file\n        target_h (int): Target number of EEG channels\n        target_w (int): Target number of time samples\n        l_freq (float): Low cutoff frequency for band-pass filter\n        h_freq (float): High cutoff frequency for band-pass filter\n\n    Returns:\n        all_X (np.ndarray): Preprocessed EEG data of shape (trials, 1, channels, time_samples)\n        all_y (np.ndarray): Labels of shape (trials,)\n        all_subjects (np.ndarray): Subject indices for each trial\n    \"\"\"\n    with open(pickle_path, 'rb') as f:\n        raw_data = pickle.load(f)  # List of MNE Epochs, one per subject\n\n    all_X = []\n    all_y = []\n    all_subjects = []\n\n    for subj_idx, epochs in enumerate(raw_data):\n        # Apply band-pass filter\n        epochs = epochs.copy().filter(l_freq=l_freq, h_freq=h_freq)\n        X = epochs.get_data()  # (trials, channels, time_samples)\n        y = epochs.events[:, 2]  # (trials,)\n\n        # Mapping: Left Hand = 0, Right Hand = 1\n        y = np.where(y == 1, 0, 1)\n\n        # Resize each trial\n        padded = [pad_and_resize_eeg(trial, target_h, target_w) for trial in X]\n        padded_array = np.array(padded)  # (trials, channels, time_samples)\n\n        # Expand dimensions to match (B, 1, C, T)\n        padded_array = np.expand_dims(padded_array, axis=1)  # (trials, 1, channels, time_samples)\n\n        all_X.append(padded_array)\n        all_y.append(y)\n        all_subjects += [subj_idx] * len(y)\n\n    all_X = np.concatenate(all_X, axis=0)  # (total_trials, 1, channels, time_samples)\n    all_y = np.concatenate(all_y, axis=0)  # (total_trials,)\n    all_subjects = np.array(all_subjects)   # (total_trials,)\n\n    # Standardize\n    scaler = StandardScaler()\n    # Reshape to (trials, channels * time_samples) for scaling\n    all_X_reshaped = all_X.reshape(all_X.shape[0], -1)\n    all_X_scaled = scaler.fit_transform(all_X_reshaped)\n    all_X = all_X_scaled.reshape(all_X.shape[0], 1, target_h, target_w).astype(np.float32)\n\n    return all_X, all_y, all_subjects\n\n###############################################################################\n# 3) Dataset with optional augmentation\n###############################################################################\nclass MultiSubjectBCIDataset(Dataset):\n    def __init__(self, X, y, augment=False):\n        \"\"\"\n        Args:\n            X (np.ndarray): EEG data of shape (trials, 1, channels, time_samples)\n            y (np.ndarray): Labels of shape (trials,)\n            augment (bool): Whether to apply data augmentation\n        \"\"\"\n        self.X = X\n        self.y = y\n        self.augment = augment\n\n        self.max_shift = 10     # shift range\n        self.noise_amp = 0.01   # noise amplitude\n        self.dropout_rate = 0.05\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x_np = self.X[idx]  # shape: (1, channels, time_samples)\n        y = self.y[idx]\n\n        # Convert to torch\n        x_t = torch.from_numpy(x_np).float()  # (1, channels, time_samples)\n\n        if self.augment:\n            # Random shift in time\n            shift = random.randint(-self.max_shift, self.max_shift)\n            if shift > 0:\n                x_t = torch.cat([x_t[:, :, shift:], \n                                 torch.zeros((1, x_t.size(1), shift))], dim=2)\n            elif shift < 0:\n                x_t = torch.cat([torch.zeros((1, x_t.size(1), -shift)),\n                                 x_t[:, :, :shift]], dim=2)\n\n            # Add small Gaussian noise\n            noise = torch.randn_like(x_t) * self.noise_amp\n            x_t += noise\n\n            # Random electrode dropout\n            num_drop = int(x_t.size(1) * self.dropout_rate)\n            drop_indices = torch.randperm(x_t.size(1))[:num_drop]\n            x_t[:, drop_indices, :] = 0\n\n        return x_t, torch.tensor(y).long()\n\n###############################################################################\n# 4) Window-partitioning & shift for Swin-1D\n###############################################################################\ndef pad_sequence_1d(x, window_size):\n    \"\"\"\n    x shape: (B, L, C)\n    Zero-pad or resize length to multiple of window_size\n    \"\"\"\n    B, L, C = x.shape\n    remainder = L % window_size\n    if remainder == 0:\n        return x, L, 0\n    pad_len = window_size - remainder\n    pad_vec = torch.zeros(B, pad_len, C, dtype=x.dtype, device=x.device)\n    x_padded = torch.cat([x, pad_vec], dim=1)\n    return x_padded, L, pad_len\n\ndef window_partition_1d(x, window_size):\n    \"\"\"\n    x shape: (B, L, C) => (B*nW, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x_padded, orig_L, pad_len = pad_sequence_1d(x, window_size)\n    Bp, Lp, Cp = x_padded.shape\n    num_windows = Lp // window_size\n    x_padded = x_padded.view(Bp, num_windows, window_size, Cp)\n    x_windows = x_padded.reshape(Bp * num_windows, window_size, Cp)\n    return x_windows, (orig_L, pad_len, num_windows)\n\ndef window_reverse_1d(x_windows, window_size, pad_info):\n    \"\"\"\n    Reconstruct from (B*nW, window_size, C) => (B, L, C)\n    \"\"\"\n    orig_L, pad_len, num_windows = pad_info\n    BnW, WS, C = x_windows.shape\n    B = BnW // num_windows\n    x_reshaped = x_windows.view(B, num_windows, WS, C)\n    x_merged = x_reshaped.reshape(B, num_windows * WS, C)\n    if pad_len > 0:\n        x_merged = x_merged[:, :orig_L, :]\n    return x_merged\n\ndef cyclic_shift_1d(x, shift_size):\n    \"\"\"\n    Negative roll along dimension=1\n    \"\"\"\n    return torch.roll(x, shifts=-shift_size, dims=1)\n\ndef cyclic_shift_back_1d(x, shift_size):\n    return torch.roll(x, shifts=shift_size, dims=1)\n\n###############################################################################\n# 5) Custom QKV Attention & MLP\n###############################################################################\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads, attn_dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.attn_drop = nn.Dropout(attn_dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(attn_dropout)\n\n    def forward(self, x):\n        B, L, C = x.shape\n        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, L, head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, num_heads, L, L)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        out = (attn @ v).transpose(1, 2).reshape(B, L, C)  # (B, L, C)\n        out = self.proj(out)\n        out = self.proj_drop(out)\n        return out\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n###############################################################################\n# 6) Swin1DBlock & Swin1DTransformer\n###############################################################################\nclass Swin1DBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=4,\n        shift_size=2,\n        mlp_hidden=128,\n        attn_dropout=0.1\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n        # LayerNorm & Attention\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim=dim, num_heads=num_heads, attn_dropout=attn_dropout)\n\n        # LayerNorm & MLP\n        self.ln2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim=dim, hidden_dim=mlp_hidden, dropout=attn_dropout)\n\n    def forward(self, x):\n        # x: (B, L, dim)\n        if self.shift_size > 0:\n            x = cyclic_shift_1d(x, self.shift_size)\n\n        x_windows, pad_info = window_partition_1d(x, self.window_size)\n\n        # Attention + residual\n        shortcut = x_windows\n        x_windows = self.ln1(x_windows)\n        x_windows = self.attn(x_windows)\n        x_windows = shortcut + x_windows\n\n        # MLP + residual\n        shortcut = x_windows\n        x_windows = self.ln2(x_windows)\n        x_windows = self.mlp(x_windows)\n        x_windows = shortcut + x_windows\n\n        x_merged = window_reverse_1d(x_windows, self.window_size, pad_info)\n        if self.shift_size > 0:\n            x_merged = cyclic_shift_back_1d(x_merged, self.shift_size)\n        return x_merged\n\nclass Swin1DTransformer(nn.Module):\n    def __init__(\n        self,\n        dim=64,           # Embedding dimension\n        num_layers=3,     # Number of Swin1DBlock layers\n        num_heads=4,      # Number of attention heads\n        mlp_hidden=128,   # Hidden dimension in MLP\n        window_size=4,    # Window size for attention\n        attn_dropout=0.1, # Dropout rate for attention\n        fc_dropout=0.3    # Dropout rate before final FC\n    ):\n        super().__init__()\n        blocks = []\n        for i in range(num_layers):\n            shift = window_size // 2 if (i % 2 == 1) else 0\n            block = Swin1DBlock(\n                dim=dim,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=shift,\n                mlp_hidden=mlp_hidden,\n                attn_dropout=attn_dropout\n            )\n            blocks.append(block)\n\n        self.blocks = nn.ModuleList(blocks)\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        # x: (B, L, dim)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        # average pooling over the L dimension\n        x = x.mean(dim=1)  # (B, dim)\n        return x  # Feature Vector: (B, dim)\n\n###############################################################################\n# 7) Positional Embeddings: learnable, sine, or none\n###############################################################################\ndef create_positional_embedding(mode, seq_len, dim):\n    \"\"\"\n    Creates positional embeddings\n\n    Args:\n        mode (str): 'learnable', 'sine', or 'none'\n        seq_len (int): Sequence length\n        dim (int): Embedding dimension\n\n    Returns:\n        nn.Parameter or None: Positional embedding tensor\n    \"\"\"\n    if mode == 'none':\n        return None\n    elif mode == 'learnable':\n        pe = nn.Parameter(torch.zeros(1, seq_len, dim))\n        nn.init.trunc_normal_(pe, std=0.02)\n        return pe\n    elif mode == 'sine':\n        # Classic sinusoidal\n        pe_np = np.zeros((seq_len, dim))\n        for pos in range(seq_len):\n            for i in range(0, dim, 2):\n                theta = pos / (10000 ** ((2 * i) / dim))\n                pe_np[pos, i]   = np.sin(theta)\n                if i+1 < dim:\n                    pe_np[pos, i+1] = np.cos(theta)\n        pe = torch.from_numpy(pe_np).float().unsqueeze(0)  # shape (1, seq_len, dim)\n        return nn.Parameter(pe, requires_grad=False)\n    else:\n        raise ValueError(f\"Unsupported positional embedding mode: {mode}\")\n\n###############################################################################\n# 8) Patch Embedding\n###############################################################################\nclass PatchEmbedding(nn.Module):\n    def __init__(self, emb_size=40):\n        super().__init__()\n\n        self.shallownet = nn.Sequential(\n            nn.Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1)),  # Temporal convolution\n            nn.Conv2d(40, 40, kernel_size=(3, 1), stride=(1, 1)),  # Spatial convolution across electrodes\n            nn.BatchNorm2d(40),\n            nn.ELU(),\n            nn.AvgPool2d(kernel_size=(1, 75), stride=(1, 15)),  # Downsample temporal dimension\n            nn.Dropout(p=0.5),\n            nn.Conv2d(40, emb_size, kernel_size=(1,1), stride=(1,1))  # Projection to emb_size\n        )\n        self.rearrange = Rearrange('b e (h) (w) -> b (h w) e')  # Reshape to (B, seq_len, emb_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.shallownet(x)           # [B, emb_size, 1, w]\n        x = self.rearrange(x)            # [B, seq_len, emb_size]\n        return x\n\n###############################################################################\n# 9) Classification Head\n###############################################################################\nclass ClassificationHead(nn.Module):\n    def __init__(self, input_dim=64, num_classes=2):\n        super(ClassificationHead, self).__init__()\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):\n        x = self.dropout(x)            # [B, dim]\n        logits = self.linear(x)        # [B, num_classes]\n        return logits\n\n\n###############################################################################\n# 10) CCST\n###############################################################################\nclass CCST(nn.Module):\n    def __init__(\n        self,\n        emb_size=40,\n        swin_embedding_dim=64,\n        num_swin_layers=3,\n        num_heads=4,\n        mlp_size=128,\n        fc_dropout=0.3,\n        pos_emb_mode='learnable',\n        num_classes=2\n    ):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(emb_size=emb_size)  # [B, seq_len, emb_size]\n        self.embedding_projection = nn.Linear(emb_size, swin_embedding_dim)  # [B, seq_len, swin_embedding_dim]\n        self.pos_encoding = create_positional_embedding(pos_emb_mode, seq_len=15, dim=swin_embedding_dim)\n        self.transformer = Swin1DTransformer(\n            dim=swin_embedding_dim,\n            num_layers=num_swin_layers,\n            num_heads=num_heads,\n            mlp_hidden=mlp_size,\n            window_size=4,\n            attn_dropout=0.1\n        )\n        self.classification_head = ClassificationHead(input_dim=swin_embedding_dim, num_classes=num_classes)  # [B, num_classes]\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (B, 1, 3, 321)\n        \n        Returns:\n            torch.Tensor: Logits of shape (B, num_classes)\n        \"\"\"\n        x = self.patch_embedding(x)         # [B, seq_len, emb_size]\n        x = self.embedding_projection(x)    # [B, seq_len, swin_embedding_dim]\n        if self.pos_encoding is not None:\n            x = x + self.pos_encoding[:, :x.size(1), :]\n        features = self.transformer(x)      # [B, swin_embedding_dim]\n        logits = self.classification_head(features)  # [B, num_classes]\n        return logits\n\n###############################################################################\n# 11) Training and Evaluation\n###############################################################################\nclass HybridExP():\n    def __init__(self, nsub, pickle_path='/kaggle/input/bcic-iv/2b.pickle', device='cuda:0'):\n        super(HybridExP, self).__init__()\n        self.batch_size = 72\n        self.n_epochs = 100\n        self.patience = 10\n        self.c_dim = 2  # Number of classes (Left Hand, Right Hand)\n        self.lr = 3e-4\n        self.betas = (0.9, 0.999)\n        self.nSub = nsub\n        self.pickle_path = pickle_path\n        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n\n        self.criterion_cls = nn.CrossEntropyLoss().to(self.device)\n\n        self.model = CCST(num_classes=self.c_dim).to(self.device)\n        self.model = nn.DataParallel(self.model)\n        self.model = self.model.to(self.device)\n\n    # Data augmentation method\n    def interaug(self, timg, label):\n        \"\"\"\n        Placeholder for additional data augmentation if required\n        \"\"\"\n        return timg, label\n\n    # Load data\n    def get_source_data(self):\n        # Load and preprocess data\n        all_X, all_y, all_subjects = load_and_preprocess_data(\n            pickle_path=self.pickle_path,\n            target_h=3,\n            target_w=321,\n            l_freq=7,\n            h_freq=30\n        )\n\n        return all_X, all_y, all_subjects\n\n    # Training method for one fold\n    def train_fold(self, train_indices, val_indices, test_indices, train_labels, test_labels):\n        # Split data into training and validation\n        X_train = self.X[train_indices]\n        y_train = self.y[train_indices]\n        X_val = self.X[val_indices]\n        y_val = self.y[val_indices]\n        X_test = self.X[test_indices]\n        y_test = self.y[test_indices]\n\n        # Create datasets\n        train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=True)\n        val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n        test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n\n        # Create dataloaders\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n\n        # Define optimizer and scheduler\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                         factor=0.5, patience=5, verbose=True)\n\n        best_val_acc = 0.0\n        patience_counter = 0\n        best_model_state = None\n\n        for epoch in range(self.n_epochs):\n            # Training Phase\n            self.model.train()\n            running_loss = 0.0\n            correct = 0\n            total = 0\n\n            for img, label in train_loader:\n                img = img.to(self.device)  # (B, 1, 3, 321)\n                label = label.to(self.device)  # (B,)\n\n                optimizer.zero_grad()\n                outputs = self.model(img)  # (B, num_classes)\n                loss = self.criterion_cls(outputs, label)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * img.size(0)\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == label).sum().item()\n                total += label.size(0)\n\n            train_loss = running_loss / total\n            train_acc = correct / total\n\n            # Validation Phase\n            self.model.eval()\n            val_loss = 0.0\n            val_correct = 0\n            val_total = 0\n\n            with torch.no_grad():\n                for img, label in val_loader:\n                    img = img.to(self.device)\n                    label = label.to(self.device)\n\n                    outputs = self.model(img)\n                    loss = self.criterion_cls(outputs, label)\n\n                    val_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    val_correct += (preds == label).sum().item()\n                    val_total += label.size(0)\n\n            val_loss /= val_total\n            val_acc = val_correct / val_total\n\n            # Step scheduler\n            scheduler.step(val_loss)\n\n            print(f'Epoch {epoch+1}/{self.n_epochs} | '\n                  f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | '\n                  f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n\n            # Early Stopping\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                best_model_state = self.model.state_dict()\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                # if patience_counter >= self.patience:\n                #     print(f\"Early stopping triggered at epoch {epoch+1}\")\n                #     break\n\n        # Load best model\n        if best_model_state is not None:\n            self.model.load_state_dict(best_model_state)\n\n        # Testing Phase\n        self.model.eval()\n        test_loss = 0.0\n        test_correct = 0\n        test_total = 0\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for img, label in test_loader:\n                img = img.to(self.device)\n                label = label.to(self.device)\n\n                outputs = self.model(img)\n                loss = self.criterion_cls(outputs, label)\n\n                test_loss += loss.item() * img.size(0)\n                _, preds = torch.max(outputs, 1)\n                test_correct += (preds == label).sum().item()\n                test_total += label.size(0)\n\n                all_preds.append(preds.cpu().numpy())\n                all_labels.append(label.cpu().numpy())\n\n        test_loss /= test_total\n        test_acc = test_correct / test_total\n\n        all_preds = np.concatenate(all_preds)\n        all_labels = np.concatenate(all_labels)\n\n        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')\n\n        return test_acc, all_labels, all_preds\n\n    # Training method with LOSO cross-validation\n    def train_loso(self):\n        # Load data\n        self.X, self.y, self.subjects = self.get_source_data()\n\n        logo = LeaveOneGroupOut()\n        device = self.device\n\n        test_accuracies = []\n        subject_ids = []\n        fold_count = 0\n\n        print(\"\\n####################\")\n        print(\"Traininig Started\")\n        print(\"####################\")\n        \n        for train_idx, test_idx in logo.split(self.X, self.y, groups=self.subjects):\n            fold_count += 1\n            heldout_subj = self.subjects[test_idx[0]]\n            training_subjects = np.unique(self.subjects[train_idx])\n            # Convert to 1-based indexing for display\n            training_subjects_1based = [s + 1 for s in training_subjects]\n            test_subj_1based = heldout_subj + 1\n            print(f\"\\n===== Fold {fold_count}\")\n            print(f\"===== Seed : {42+fold_count}\")\n            print(f\"===== Training Subject : {', '.join(map(str, training_subjects_1based))}\")\n            print(f\"===== Test Subject : {test_subj_1based}\\n\")\n\n            # Re-seed per fold for reproducibility\n            set_seed(42 + fold_count)\n\n            X_train_full, X_test = self.X[train_idx], self.X[test_idx]\n            y_train_full, y_test = self.y[train_idx], self.y[test_idx]\n\n            # Further split training data into training and validation (e.g., 90-10)\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_full, y_train_full, test_size=0.1,\n                stratify=y_train_full, random_state=42 + fold_count\n            )\n\n            # Create datasets\n            train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=True)\n            val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n            test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n\n            # Create dataloaders\n            train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n            test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n\n            # Define optimizer and scheduler\n            optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                             factor=0.5, patience=5, verbose=True)\n\n            best_val_acc = 0.0\n            patience_counter = 0\n            best_model_state = None\n\n            for epoch in range(self.n_epochs):\n                # Training Phase\n                self.model.train()\n                running_loss = 0.0\n                correct = 0\n                total = 0\n\n                for img, label in train_loader:\n                    img = img.to(self.device)  # (B, 1, 3, 321)\n                    label = label.to(self.device)  # (B,)\n\n                    optimizer.zero_grad()\n                    outputs = self.model(img)  # (B, num_classes)\n                    loss = self.criterion_cls(outputs, label)\n                    loss.backward()\n                    optimizer.step()\n\n                    running_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    correct += (preds == label).sum().item()\n                    total += label.size(0)\n\n                train_loss = running_loss / total\n                train_acc = correct / total\n\n                # Validation Phase\n                self.model.eval()\n                val_loss = 0.0\n                val_correct = 0\n                val_total = 0\n\n                with torch.no_grad():\n                    for img, label in val_loader:\n                        img = img.to(self.device)\n                        label = label.to(self.device)\n\n                        outputs = self.model(img)\n                        loss = self.criterion_cls(outputs, label)\n\n                        val_loss += loss.item() * img.size(0)\n                        _, preds = torch.max(outputs, 1)\n                        val_correct += (preds == label).sum().item()\n                        val_total += label.size(0)\n\n                val_loss /= val_total\n                val_acc = val_correct / val_total\n\n                # Step scheduler\n                scheduler.step(val_loss)\n\n                print(f'Epoch {epoch+1}/{self.n_epochs} | '\n                      f'Training Loss : {train_loss:.4f} | Training Accuracy : {train_acc*100:.2f} % | '\n                      f'Validation Loss : {val_loss:.4f} | Validation Accuracy : {val_acc*100:.2f} %')\n\n                # Early Stopping\n                if val_acc > best_val_acc:\n                    best_val_acc = val_acc\n                    best_model_state = self.model.state_dict()\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    # if patience_counter >= self.patience:\n                    #     print(f\"\\n=== Early Stopping Triggered at Epoch {epoch+1}\\n\")\n                    #     break\n\n            # Load best model\n            if best_model_state is not None:\n                self.model.load_state_dict(best_model_state)\n\n            # Testing Phase\n            self.model.eval()\n            test_loss = 0.0\n            test_correct = 0\n            test_total = 0\n            all_preds = []\n            all_labels = []\n\n            with torch.no_grad():\n                for img, label in test_loader:\n                    img = img.to(self.device)\n                    label = label.to(self.device)\n\n                    outputs = self.model(img)\n                    loss = self.criterion_cls(outputs, label)\n\n                    test_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    test_correct += (preds == label).sum().item()\n                    test_total += label.size(0)\n\n                    all_preds.append(preds.cpu().numpy())\n                    all_labels.append(label.cpu().numpy())\n\n            test_loss /= test_total\n            test_acc = test_correct / test_total\n\n            all_preds = np.concatenate(all_preds)\n            all_labels = np.concatenate(all_labels)\n\n            print(f'Test Subject : {test_subj_1based} | Test Loss : {test_loss:.4f} | Test Accuracy : {test_acc*100:.2f} %')\n            print(\"\\n################################\")\n\n            test_accuracies.append(test_acc)\n            subject_ids.append(heldout_subj)\n\n        # Summary of LOSO\n        avg_test_acc = np.mean(test_accuracies) * 100\n        \n        print(\"\\n================\")\n        print(\"LOSO Summary\")\n        print(\"================\")\n        for i, sid in enumerate(subject_ids):\n            print(f\"Subject {sid} -> Test Accuracy : {test_accuracies[i]*100:.2f} %\")\n        print(\"\\n-----------------------------\")\n        print(f\"Average Test Accuracy : {avg_test_acc:.2f} %\")\n        print(\"-----------------------------\")\n\n###############################################################################\n# 12) Main Execution\n###############################################################################\ndef main():\n    best = 0\n    aver = 0\n\n    # Path to the pickle file\n    pickle_path = '/kaggle/input/bcic-iv/2b.pickle'\n\n    # Initialize and train the model\n    exp = HybridExP(nsub=1, pickle_path=pickle_path, device='cuda:0')  # 'nsub' will be managed by LOSO\n    exp.train_loso()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T06:12:09.394408Z","iopub.execute_input":"2025-10-14T06:12:09.394759Z","iopub.status.idle":"2025-10-14T07:52:20.287817Z","shell.execute_reply.started":"2025-10-14T06:12:09.394730Z","shell.execute_reply":"2025-10-14T07:52:20.286387Z"}},"outputs":[{"name":"stdout","text":"Setting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\nSetting up band-pass filter from 7 - 30 Hz\n\nFIR filter parameters\n---------------------\nDesigning a one-pass, zero-phase, non-causal bandpass filter:\n- Windowed time-domain design (firwin) method\n- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n- Lower passband edge: 7.00\n- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n- Upper passband edge: 30.00 Hz\n- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n- Filter length: 133 samples (1.663 sec)\n\n\n####################\nTraininig Started\n####################\n\n===== Fold 1\n===== Seed : 43\n===== Training Subject : 2, 3, 4, 5, 6, 7, 8, 9\n===== Test Subject : 1\n\nEpoch 1/100 | Training Loss : 0.6555 | Training Accuracy : 59.06 % | Validation Loss : 0.5871 | Validation Accuracy : 65.00 %\nEpoch 2/100 | Training Loss : 0.5870 | Training Accuracy : 66.92 % | Validation Loss : 0.5228 | Validation Accuracy : 73.62 %\nEpoch 3/100 | Training Loss : 0.5515 | Training Accuracy : 70.44 % | Validation Loss : 0.5212 | Validation Accuracy : 72.24 %\nEpoch 4/100 | Training Loss : 0.5409 | Training Accuracy : 71.11 % | Validation Loss : 0.5151 | Validation Accuracy : 72.41 %\nEpoch 5/100 | Training Loss : 0.5120 | Training Accuracy : 73.28 % | Validation Loss : 0.5094 | Validation Accuracy : 73.79 %\nEpoch 6/100 | Training Loss : 0.5063 | Training Accuracy : 74.16 % | Validation Loss : 0.4864 | Validation Accuracy : 72.24 %\nEpoch 7/100 | Training Loss : 0.5056 | Training Accuracy : 73.68 % | Validation Loss : 0.5009 | Validation Accuracy : 72.76 %\nEpoch 8/100 | Training Loss : 0.4995 | Training Accuracy : 73.95 % | Validation Loss : 0.5077 | Validation Accuracy : 73.45 %\nEpoch 9/100 | Training Loss : 0.4974 | Training Accuracy : 74.60 % | Validation Loss : 0.4828 | Validation Accuracy : 71.55 %\nEpoch 10/100 | Training Loss : 0.4943 | Training Accuracy : 74.85 % | Validation Loss : 0.5275 | Validation Accuracy : 69.31 %\nEpoch 11/100 | Training Loss : 0.4925 | Training Accuracy : 73.74 % | Validation Loss : 0.4798 | Validation Accuracy : 76.03 %\nEpoch 12/100 | Training Loss : 0.4854 | Training Accuracy : 74.31 % | Validation Loss : 0.4717 | Validation Accuracy : 73.62 %\nEpoch 13/100 | Training Loss : 0.4832 | Training Accuracy : 74.69 % | Validation Loss : 0.4746 | Validation Accuracy : 74.83 %\nEpoch 14/100 | Training Loss : 0.4834 | Training Accuracy : 74.54 % | Validation Loss : 0.4918 | Validation Accuracy : 74.66 %\nEpoch 15/100 | Training Loss : 0.4815 | Training Accuracy : 75.63 % | Validation Loss : 0.4933 | Validation Accuracy : 72.93 %\nEpoch 16/100 | Training Loss : 0.4771 | Training Accuracy : 75.40 % | Validation Loss : 0.4716 | Validation Accuracy : 75.34 %\nEpoch 17/100 | Training Loss : 0.4722 | Training Accuracy : 75.54 % | Validation Loss : 0.4786 | Validation Accuracy : 73.45 %\nEpoch 18/100 | Training Loss : 0.4721 | Training Accuracy : 75.25 % | Validation Loss : 0.4721 | Validation Accuracy : 74.83 %\nEpoch 19/100 | Training Loss : 0.4752 | Training Accuracy : 75.17 % | Validation Loss : 0.4784 | Validation Accuracy : 73.97 %\nEpoch 20/100 | Training Loss : 0.4687 | Training Accuracy : 76.34 % | Validation Loss : 0.4727 | Validation Accuracy : 73.79 %\nEpoch 21/100 | Training Loss : 0.4690 | Training Accuracy : 75.52 % | Validation Loss : 0.4752 | Validation Accuracy : 74.31 %\nEpoch 22/100 | Training Loss : 0.4742 | Training Accuracy : 74.87 % | Validation Loss : 0.4889 | Validation Accuracy : 75.34 %\nEpoch 23/100 | Training Loss : 0.4620 | Training Accuracy : 76.07 % | Validation Loss : 0.4746 | Validation Accuracy : 75.86 %\nEpoch 24/100 | Training Loss : 0.4615 | Training Accuracy : 76.28 % | Validation Loss : 0.4671 | Validation Accuracy : 76.03 %\nEpoch 25/100 | Training Loss : 0.4559 | Training Accuracy : 76.46 % | Validation Loss : 0.4737 | Validation Accuracy : 73.62 %\nEpoch 26/100 | Training Loss : 0.4572 | Training Accuracy : 76.65 % | Validation Loss : 0.4687 | Validation Accuracy : 75.69 %\nEpoch 27/100 | Training Loss : 0.4590 | Training Accuracy : 76.51 % | Validation Loss : 0.4713 | Validation Accuracy : 73.10 %\nEpoch 28/100 | Training Loss : 0.4557 | Training Accuracy : 76.59 % | Validation Loss : 0.4673 | Validation Accuracy : 74.31 %\nEpoch 29/100 | Training Loss : 0.4575 | Training Accuracy : 75.84 % | Validation Loss : 0.4662 | Validation Accuracy : 75.86 %\nEpoch 30/100 | Training Loss : 0.4598 | Training Accuracy : 75.73 % | Validation Loss : 0.4670 | Validation Accuracy : 75.86 %\nEpoch 31/100 | Training Loss : 0.4598 | Training Accuracy : 75.96 % | Validation Loss : 0.4803 | Validation Accuracy : 74.31 %\nEpoch 32/100 | Training Loss : 0.4644 | Training Accuracy : 75.96 % | Validation Loss : 0.4708 | Validation Accuracy : 75.69 %\nEpoch 33/100 | Training Loss : 0.4558 | Training Accuracy : 76.63 % | Validation Loss : 0.4714 | Validation Accuracy : 75.34 %\nEpoch 34/100 | Training Loss : 0.4606 | Training Accuracy : 76.63 % | Validation Loss : 0.4693 | Validation Accuracy : 75.00 %\nEpoch 35/100 | Training Loss : 0.4626 | Training Accuracy : 75.71 % | Validation Loss : 0.4721 | Validation Accuracy : 74.31 %\nEpoch 36/100 | Training Loss : 0.4480 | Training Accuracy : 76.63 % | Validation Loss : 0.4692 | Validation Accuracy : 74.83 %\nEpoch 37/100 | Training Loss : 0.4566 | Training Accuracy : 76.13 % | Validation Loss : 0.4710 | Validation Accuracy : 74.14 %\nEpoch 38/100 | Training Loss : 0.4516 | Training Accuracy : 76.32 % | Validation Loss : 0.4712 | Validation Accuracy : 74.83 %\nEpoch 39/100 | Training Loss : 0.4511 | Training Accuracy : 76.34 % | Validation Loss : 0.4703 | Validation Accuracy : 76.72 %\nEpoch 40/100 | Training Loss : 0.4530 | Training Accuracy : 76.97 % | Validation Loss : 0.4670 | Validation Accuracy : 76.38 %\nEpoch 41/100 | Training Loss : 0.4549 | Training Accuracy : 75.98 % | Validation Loss : 0.4726 | Validation Accuracy : 75.34 %\nEpoch 42/100 | Training Loss : 0.4503 | Training Accuracy : 75.98 % | Validation Loss : 0.4703 | Validation Accuracy : 75.52 %\nEpoch 43/100 | Training Loss : 0.4526 | Training Accuracy : 76.78 % | Validation Loss : 0.4667 | Validation Accuracy : 75.86 %\nEpoch 44/100 | Training Loss : 0.4430 | Training Accuracy : 77.38 % | Validation Loss : 0.4667 | Validation Accuracy : 75.86 %\nEpoch 45/100 | Training Loss : 0.4528 | Training Accuracy : 76.65 % | Validation Loss : 0.4666 | Validation Accuracy : 75.34 %\nEpoch 46/100 | Training Loss : 0.4411 | Training Accuracy : 77.82 % | Validation Loss : 0.4674 | Validation Accuracy : 75.52 %\nEpoch 47/100 | Training Loss : 0.4412 | Training Accuracy : 77.66 % | Validation Loss : 0.4666 | Validation Accuracy : 76.90 %\nEpoch 48/100 | Training Loss : 0.4502 | Training Accuracy : 76.19 % | Validation Loss : 0.4661 | Validation Accuracy : 77.07 %\nEpoch 49/100 | Training Loss : 0.4469 | Training Accuracy : 77.28 % | Validation Loss : 0.4652 | Validation Accuracy : 75.17 %\nEpoch 50/100 | Training Loss : 0.4479 | Training Accuracy : 77.38 % | Validation Loss : 0.4688 | Validation Accuracy : 74.31 %\nEpoch 51/100 | Training Loss : 0.4419 | Training Accuracy : 77.28 % | Validation Loss : 0.4659 | Validation Accuracy : 76.03 %\nEpoch 52/100 | Training Loss : 0.4436 | Training Accuracy : 77.43 % | Validation Loss : 0.4676 | Validation Accuracy : 75.00 %\nEpoch 53/100 | Training Loss : 0.4428 | Training Accuracy : 76.80 % | Validation Loss : 0.4690 | Validation Accuracy : 75.52 %\nEpoch 54/100 | Training Loss : 0.4412 | Training Accuracy : 76.82 % | Validation Loss : 0.4693 | Validation Accuracy : 75.00 %\nEpoch 55/100 | Training Loss : 0.4425 | Training Accuracy : 77.18 % | Validation Loss : 0.4686 | Validation Accuracy : 76.03 %\nEpoch 56/100 | Training Loss : 0.4401 | Training Accuracy : 76.95 % | Validation Loss : 0.4679 | Validation Accuracy : 75.86 %\nEpoch 57/100 | Training Loss : 0.4432 | Training Accuracy : 76.90 % | Validation Loss : 0.4692 | Validation Accuracy : 76.55 %\nEpoch 58/100 | Training Loss : 0.4419 | Training Accuracy : 76.57 % | Validation Loss : 0.4679 | Validation Accuracy : 76.38 %\nEpoch 59/100 | Training Loss : 0.4423 | Training Accuracy : 76.63 % | Validation Loss : 0.4680 | Validation Accuracy : 75.34 %\nEpoch 60/100 | Training Loss : 0.4406 | Training Accuracy : 77.11 % | Validation Loss : 0.4665 | Validation Accuracy : 75.86 %\nEpoch 61/100 | Training Loss : 0.4420 | Training Accuracy : 77.30 % | Validation Loss : 0.4697 | Validation Accuracy : 75.69 %\nEpoch 62/100 | Training Loss : 0.4454 | Training Accuracy : 76.92 % | Validation Loss : 0.4668 | Validation Accuracy : 76.38 %\nEpoch 63/100 | Training Loss : 0.4367 | Training Accuracy : 76.99 % | Validation Loss : 0.4663 | Validation Accuracy : 76.21 %\nEpoch 64/100 | Training Loss : 0.4480 | Training Accuracy : 76.84 % | Validation Loss : 0.4672 | Validation Accuracy : 75.34 %\nEpoch 65/100 | Training Loss : 0.4431 | Training Accuracy : 76.70 % | Validation Loss : 0.4691 | Validation Accuracy : 75.86 %\nEpoch 66/100 | Training Loss : 0.4417 | Training Accuracy : 76.72 % | Validation Loss : 0.4676 | Validation Accuracy : 76.21 %\nEpoch 67/100 | Training Loss : 0.4457 | Training Accuracy : 77.20 % | Validation Loss : 0.4658 | Validation Accuracy : 76.03 %\nEpoch 68/100 | Training Loss : 0.4442 | Training Accuracy : 76.38 % | Validation Loss : 0.4686 | Validation Accuracy : 76.38 %\nEpoch 69/100 | Training Loss : 0.4364 | Training Accuracy : 77.47 % | Validation Loss : 0.4664 | Validation Accuracy : 76.38 %\nEpoch 70/100 | Training Loss : 0.4417 | Training Accuracy : 77.49 % | Validation Loss : 0.4656 | Validation Accuracy : 76.72 %\nEpoch 71/100 | Training Loss : 0.4435 | Training Accuracy : 76.92 % | Validation Loss : 0.4659 | Validation Accuracy : 75.69 %\nEpoch 72/100 | Training Loss : 0.4431 | Training Accuracy : 76.92 % | Validation Loss : 0.4653 | Validation Accuracy : 76.21 %\nEpoch 73/100 | Training Loss : 0.4390 | Training Accuracy : 77.32 % | Validation Loss : 0.4685 | Validation Accuracy : 76.03 %\nEpoch 74/100 | Training Loss : 0.4410 | Training Accuracy : 76.48 % | Validation Loss : 0.4663 | Validation Accuracy : 76.38 %\nEpoch 75/100 | Training Loss : 0.4358 | Training Accuracy : 77.45 % | Validation Loss : 0.4676 | Validation Accuracy : 75.52 %\nEpoch 76/100 | Training Loss : 0.4433 | Training Accuracy : 77.16 % | Validation Loss : 0.4660 | Validation Accuracy : 76.38 %\nEpoch 77/100 | Training Loss : 0.4485 | Training Accuracy : 76.69 % | Validation Loss : 0.4660 | Validation Accuracy : 76.38 %\nEpoch 78/100 | Training Loss : 0.4376 | Training Accuracy : 77.36 % | Validation Loss : 0.4657 | Validation Accuracy : 76.21 %\nEpoch 79/100 | Training Loss : 0.4398 | Training Accuracy : 77.11 % | Validation Loss : 0.4674 | Validation Accuracy : 75.69 %\nEpoch 80/100 | Training Loss : 0.4377 | Training Accuracy : 78.16 % | Validation Loss : 0.4665 | Validation Accuracy : 75.69 %\nEpoch 81/100 | Training Loss : 0.4467 | Training Accuracy : 77.13 % | Validation Loss : 0.4662 | Validation Accuracy : 76.03 %\nEpoch 82/100 | Training Loss : 0.4439 | Training Accuracy : 76.84 % | Validation Loss : 0.4655 | Validation Accuracy : 76.55 %\nEpoch 83/100 | Training Loss : 0.4380 | Training Accuracy : 77.74 % | Validation Loss : 0.4677 | Validation Accuracy : 74.48 %\nEpoch 84/100 | Training Loss : 0.4483 | Training Accuracy : 76.34 % | Validation Loss : 0.4674 | Validation Accuracy : 76.38 %\nEpoch 85/100 | Training Loss : 0.4378 | Training Accuracy : 76.69 % | Validation Loss : 0.4674 | Validation Accuracy : 75.52 %\nEpoch 86/100 | Training Loss : 0.4424 | Training Accuracy : 76.97 % | Validation Loss : 0.4678 | Validation Accuracy : 76.38 %\nEpoch 87/100 | Training Loss : 0.4322 | Training Accuracy : 78.10 % | Validation Loss : 0.4667 | Validation Accuracy : 76.38 %\nEpoch 88/100 | Training Loss : 0.4404 | Training Accuracy : 78.01 % | Validation Loss : 0.4664 | Validation Accuracy : 76.55 %\nEpoch 89/100 | Training Loss : 0.4398 | Training Accuracy : 77.28 % | Validation Loss : 0.4662 | Validation Accuracy : 76.21 %\nEpoch 90/100 | Training Loss : 0.4380 | Training Accuracy : 77.26 % | Validation Loss : 0.4659 | Validation Accuracy : 76.55 %\nEpoch 91/100 | Training Loss : 0.4447 | Training Accuracy : 76.92 % | Validation Loss : 0.4673 | Validation Accuracy : 76.55 %\nEpoch 92/100 | Training Loss : 0.4434 | Training Accuracy : 76.93 % | Validation Loss : 0.4664 | Validation Accuracy : 76.55 %\nEpoch 93/100 | Training Loss : 0.4409 | Training Accuracy : 77.36 % | Validation Loss : 0.4674 | Validation Accuracy : 75.69 %\nEpoch 94/100 | Training Loss : 0.4431 | Training Accuracy : 76.92 % | Validation Loss : 0.4670 | Validation Accuracy : 76.55 %\nEpoch 95/100 | Training Loss : 0.4439 | Training Accuracy : 77.43 % | Validation Loss : 0.4660 | Validation Accuracy : 76.38 %\nEpoch 96/100 | Training Loss : 0.4375 | Training Accuracy : 77.76 % | Validation Loss : 0.4664 | Validation Accuracy : 76.21 %\nEpoch 97/100 | Training Loss : 0.4389 | Training Accuracy : 77.01 % | Validation Loss : 0.4664 | Validation Accuracy : 75.00 %\nEpoch 98/100 | Training Loss : 0.4368 | Training Accuracy : 77.68 % | Validation Loss : 0.4678 | Validation Accuracy : 76.72 %\nEpoch 99/100 | Training Loss : 0.4343 | Training Accuracy : 77.32 % | Validation Loss : 0.4690 | Validation Accuracy : 76.03 %\nEpoch 100/100 | Training Loss : 0.4394 | Training Accuracy : 76.74 % | Validation Loss : 0.4688 | Validation Accuracy : 76.21 %\nTest Subject : 1 | Test Loss : 0.5404 | Test Accuracy : 70.14 %\n\n################################\n\n===== Fold 2\n===== Seed : 44\n===== Training Subject : 1, 3, 4, 5, 6, 7, 8, 9\n===== Test Subject : 2\n\nEpoch 1/100 | Training Loss : 0.4493 | Training Accuracy : 77.40 % | Validation Loss : 0.4042 | Validation Accuracy : 78.08 %\nEpoch 2/100 | Training Loss : 0.4540 | Training Accuracy : 76.88 % | Validation Loss : 0.4028 | Validation Accuracy : 77.05 %\nEpoch 3/100 | Training Loss : 0.4474 | Training Accuracy : 77.38 % | Validation Loss : 0.4246 | Validation Accuracy : 78.42 %\nEpoch 4/100 | Training Loss : 0.4398 | Training Accuracy : 78.46 % | Validation Loss : 0.4233 | Validation Accuracy : 76.71 %\nEpoch 5/100 | Training Loss : 0.4447 | Training Accuracy : 77.89 % | Validation Loss : 0.4145 | Validation Accuracy : 79.97 %\nEpoch 6/100 | Training Loss : 0.4392 | Training Accuracy : 78.33 % | Validation Loss : 0.4213 | Validation Accuracy : 77.74 %\nEpoch 7/100 | Training Loss : 0.4410 | Training Accuracy : 78.04 % | Validation Loss : 0.4043 | Validation Accuracy : 78.77 %\nEpoch 8/100 | Training Loss : 0.4333 | Training Accuracy : 78.56 % | Validation Loss : 0.4160 | Validation Accuracy : 78.08 %\nEpoch 9/100 | Training Loss : 0.4253 | Training Accuracy : 79.26 % | Validation Loss : 0.4030 | Validation Accuracy : 79.79 %\nEpoch 10/100 | Training Loss : 0.4287 | Training Accuracy : 78.71 % | Validation Loss : 0.4004 | Validation Accuracy : 78.77 %\nEpoch 11/100 | Training Loss : 0.4256 | Training Accuracy : 79.09 % | Validation Loss : 0.4123 | Validation Accuracy : 78.08 %\nEpoch 12/100 | Training Loss : 0.4213 | Training Accuracy : 79.26 % | Validation Loss : 0.4011 | Validation Accuracy : 79.11 %\nEpoch 13/100 | Training Loss : 0.4300 | Training Accuracy : 77.93 % | Validation Loss : 0.4003 | Validation Accuracy : 78.77 %\nEpoch 14/100 | Training Loss : 0.4229 | Training Accuracy : 78.86 % | Validation Loss : 0.3968 | Validation Accuracy : 78.42 %\nEpoch 15/100 | Training Loss : 0.4314 | Training Accuracy : 78.60 % | Validation Loss : 0.4009 | Validation Accuracy : 78.94 %\nEpoch 16/100 | Training Loss : 0.4252 | Training Accuracy : 79.36 % | Validation Loss : 0.3989 | Validation Accuracy : 78.77 %\nEpoch 17/100 | Training Loss : 0.4173 | Training Accuracy : 79.81 % | Validation Loss : 0.3922 | Validation Accuracy : 79.11 %\nEpoch 18/100 | Training Loss : 0.4263 | Training Accuracy : 78.58 % | Validation Loss : 0.4170 | Validation Accuracy : 78.08 %\nEpoch 19/100 | Training Loss : 0.4247 | Training Accuracy : 78.71 % | Validation Loss : 0.4013 | Validation Accuracy : 79.79 %\nEpoch 20/100 | Training Loss : 0.4209 | Training Accuracy : 79.41 % | Validation Loss : 0.4003 | Validation Accuracy : 78.60 %\nEpoch 21/100 | Training Loss : 0.4233 | Training Accuracy : 78.60 % | Validation Loss : 0.4044 | Validation Accuracy : 79.79 %\nEpoch 22/100 | Training Loss : 0.4285 | Training Accuracy : 78.81 % | Validation Loss : 0.4304 | Validation Accuracy : 78.08 %\nEpoch 23/100 | Training Loss : 0.4171 | Training Accuracy : 79.15 % | Validation Loss : 0.3989 | Validation Accuracy : 78.77 %\nEpoch 24/100 | Training Loss : 0.4172 | Training Accuracy : 79.34 % | Validation Loss : 0.3970 | Validation Accuracy : 79.79 %\nEpoch 25/100 | Training Loss : 0.4216 | Training Accuracy : 78.71 % | Validation Loss : 0.3969 | Validation Accuracy : 79.11 %\nEpoch 26/100 | Training Loss : 0.4066 | Training Accuracy : 80.18 % | Validation Loss : 0.3924 | Validation Accuracy : 80.65 %\nEpoch 27/100 | Training Loss : 0.4109 | Training Accuracy : 79.45 % | Validation Loss : 0.4051 | Validation Accuracy : 79.62 %\nEpoch 28/100 | Training Loss : 0.4145 | Training Accuracy : 79.64 % | Validation Loss : 0.3976 | Validation Accuracy : 79.62 %\nEpoch 29/100 | Training Loss : 0.4117 | Training Accuracy : 80.57 % | Validation Loss : 0.3935 | Validation Accuracy : 80.31 %\nEpoch 30/100 | Training Loss : 0.4131 | Training Accuracy : 80.29 % | Validation Loss : 0.3936 | Validation Accuracy : 81.16 %\nEpoch 31/100 | Training Loss : 0.4151 | Training Accuracy : 79.95 % | Validation Loss : 0.3940 | Validation Accuracy : 80.14 %\nEpoch 32/100 | Training Loss : 0.4046 | Training Accuracy : 80.19 % | Validation Loss : 0.3988 | Validation Accuracy : 79.79 %\nEpoch 33/100 | Training Loss : 0.4079 | Training Accuracy : 80.16 % | Validation Loss : 0.3986 | Validation Accuracy : 79.79 %\nEpoch 34/100 | Training Loss : 0.4125 | Training Accuracy : 79.38 % | Validation Loss : 0.3916 | Validation Accuracy : 79.97 %\nEpoch 35/100 | Training Loss : 0.4073 | Training Accuracy : 79.49 % | Validation Loss : 0.3977 | Validation Accuracy : 79.28 %\nEpoch 36/100 | Training Loss : 0.4142 | Training Accuracy : 79.59 % | Validation Loss : 0.4001 | Validation Accuracy : 80.65 %\nEpoch 37/100 | Training Loss : 0.4130 | Training Accuracy : 79.17 % | Validation Loss : 0.3971 | Validation Accuracy : 79.97 %\nEpoch 38/100 | Training Loss : 0.4114 | Training Accuracy : 79.30 % | Validation Loss : 0.3983 | Validation Accuracy : 80.31 %\nEpoch 39/100 | Training Loss : 0.4045 | Training Accuracy : 80.46 % | Validation Loss : 0.4035 | Validation Accuracy : 79.62 %\nEpoch 40/100 | Training Loss : 0.4106 | Training Accuracy : 79.91 % | Validation Loss : 0.3997 | Validation Accuracy : 80.48 %\nEpoch 41/100 | Training Loss : 0.4050 | Training Accuracy : 80.18 % | Validation Loss : 0.3915 | Validation Accuracy : 80.48 %\nEpoch 42/100 | Training Loss : 0.4118 | Training Accuracy : 79.49 % | Validation Loss : 0.3931 | Validation Accuracy : 80.65 %\nEpoch 43/100 | Training Loss : 0.4048 | Training Accuracy : 79.79 % | Validation Loss : 0.4010 | Validation Accuracy : 80.48 %\nEpoch 44/100 | Training Loss : 0.4110 | Training Accuracy : 79.87 % | Validation Loss : 0.3982 | Validation Accuracy : 80.14 %\nEpoch 45/100 | Training Loss : 0.4076 | Training Accuracy : 80.02 % | Validation Loss : 0.3924 | Validation Accuracy : 80.48 %\nEpoch 46/100 | Training Loss : 0.3983 | Training Accuracy : 80.56 % | Validation Loss : 0.3905 | Validation Accuracy : 80.82 %\nEpoch 47/100 | Training Loss : 0.3985 | Training Accuracy : 80.42 % | Validation Loss : 0.3934 | Validation Accuracy : 80.82 %\nEpoch 48/100 | Training Loss : 0.4031 | Training Accuracy : 80.71 % | Validation Loss : 0.3934 | Validation Accuracy : 81.34 %\nEpoch 49/100 | Training Loss : 0.4033 | Training Accuracy : 80.16 % | Validation Loss : 0.3948 | Validation Accuracy : 80.99 %\nEpoch 50/100 | Training Loss : 0.4037 | Training Accuracy : 79.91 % | Validation Loss : 0.3918 | Validation Accuracy : 80.65 %\nEpoch 51/100 | Training Loss : 0.4061 | Training Accuracy : 80.02 % | Validation Loss : 0.3952 | Validation Accuracy : 80.65 %\nEpoch 52/100 | Training Loss : 0.4071 | Training Accuracy : 80.23 % | Validation Loss : 0.3990 | Validation Accuracy : 79.79 %\nEpoch 53/100 | Training Loss : 0.4072 | Training Accuracy : 80.00 % | Validation Loss : 0.3907 | Validation Accuracy : 80.82 %\nEpoch 54/100 | Training Loss : 0.4134 | Training Accuracy : 79.39 % | Validation Loss : 0.4034 | Validation Accuracy : 79.97 %\nEpoch 55/100 | Training Loss : 0.4064 | Training Accuracy : 79.62 % | Validation Loss : 0.3959 | Validation Accuracy : 81.51 %\nEpoch 56/100 | Training Loss : 0.4015 | Training Accuracy : 80.35 % | Validation Loss : 0.3923 | Validation Accuracy : 80.99 %\nEpoch 57/100 | Training Loss : 0.4045 | Training Accuracy : 79.66 % | Validation Loss : 0.3930 | Validation Accuracy : 80.99 %\nEpoch 58/100 | Training Loss : 0.4012 | Training Accuracy : 80.18 % | Validation Loss : 0.3955 | Validation Accuracy : 80.99 %\nEpoch 59/100 | Training Loss : 0.4047 | Training Accuracy : 80.10 % | Validation Loss : 0.3928 | Validation Accuracy : 80.99 %\nEpoch 60/100 | Training Loss : 0.4088 | Training Accuracy : 79.98 % | Validation Loss : 0.3928 | Validation Accuracy : 80.99 %\nEpoch 61/100 | Training Loss : 0.4052 | Training Accuracy : 79.68 % | Validation Loss : 0.3949 | Validation Accuracy : 81.34 %\nEpoch 62/100 | Training Loss : 0.3985 | Training Accuracy : 80.48 % | Validation Loss : 0.3908 | Validation Accuracy : 80.48 %\nEpoch 63/100 | Training Loss : 0.4004 | Training Accuracy : 80.57 % | Validation Loss : 0.3939 | Validation Accuracy : 80.82 %\nEpoch 64/100 | Training Loss : 0.4012 | Training Accuracy : 80.78 % | Validation Loss : 0.3904 | Validation Accuracy : 80.82 %\nEpoch 65/100 | Training Loss : 0.4009 | Training Accuracy : 80.35 % | Validation Loss : 0.3940 | Validation Accuracy : 81.34 %\nEpoch 66/100 | Training Loss : 0.4003 | Training Accuracy : 80.27 % | Validation Loss : 0.3909 | Validation Accuracy : 80.99 %\nEpoch 67/100 | Training Loss : 0.3995 | Training Accuracy : 81.07 % | Validation Loss : 0.3912 | Validation Accuracy : 80.82 %\nEpoch 68/100 | Training Loss : 0.4070 | Training Accuracy : 80.38 % | Validation Loss : 0.3923 | Validation Accuracy : 80.65 %\nEpoch 69/100 | Training Loss : 0.3992 | Training Accuracy : 80.33 % | Validation Loss : 0.3919 | Validation Accuracy : 81.16 %\nEpoch 70/100 | Training Loss : 0.4023 | Training Accuracy : 79.93 % | Validation Loss : 0.3933 | Validation Accuracy : 81.16 %\nEpoch 71/100 | Training Loss : 0.4014 | Training Accuracy : 80.44 % | Validation Loss : 0.3914 | Validation Accuracy : 80.82 %\nEpoch 72/100 | Training Loss : 0.4040 | Training Accuracy : 79.97 % | Validation Loss : 0.3953 | Validation Accuracy : 80.99 %\nEpoch 73/100 | Training Loss : 0.3982 | Training Accuracy : 80.54 % | Validation Loss : 0.3919 | Validation Accuracy : 80.99 %\nEpoch 74/100 | Training Loss : 0.4030 | Training Accuracy : 79.72 % | Validation Loss : 0.3909 | Validation Accuracy : 80.82 %\nEpoch 75/100 | Training Loss : 0.3973 | Training Accuracy : 80.52 % | Validation Loss : 0.3958 | Validation Accuracy : 81.16 %\nEpoch 76/100 | Training Loss : 0.4023 | Training Accuracy : 79.97 % | Validation Loss : 0.3929 | Validation Accuracy : 80.99 %\nEpoch 77/100 | Training Loss : 0.4000 | Training Accuracy : 80.40 % | Validation Loss : 0.3940 | Validation Accuracy : 81.34 %\nEpoch 78/100 | Training Loss : 0.3990 | Training Accuracy : 80.46 % | Validation Loss : 0.3932 | Validation Accuracy : 81.51 %\nEpoch 79/100 | Training Loss : 0.4078 | Training Accuracy : 79.68 % | Validation Loss : 0.3994 | Validation Accuracy : 80.14 %\nEpoch 80/100 | Training Loss : 0.3915 | Training Accuracy : 80.86 % | Validation Loss : 0.3898 | Validation Accuracy : 80.99 %\nEpoch 81/100 | Training Loss : 0.3996 | Training Accuracy : 80.94 % | Validation Loss : 0.3921 | Validation Accuracy : 81.16 %\nEpoch 82/100 | Training Loss : 0.4082 | Training Accuracy : 80.19 % | Validation Loss : 0.3915 | Validation Accuracy : 80.48 %\nEpoch 83/100 | Training Loss : 0.4058 | Training Accuracy : 79.20 % | Validation Loss : 0.3918 | Validation Accuracy : 80.82 %\nEpoch 84/100 | Training Loss : 0.4028 | Training Accuracy : 80.27 % | Validation Loss : 0.3899 | Validation Accuracy : 80.82 %\nEpoch 85/100 | Training Loss : 0.4032 | Training Accuracy : 80.31 % | Validation Loss : 0.3921 | Validation Accuracy : 81.16 %\nEpoch 86/100 | Training Loss : 0.4042 | Training Accuracy : 80.31 % | Validation Loss : 0.3915 | Validation Accuracy : 80.99 %\nEpoch 87/100 | Training Loss : 0.3988 | Training Accuracy : 80.29 % | Validation Loss : 0.3909 | Validation Accuracy : 80.31 %\nEpoch 88/100 | Training Loss : 0.4039 | Training Accuracy : 80.06 % | Validation Loss : 0.3919 | Validation Accuracy : 80.99 %\nEpoch 89/100 | Training Loss : 0.4148 | Training Accuracy : 79.24 % | Validation Loss : 0.3924 | Validation Accuracy : 80.99 %\nEpoch 90/100 | Training Loss : 0.4076 | Training Accuracy : 79.51 % | Validation Loss : 0.3907 | Validation Accuracy : 80.48 %\nEpoch 91/100 | Training Loss : 0.4029 | Training Accuracy : 80.19 % | Validation Loss : 0.3919 | Validation Accuracy : 80.99 %\nEpoch 92/100 | Training Loss : 0.4038 | Training Accuracy : 80.82 % | Validation Loss : 0.3927 | Validation Accuracy : 80.99 %\nEpoch 93/100 | Training Loss : 0.4004 | Training Accuracy : 80.06 % | Validation Loss : 0.3944 | Validation Accuracy : 81.34 %\nEpoch 94/100 | Training Loss : 0.4057 | Training Accuracy : 80.18 % | Validation Loss : 0.3917 | Validation Accuracy : 80.99 %\nEpoch 95/100 | Training Loss : 0.4031 | Training Accuracy : 79.98 % | Validation Loss : 0.3948 | Validation Accuracy : 81.34 %\nEpoch 96/100 | Training Loss : 0.4077 | Training Accuracy : 79.47 % | Validation Loss : 0.3921 | Validation Accuracy : 81.34 %\nEpoch 97/100 | Training Loss : 0.4015 | Training Accuracy : 79.85 % | Validation Loss : 0.3898 | Validation Accuracy : 80.65 %\nEpoch 98/100 | Training Loss : 0.4042 | Training Accuracy : 80.31 % | Validation Loss : 0.3953 | Validation Accuracy : 81.16 %\nEpoch 99/100 | Training Loss : 0.4035 | Training Accuracy : 80.50 % | Validation Loss : 0.3943 | Validation Accuracy : 80.99 %\nEpoch 100/100 | Training Loss : 0.4081 | Training Accuracy : 80.38 % | Validation Loss : 0.3917 | Validation Accuracy : 81.51 %\nTest Subject : 2 | Test Loss : 0.6965 | Test Accuracy : 58.53 %\n\n################################\n\n===== Fold 3\n===== Seed : 45\n===== Training Subject : 1, 2, 4, 5, 6, 7, 8, 9\n===== Test Subject : 3\n\nEpoch 1/100 | Training Loss : 0.4252 | Training Accuracy : 78.97 % | Validation Loss : 0.3936 | Validation Accuracy : 82.24 %\nEpoch 2/100 | Training Loss : 0.4256 | Training Accuracy : 79.02 % | Validation Loss : 0.3864 | Validation Accuracy : 81.03 %\nEpoch 3/100 | Training Loss : 0.4213 | Training Accuracy : 79.08 % | Validation Loss : 0.3976 | Validation Accuracy : 81.03 %\nEpoch 4/100 | Training Loss : 0.4227 | Training Accuracy : 79.33 % | Validation Loss : 0.3975 | Validation Accuracy : 81.21 %\nEpoch 5/100 | Training Loss : 0.4227 | Training Accuracy : 79.48 % | Validation Loss : 0.3878 | Validation Accuracy : 81.38 %\nEpoch 6/100 | Training Loss : 0.4117 | Training Accuracy : 79.89 % | Validation Loss : 0.3805 | Validation Accuracy : 82.59 %\nEpoch 7/100 | Training Loss : 0.4246 | Training Accuracy : 79.48 % | Validation Loss : 0.3870 | Validation Accuracy : 81.38 %\nEpoch 8/100 | Training Loss : 0.4106 | Training Accuracy : 80.21 % | Validation Loss : 0.3861 | Validation Accuracy : 82.59 %\nEpoch 9/100 | Training Loss : 0.4214 | Training Accuracy : 79.21 % | Validation Loss : 0.3811 | Validation Accuracy : 82.41 %\nEpoch 10/100 | Training Loss : 0.4182 | Training Accuracy : 79.44 % | Validation Loss : 0.3889 | Validation Accuracy : 80.52 %\nEpoch 11/100 | Training Loss : 0.4140 | Training Accuracy : 79.94 % | Validation Loss : 0.3845 | Validation Accuracy : 82.24 %\nEpoch 12/100 | Training Loss : 0.4104 | Training Accuracy : 79.67 % | Validation Loss : 0.4016 | Validation Accuracy : 82.76 %\nEpoch 13/100 | Training Loss : 0.3990 | Training Accuracy : 81.07 % | Validation Loss : 0.3792 | Validation Accuracy : 81.90 %\nEpoch 14/100 | Training Loss : 0.4082 | Training Accuracy : 80.50 % | Validation Loss : 0.3808 | Validation Accuracy : 82.24 %\nEpoch 15/100 | Training Loss : 0.3914 | Training Accuracy : 81.44 % | Validation Loss : 0.3763 | Validation Accuracy : 82.07 %\nEpoch 16/100 | Training Loss : 0.3985 | Training Accuracy : 80.48 % | Validation Loss : 0.3779 | Validation Accuracy : 82.76 %\nEpoch 17/100 | Training Loss : 0.4038 | Training Accuracy : 80.38 % | Validation Loss : 0.3808 | Validation Accuracy : 82.93 %\nEpoch 18/100 | Training Loss : 0.3962 | Training Accuracy : 80.65 % | Validation Loss : 0.3729 | Validation Accuracy : 82.59 %\nEpoch 19/100 | Training Loss : 0.3900 | Training Accuracy : 81.42 % | Validation Loss : 0.3864 | Validation Accuracy : 81.21 %\nEpoch 20/100 | Training Loss : 0.3882 | Training Accuracy : 80.77 % | Validation Loss : 0.3869 | Validation Accuracy : 81.21 %\nEpoch 21/100 | Training Loss : 0.4095 | Training Accuracy : 79.67 % | Validation Loss : 0.3854 | Validation Accuracy : 81.03 %\nEpoch 22/100 | Training Loss : 0.3891 | Training Accuracy : 81.69 % | Validation Loss : 0.3745 | Validation Accuracy : 81.90 %\nEpoch 23/100 | Training Loss : 0.3913 | Training Accuracy : 81.61 % | Validation Loss : 0.3940 | Validation Accuracy : 79.66 %\nEpoch 24/100 | Training Loss : 0.3969 | Training Accuracy : 80.69 % | Validation Loss : 0.3784 | Validation Accuracy : 82.41 %\nEpoch 25/100 | Training Loss : 0.3938 | Training Accuracy : 81.25 % | Validation Loss : 0.3838 | Validation Accuracy : 80.52 %\nEpoch 26/100 | Training Loss : 0.3878 | Training Accuracy : 81.53 % | Validation Loss : 0.3809 | Validation Accuracy : 82.07 %\nEpoch 27/100 | Training Loss : 0.3880 | Training Accuracy : 81.69 % | Validation Loss : 0.3777 | Validation Accuracy : 82.93 %\nEpoch 28/100 | Training Loss : 0.3861 | Training Accuracy : 81.42 % | Validation Loss : 0.3771 | Validation Accuracy : 82.59 %\nEpoch 29/100 | Training Loss : 0.3877 | Training Accuracy : 80.75 % | Validation Loss : 0.3822 | Validation Accuracy : 80.86 %\nEpoch 30/100 | Training Loss : 0.3904 | Training Accuracy : 81.25 % | Validation Loss : 0.3737 | Validation Accuracy : 83.28 %\nEpoch 31/100 | Training Loss : 0.3826 | Training Accuracy : 81.51 % | Validation Loss : 0.3801 | Validation Accuracy : 82.07 %\nEpoch 32/100 | Training Loss : 0.3823 | Training Accuracy : 81.84 % | Validation Loss : 0.3755 | Validation Accuracy : 83.62 %\nEpoch 33/100 | Training Loss : 0.3794 | Training Accuracy : 82.32 % | Validation Loss : 0.3724 | Validation Accuracy : 83.45 %\nEpoch 34/100 | Training Loss : 0.3871 | Training Accuracy : 81.93 % | Validation Loss : 0.3751 | Validation Accuracy : 83.45 %\nEpoch 35/100 | Training Loss : 0.3861 | Training Accuracy : 81.67 % | Validation Loss : 0.3730 | Validation Accuracy : 83.45 %\nEpoch 36/100 | Training Loss : 0.3858 | Training Accuracy : 81.28 % | Validation Loss : 0.3725 | Validation Accuracy : 83.45 %\nEpoch 37/100 | Training Loss : 0.3751 | Training Accuracy : 81.97 % | Validation Loss : 0.3765 | Validation Accuracy : 83.28 %\nEpoch 38/100 | Training Loss : 0.3736 | Training Accuracy : 82.32 % | Validation Loss : 0.3732 | Validation Accuracy : 83.28 %\nEpoch 39/100 | Training Loss : 0.3799 | Training Accuracy : 81.80 % | Validation Loss : 0.3792 | Validation Accuracy : 81.90 %\nEpoch 40/100 | Training Loss : 0.3821 | Training Accuracy : 81.80 % | Validation Loss : 0.3743 | Validation Accuracy : 83.10 %\nEpoch 41/100 | Training Loss : 0.3896 | Training Accuracy : 81.21 % | Validation Loss : 0.3753 | Validation Accuracy : 82.93 %\nEpoch 42/100 | Training Loss : 0.3762 | Training Accuracy : 81.53 % | Validation Loss : 0.3751 | Validation Accuracy : 82.76 %\nEpoch 43/100 | Training Loss : 0.3798 | Training Accuracy : 81.74 % | Validation Loss : 0.3763 | Validation Accuracy : 81.72 %\nEpoch 44/100 | Training Loss : 0.3798 | Training Accuracy : 81.70 % | Validation Loss : 0.3722 | Validation Accuracy : 82.59 %\nEpoch 45/100 | Training Loss : 0.3809 | Training Accuracy : 81.70 % | Validation Loss : 0.3744 | Validation Accuracy : 83.10 %\nEpoch 46/100 | Training Loss : 0.3772 | Training Accuracy : 81.53 % | Validation Loss : 0.3726 | Validation Accuracy : 83.45 %\nEpoch 47/100 | Training Loss : 0.3760 | Training Accuracy : 81.59 % | Validation Loss : 0.3737 | Validation Accuracy : 82.59 %\nEpoch 48/100 | Training Loss : 0.3806 | Training Accuracy : 81.57 % | Validation Loss : 0.3708 | Validation Accuracy : 83.62 %\nEpoch 49/100 | Training Loss : 0.3825 | Training Accuracy : 81.13 % | Validation Loss : 0.3730 | Validation Accuracy : 83.10 %\nEpoch 50/100 | Training Loss : 0.3806 | Training Accuracy : 81.38 % | Validation Loss : 0.3725 | Validation Accuracy : 82.93 %\nEpoch 51/100 | Training Loss : 0.3852 | Training Accuracy : 81.46 % | Validation Loss : 0.3732 | Validation Accuracy : 83.10 %\nEpoch 52/100 | Training Loss : 0.3831 | Training Accuracy : 81.67 % | Validation Loss : 0.3724 | Validation Accuracy : 83.45 %\nEpoch 53/100 | Training Loss : 0.3784 | Training Accuracy : 81.74 % | Validation Loss : 0.3699 | Validation Accuracy : 82.93 %\nEpoch 54/100 | Training Loss : 0.3670 | Training Accuracy : 82.66 % | Validation Loss : 0.3792 | Validation Accuracy : 81.55 %\nEpoch 55/100 | Training Loss : 0.3728 | Training Accuracy : 82.18 % | Validation Loss : 0.3743 | Validation Accuracy : 82.59 %\nEpoch 56/100 | Training Loss : 0.3777 | Training Accuracy : 81.44 % | Validation Loss : 0.3775 | Validation Accuracy : 82.41 %\nEpoch 57/100 | Training Loss : 0.3741 | Training Accuracy : 81.65 % | Validation Loss : 0.3753 | Validation Accuracy : 82.59 %\nEpoch 58/100 | Training Loss : 0.3739 | Training Accuracy : 82.15 % | Validation Loss : 0.3719 | Validation Accuracy : 83.45 %\nEpoch 59/100 | Training Loss : 0.3775 | Training Accuracy : 81.44 % | Validation Loss : 0.3717 | Validation Accuracy : 82.76 %\nEpoch 60/100 | Training Loss : 0.3816 | Training Accuracy : 82.01 % | Validation Loss : 0.3758 | Validation Accuracy : 81.72 %\nEpoch 61/100 | Training Loss : 0.3731 | Training Accuracy : 82.05 % | Validation Loss : 0.3738 | Validation Accuracy : 82.24 %\nEpoch 62/100 | Training Loss : 0.3842 | Training Accuracy : 81.59 % | Validation Loss : 0.3745 | Validation Accuracy : 82.24 %\nEpoch 63/100 | Training Loss : 0.3851 | Training Accuracy : 81.28 % | Validation Loss : 0.3719 | Validation Accuracy : 82.76 %\nEpoch 64/100 | Training Loss : 0.3717 | Training Accuracy : 81.88 % | Validation Loss : 0.3741 | Validation Accuracy : 83.45 %\nEpoch 65/100 | Training Loss : 0.3787 | Training Accuracy : 81.76 % | Validation Loss : 0.3730 | Validation Accuracy : 83.10 %\nEpoch 66/100 | Training Loss : 0.3808 | Training Accuracy : 81.78 % | Validation Loss : 0.3731 | Validation Accuracy : 82.59 %\nEpoch 67/100 | Training Loss : 0.3674 | Training Accuracy : 82.74 % | Validation Loss : 0.3725 | Validation Accuracy : 82.93 %\nEpoch 68/100 | Training Loss : 0.3824 | Training Accuracy : 81.57 % | Validation Loss : 0.3764 | Validation Accuracy : 81.90 %\nEpoch 69/100 | Training Loss : 0.3770 | Training Accuracy : 81.57 % | Validation Loss : 0.3747 | Validation Accuracy : 82.41 %\nEpoch 70/100 | Training Loss : 0.3790 | Training Accuracy : 81.40 % | Validation Loss : 0.3729 | Validation Accuracy : 82.76 %\nEpoch 71/100 | Training Loss : 0.3790 | Training Accuracy : 81.90 % | Validation Loss : 0.3764 | Validation Accuracy : 82.24 %\nEpoch 72/100 | Training Loss : 0.3735 | Training Accuracy : 81.99 % | Validation Loss : 0.3721 | Validation Accuracy : 82.59 %\nEpoch 73/100 | Training Loss : 0.3743 | Training Accuracy : 81.70 % | Validation Loss : 0.3751 | Validation Accuracy : 82.76 %\nEpoch 74/100 | Training Loss : 0.3819 | Training Accuracy : 81.44 % | Validation Loss : 0.3758 | Validation Accuracy : 82.59 %\nEpoch 75/100 | Training Loss : 0.3792 | Training Accuracy : 81.88 % | Validation Loss : 0.3725 | Validation Accuracy : 83.10 %\nEpoch 76/100 | Training Loss : 0.3707 | Training Accuracy : 82.13 % | Validation Loss : 0.3768 | Validation Accuracy : 81.90 %\nEpoch 77/100 | Training Loss : 0.3815 | Training Accuracy : 81.48 % | Validation Loss : 0.3768 | Validation Accuracy : 81.90 %\nEpoch 78/100 | Training Loss : 0.3767 | Training Accuracy : 81.76 % | Validation Loss : 0.3722 | Validation Accuracy : 83.10 %\nEpoch 79/100 | Training Loss : 0.3778 | Training Accuracy : 81.76 % | Validation Loss : 0.3717 | Validation Accuracy : 82.76 %\nEpoch 80/100 | Training Loss : 0.3728 | Training Accuracy : 82.28 % | Validation Loss : 0.3740 | Validation Accuracy : 82.24 %\nEpoch 81/100 | Training Loss : 0.3748 | Training Accuracy : 81.95 % | Validation Loss : 0.3715 | Validation Accuracy : 82.76 %\nEpoch 82/100 | Training Loss : 0.3762 | Training Accuracy : 81.74 % | Validation Loss : 0.3720 | Validation Accuracy : 83.10 %\nEpoch 83/100 | Training Loss : 0.3734 | Training Accuracy : 81.65 % | Validation Loss : 0.3745 | Validation Accuracy : 82.41 %\nEpoch 84/100 | Training Loss : 0.3753 | Training Accuracy : 82.82 % | Validation Loss : 0.3730 | Validation Accuracy : 83.10 %\nEpoch 85/100 | Training Loss : 0.3743 | Training Accuracy : 81.95 % | Validation Loss : 0.3747 | Validation Accuracy : 82.76 %\nEpoch 86/100 | Training Loss : 0.3809 | Training Accuracy : 81.80 % | Validation Loss : 0.3724 | Validation Accuracy : 82.76 %\nEpoch 87/100 | Training Loss : 0.3742 | Training Accuracy : 82.28 % | Validation Loss : 0.3726 | Validation Accuracy : 83.45 %\nEpoch 88/100 | Training Loss : 0.3735 | Training Accuracy : 81.93 % | Validation Loss : 0.3719 | Validation Accuracy : 83.62 %\nEpoch 89/100 | Training Loss : 0.3717 | Training Accuracy : 82.13 % | Validation Loss : 0.3743 | Validation Accuracy : 82.93 %\nEpoch 90/100 | Training Loss : 0.3827 | Training Accuracy : 81.76 % | Validation Loss : 0.3717 | Validation Accuracy : 82.41 %\nEpoch 91/100 | Training Loss : 0.3725 | Training Accuracy : 81.97 % | Validation Loss : 0.3713 | Validation Accuracy : 83.10 %\nEpoch 92/100 | Training Loss : 0.3738 | Training Accuracy : 82.13 % | Validation Loss : 0.3728 | Validation Accuracy : 82.59 %\nEpoch 93/100 | Training Loss : 0.3768 | Training Accuracy : 81.97 % | Validation Loss : 0.3719 | Validation Accuracy : 82.93 %\nEpoch 94/100 | Training Loss : 0.3841 | Training Accuracy : 81.78 % | Validation Loss : 0.3719 | Validation Accuracy : 82.76 %\nEpoch 95/100 | Training Loss : 0.3835 | Training Accuracy : 81.55 % | Validation Loss : 0.3718 | Validation Accuracy : 82.93 %\nEpoch 96/100 | Training Loss : 0.3797 | Training Accuracy : 82.20 % | Validation Loss : 0.3725 | Validation Accuracy : 83.28 %\nEpoch 97/100 | Training Loss : 0.3769 | Training Accuracy : 81.36 % | Validation Loss : 0.3749 | Validation Accuracy : 82.59 %\nEpoch 98/100 | Training Loss : 0.3778 | Training Accuracy : 82.36 % | Validation Loss : 0.3718 | Validation Accuracy : 82.59 %\nEpoch 99/100 | Training Loss : 0.3752 | Training Accuracy : 82.01 % | Validation Loss : 0.3725 | Validation Accuracy : 83.10 %\nEpoch 100/100 | Training Loss : 0.3761 | Training Accuracy : 81.86 % | Validation Loss : 0.3732 | Validation Accuracy : 82.76 %\nTest Subject : 3 | Test Loss : 0.7961 | Test Accuracy : 58.47 %\n\n################################\n\n===== Fold 4\n===== Seed : 46\n===== Training Subject : 1, 2, 3, 5, 6, 7, 8, 9\n===== Test Subject : 4\n\nEpoch 1/100 | Training Loss : 0.4791 | Training Accuracy : 74.66 % | Validation Loss : 0.3942 | Validation Accuracy : 80.80 %\nEpoch 2/100 | Training Loss : 0.4703 | Training Accuracy : 76.43 % | Validation Loss : 0.4203 | Validation Accuracy : 79.07 %\nEpoch 3/100 | Training Loss : 0.4639 | Training Accuracy : 75.99 % | Validation Loss : 0.4222 | Validation Accuracy : 78.72 %\nEpoch 4/100 | Training Loss : 0.4632 | Training Accuracy : 76.62 % | Validation Loss : 0.4161 | Validation Accuracy : 79.93 %\nEpoch 5/100 | Training Loss : 0.4572 | Training Accuracy : 75.64 % | Validation Loss : 0.4175 | Validation Accuracy : 78.20 %\nEpoch 6/100 | Training Loss : 0.4602 | Training Accuracy : 76.41 % | Validation Loss : 0.4208 | Validation Accuracy : 77.51 %\nEpoch 7/100 | Training Loss : 0.4563 | Training Accuracy : 76.87 % | Validation Loss : 0.4201 | Validation Accuracy : 79.58 %\nEpoch 8/100 | Training Loss : 0.4525 | Training Accuracy : 76.30 % | Validation Loss : 0.4204 | Validation Accuracy : 79.24 %\nEpoch 9/100 | Training Loss : 0.4535 | Training Accuracy : 76.68 % | Validation Loss : 0.4161 | Validation Accuracy : 79.93 %\nEpoch 10/100 | Training Loss : 0.4458 | Training Accuracy : 77.93 % | Validation Loss : 0.4126 | Validation Accuracy : 79.24 %\nEpoch 11/100 | Training Loss : 0.4527 | Training Accuracy : 76.84 % | Validation Loss : 0.4300 | Validation Accuracy : 78.72 %\nEpoch 12/100 | Training Loss : 0.4527 | Training Accuracy : 76.68 % | Validation Loss : 0.4168 | Validation Accuracy : 79.76 %\nEpoch 13/100 | Training Loss : 0.4478 | Training Accuracy : 77.24 % | Validation Loss : 0.4068 | Validation Accuracy : 79.76 %\nEpoch 14/100 | Training Loss : 0.4406 | Training Accuracy : 76.89 % | Validation Loss : 0.4084 | Validation Accuracy : 79.24 %\nEpoch 15/100 | Training Loss : 0.4440 | Training Accuracy : 77.99 % | Validation Loss : 0.4094 | Validation Accuracy : 79.41 %\nEpoch 16/100 | Training Loss : 0.4488 | Training Accuracy : 76.76 % | Validation Loss : 0.4100 | Validation Accuracy : 79.41 %\nEpoch 17/100 | Training Loss : 0.4464 | Training Accuracy : 77.16 % | Validation Loss : 0.4085 | Validation Accuracy : 78.89 %\nEpoch 18/100 | Training Loss : 0.4447 | Training Accuracy : 77.28 % | Validation Loss : 0.4077 | Validation Accuracy : 79.41 %\nEpoch 19/100 | Training Loss : 0.4414 | Training Accuracy : 77.14 % | Validation Loss : 0.4087 | Validation Accuracy : 79.07 %\nEpoch 20/100 | Training Loss : 0.4391 | Training Accuracy : 77.14 % | Validation Loss : 0.4076 | Validation Accuracy : 80.10 %\nEpoch 21/100 | Training Loss : 0.4427 | Training Accuracy : 77.34 % | Validation Loss : 0.4109 | Validation Accuracy : 79.93 %\nEpoch 22/100 | Training Loss : 0.4349 | Training Accuracy : 77.05 % | Validation Loss : 0.4096 | Validation Accuracy : 79.93 %\nEpoch 23/100 | Training Loss : 0.4350 | Training Accuracy : 78.66 % | Validation Loss : 0.4099 | Validation Accuracy : 79.76 %\nEpoch 24/100 | Training Loss : 0.4419 | Training Accuracy : 76.59 % | Validation Loss : 0.4094 | Validation Accuracy : 78.72 %\nEpoch 25/100 | Training Loss : 0.4373 | Training Accuracy : 78.09 % | Validation Loss : 0.4061 | Validation Accuracy : 79.07 %\nEpoch 26/100 | Training Loss : 0.4400 | Training Accuracy : 77.26 % | Validation Loss : 0.4066 | Validation Accuracy : 79.58 %\nEpoch 27/100 | Training Loss : 0.4410 | Training Accuracy : 77.47 % | Validation Loss : 0.4072 | Validation Accuracy : 79.07 %\nEpoch 28/100 | Training Loss : 0.4448 | Training Accuracy : 77.39 % | Validation Loss : 0.4093 | Validation Accuracy : 79.93 %\nEpoch 29/100 | Training Loss : 0.4330 | Training Accuracy : 78.20 % | Validation Loss : 0.4050 | Validation Accuracy : 79.76 %\nEpoch 30/100 | Training Loss : 0.4352 | Training Accuracy : 78.12 % | Validation Loss : 0.4059 | Validation Accuracy : 80.10 %\nEpoch 31/100 | Training Loss : 0.4311 | Training Accuracy : 78.47 % | Validation Loss : 0.4062 | Validation Accuracy : 79.76 %\nEpoch 32/100 | Training Loss : 0.4380 | Training Accuracy : 77.03 % | Validation Loss : 0.4066 | Validation Accuracy : 79.58 %\nEpoch 33/100 | Training Loss : 0.4320 | Training Accuracy : 77.66 % | Validation Loss : 0.4102 | Validation Accuracy : 80.28 %\nEpoch 34/100 | Training Loss : 0.4292 | Training Accuracy : 77.72 % | Validation Loss : 0.4087 | Validation Accuracy : 79.76 %\nEpoch 35/100 | Training Loss : 0.4322 | Training Accuracy : 77.18 % | Validation Loss : 0.4088 | Validation Accuracy : 79.58 %\nEpoch 36/100 | Training Loss : 0.4378 | Training Accuracy : 78.24 % | Validation Loss : 0.4118 | Validation Accuracy : 79.58 %\nEpoch 37/100 | Training Loss : 0.4375 | Training Accuracy : 77.76 % | Validation Loss : 0.4080 | Validation Accuracy : 79.24 %\nEpoch 38/100 | Training Loss : 0.4329 | Training Accuracy : 78.32 % | Validation Loss : 0.4125 | Validation Accuracy : 79.41 %\nEpoch 39/100 | Training Loss : 0.4323 | Training Accuracy : 78.18 % | Validation Loss : 0.4078 | Validation Accuracy : 79.76 %\nEpoch 40/100 | Training Loss : 0.4355 | Training Accuracy : 78.14 % | Validation Loss : 0.4089 | Validation Accuracy : 79.76 %\nEpoch 41/100 | Training Loss : 0.4315 | Training Accuracy : 78.03 % | Validation Loss : 0.4090 | Validation Accuracy : 79.24 %\nEpoch 42/100 | Training Loss : 0.4388 | Training Accuracy : 77.57 % | Validation Loss : 0.4087 | Validation Accuracy : 79.93 %\nEpoch 43/100 | Training Loss : 0.4337 | Training Accuracy : 78.12 % | Validation Loss : 0.4121 | Validation Accuracy : 79.24 %\nEpoch 44/100 | Training Loss : 0.4309 | Training Accuracy : 78.28 % | Validation Loss : 0.4104 | Validation Accuracy : 79.41 %\nEpoch 45/100 | Training Loss : 0.4333 | Training Accuracy : 78.43 % | Validation Loss : 0.4079 | Validation Accuracy : 79.24 %\nEpoch 46/100 | Training Loss : 0.4296 | Training Accuracy : 78.43 % | Validation Loss : 0.4059 | Validation Accuracy : 79.07 %\nEpoch 47/100 | Training Loss : 0.4337 | Training Accuracy : 77.62 % | Validation Loss : 0.4100 | Validation Accuracy : 80.10 %\nEpoch 48/100 | Training Loss : 0.4275 | Training Accuracy : 78.43 % | Validation Loss : 0.4103 | Validation Accuracy : 79.76 %\nEpoch 49/100 | Training Loss : 0.4361 | Training Accuracy : 77.95 % | Validation Loss : 0.4057 | Validation Accuracy : 79.41 %\nEpoch 50/100 | Training Loss : 0.4357 | Training Accuracy : 77.84 % | Validation Loss : 0.4086 | Validation Accuracy : 79.93 %\nEpoch 51/100 | Training Loss : 0.4336 | Training Accuracy : 78.64 % | Validation Loss : 0.4116 | Validation Accuracy : 79.41 %\nEpoch 52/100 | Training Loss : 0.4410 | Training Accuracy : 77.43 % | Validation Loss : 0.4059 | Validation Accuracy : 79.41 %\nEpoch 53/100 | Training Loss : 0.4314 | Training Accuracy : 78.12 % | Validation Loss : 0.4058 | Validation Accuracy : 79.76 %\nEpoch 54/100 | Training Loss : 0.4302 | Training Accuracy : 77.85 % | Validation Loss : 0.4073 | Validation Accuracy : 79.58 %\nEpoch 55/100 | Training Loss : 0.4304 | Training Accuracy : 78.07 % | Validation Loss : 0.4076 | Validation Accuracy : 80.10 %\nEpoch 56/100 | Training Loss : 0.4383 | Training Accuracy : 77.82 % | Validation Loss : 0.4063 | Validation Accuracy : 79.41 %\nEpoch 57/100 | Training Loss : 0.4402 | Training Accuracy : 77.51 % | Validation Loss : 0.4087 | Validation Accuracy : 79.93 %\nEpoch 58/100 | Training Loss : 0.4410 | Training Accuracy : 76.95 % | Validation Loss : 0.4083 | Validation Accuracy : 80.10 %\nEpoch 59/100 | Training Loss : 0.4332 | Training Accuracy : 78.20 % | Validation Loss : 0.4065 | Validation Accuracy : 79.58 %\nEpoch 60/100 | Training Loss : 0.4277 | Training Accuracy : 78.20 % | Validation Loss : 0.4060 | Validation Accuracy : 79.24 %\nEpoch 61/100 | Training Loss : 0.4375 | Training Accuracy : 77.37 % | Validation Loss : 0.4067 | Validation Accuracy : 79.07 %\nEpoch 62/100 | Training Loss : 0.4360 | Training Accuracy : 77.55 % | Validation Loss : 0.4058 | Validation Accuracy : 79.76 %\nEpoch 63/100 | Training Loss : 0.4352 | Training Accuracy : 78.16 % | Validation Loss : 0.4156 | Validation Accuracy : 79.07 %\nEpoch 64/100 | Training Loss : 0.4291 | Training Accuracy : 78.12 % | Validation Loss : 0.4053 | Validation Accuracy : 78.89 %\nEpoch 65/100 | Training Loss : 0.4344 | Training Accuracy : 78.07 % | Validation Loss : 0.4104 | Validation Accuracy : 79.58 %\nEpoch 66/100 | Training Loss : 0.4320 | Training Accuracy : 77.64 % | Validation Loss : 0.4071 | Validation Accuracy : 79.24 %\nEpoch 67/100 | Training Loss : 0.4340 | Training Accuracy : 78.12 % | Validation Loss : 0.4115 | Validation Accuracy : 79.41 %\nEpoch 68/100 | Training Loss : 0.4329 | Training Accuracy : 77.74 % | Validation Loss : 0.4110 | Validation Accuracy : 79.93 %\nEpoch 69/100 | Training Loss : 0.4306 | Training Accuracy : 78.24 % | Validation Loss : 0.4077 | Validation Accuracy : 80.28 %\nEpoch 70/100 | Training Loss : 0.4312 | Training Accuracy : 78.84 % | Validation Loss : 0.4097 | Validation Accuracy : 79.93 %\nEpoch 71/100 | Training Loss : 0.4330 | Training Accuracy : 77.82 % | Validation Loss : 0.4058 | Validation Accuracy : 79.24 %\nEpoch 72/100 | Training Loss : 0.4309 | Training Accuracy : 78.09 % | Validation Loss : 0.4048 | Validation Accuracy : 79.24 %\nEpoch 73/100 | Training Loss : 0.4273 | Training Accuracy : 78.37 % | Validation Loss : 0.4055 | Validation Accuracy : 79.24 %\nEpoch 74/100 | Training Loss : 0.4383 | Training Accuracy : 77.32 % | Validation Loss : 0.4098 | Validation Accuracy : 79.76 %\nEpoch 75/100 | Training Loss : 0.4281 | Training Accuracy : 77.64 % | Validation Loss : 0.4060 | Validation Accuracy : 79.24 %\nEpoch 76/100 | Training Loss : 0.4353 | Training Accuracy : 77.60 % | Validation Loss : 0.4054 | Validation Accuracy : 79.24 %\nEpoch 77/100 | Training Loss : 0.4308 | Training Accuracy : 78.39 % | Validation Loss : 0.4067 | Validation Accuracy : 79.24 %\nEpoch 78/100 | Training Loss : 0.4335 | Training Accuracy : 78.20 % | Validation Loss : 0.4073 | Validation Accuracy : 79.58 %\nEpoch 79/100 | Training Loss : 0.4326 | Training Accuracy : 77.85 % | Validation Loss : 0.4075 | Validation Accuracy : 79.93 %\nEpoch 80/100 | Training Loss : 0.4252 | Training Accuracy : 78.53 % | Validation Loss : 0.4087 | Validation Accuracy : 80.28 %\nEpoch 81/100 | Training Loss : 0.4278 | Training Accuracy : 78.57 % | Validation Loss : 0.4050 | Validation Accuracy : 79.07 %\nEpoch 82/100 | Training Loss : 0.4317 | Training Accuracy : 77.57 % | Validation Loss : 0.4072 | Validation Accuracy : 79.76 %\nEpoch 83/100 | Training Loss : 0.4338 | Training Accuracy : 77.57 % | Validation Loss : 0.4082 | Validation Accuracy : 79.76 %\nEpoch 84/100 | Training Loss : 0.4244 | Training Accuracy : 78.22 % | Validation Loss : 0.4060 | Validation Accuracy : 79.41 %\nEpoch 85/100 | Training Loss : 0.4366 | Training Accuracy : 77.51 % | Validation Loss : 0.4095 | Validation Accuracy : 79.93 %\nEpoch 86/100 | Training Loss : 0.4300 | Training Accuracy : 78.68 % | Validation Loss : 0.4084 | Validation Accuracy : 80.10 %\nEpoch 87/100 | Training Loss : 0.4320 | Training Accuracy : 77.91 % | Validation Loss : 0.4052 | Validation Accuracy : 79.41 %\nEpoch 88/100 | Training Loss : 0.4415 | Training Accuracy : 77.95 % | Validation Loss : 0.4064 | Validation Accuracy : 79.41 %\nEpoch 89/100 | Training Loss : 0.4360 | Training Accuracy : 78.09 % | Validation Loss : 0.4075 | Validation Accuracy : 80.10 %\nEpoch 90/100 | Training Loss : 0.4294 | Training Accuracy : 78.14 % | Validation Loss : 0.4072 | Validation Accuracy : 78.89 %\nEpoch 91/100 | Training Loss : 0.4330 | Training Accuracy : 77.91 % | Validation Loss : 0.4137 | Validation Accuracy : 79.41 %\nEpoch 92/100 | Training Loss : 0.4345 | Training Accuracy : 78.05 % | Validation Loss : 0.4057 | Validation Accuracy : 78.89 %\nEpoch 93/100 | Training Loss : 0.4360 | Training Accuracy : 77.07 % | Validation Loss : 0.4086 | Validation Accuracy : 80.10 %\nEpoch 94/100 | Training Loss : 0.4352 | Training Accuracy : 77.82 % | Validation Loss : 0.4053 | Validation Accuracy : 79.07 %\nEpoch 95/100 | Training Loss : 0.4332 | Training Accuracy : 77.64 % | Validation Loss : 0.4055 | Validation Accuracy : 79.24 %\nEpoch 96/100 | Training Loss : 0.4376 | Training Accuracy : 77.87 % | Validation Loss : 0.4065 | Validation Accuracy : 79.58 %\nEpoch 97/100 | Training Loss : 0.4332 | Training Accuracy : 78.34 % | Validation Loss : 0.4070 | Validation Accuracy : 79.24 %\nEpoch 98/100 | Training Loss : 0.4273 | Training Accuracy : 78.45 % | Validation Loss : 0.4079 | Validation Accuracy : 80.10 %\nEpoch 99/100 | Training Loss : 0.4381 | Training Accuracy : 77.35 % | Validation Loss : 0.4067 | Validation Accuracy : 79.58 %\nEpoch 100/100 | Training Loss : 0.4367 | Training Accuracy : 77.53 % | Validation Loss : 0.4144 | Validation Accuracy : 79.41 %\nTest Subject : 4 | Test Loss : 0.3210 | Test Accuracy : 86.08 %\n\n################################\n\n===== Fold 5\n===== Seed : 47\n===== Training Subject : 1, 2, 3, 4, 6, 7, 8, 9\n===== Test Subject : 5\n\nEpoch 1/100 | Training Loss : 0.4438 | Training Accuracy : 77.03 % | Validation Loss : 0.4250 | Validation Accuracy : 78.55 %\nEpoch 2/100 | Training Loss : 0.4332 | Training Accuracy : 78.35 % | Validation Loss : 0.4113 | Validation Accuracy : 78.37 %\nEpoch 3/100 | Training Loss : 0.4353 | Training Accuracy : 78.07 % | Validation Loss : 0.4178 | Validation Accuracy : 77.51 %\nEpoch 4/100 | Training Loss : 0.4375 | Training Accuracy : 77.55 % | Validation Loss : 0.4203 | Validation Accuracy : 78.37 %\nEpoch 5/100 | Training Loss : 0.4405 | Training Accuracy : 77.32 % | Validation Loss : 0.4057 | Validation Accuracy : 77.51 %\nEpoch 6/100 | Training Loss : 0.4269 | Training Accuracy : 78.18 % | Validation Loss : 0.4093 | Validation Accuracy : 78.89 %\nEpoch 7/100 | Training Loss : 0.4283 | Training Accuracy : 77.34 % | Validation Loss : 0.4064 | Validation Accuracy : 79.07 %\nEpoch 8/100 | Training Loss : 0.4253 | Training Accuracy : 78.37 % | Validation Loss : 0.4298 | Validation Accuracy : 78.03 %\nEpoch 9/100 | Training Loss : 0.4218 | Training Accuracy : 78.34 % | Validation Loss : 0.4193 | Validation Accuracy : 76.99 %\nEpoch 10/100 | Training Loss : 0.4270 | Training Accuracy : 78.32 % | Validation Loss : 0.4093 | Validation Accuracy : 79.24 %\nEpoch 11/100 | Training Loss : 0.4212 | Training Accuracy : 79.32 % | Validation Loss : 0.4112 | Validation Accuracy : 78.03 %\nEpoch 12/100 | Training Loss : 0.4215 | Training Accuracy : 78.32 % | Validation Loss : 0.4070 | Validation Accuracy : 79.07 %\nEpoch 13/100 | Training Loss : 0.4156 | Training Accuracy : 78.74 % | Validation Loss : 0.4102 | Validation Accuracy : 79.24 %\nEpoch 14/100 | Training Loss : 0.4195 | Training Accuracy : 78.68 % | Validation Loss : 0.4167 | Validation Accuracy : 78.55 %\nEpoch 15/100 | Training Loss : 0.4119 | Training Accuracy : 79.05 % | Validation Loss : 0.4025 | Validation Accuracy : 79.76 %\nEpoch 16/100 | Training Loss : 0.4089 | Training Accuracy : 78.95 % | Validation Loss : 0.4139 | Validation Accuracy : 80.10 %\nEpoch 17/100 | Training Loss : 0.4121 | Training Accuracy : 78.76 % | Validation Loss : 0.4103 | Validation Accuracy : 78.89 %\nEpoch 18/100 | Training Loss : 0.4105 | Training Accuracy : 79.35 % | Validation Loss : 0.4026 | Validation Accuracy : 79.07 %\nEpoch 19/100 | Training Loss : 0.4127 | Training Accuracy : 79.14 % | Validation Loss : 0.4016 | Validation Accuracy : 78.20 %\nEpoch 20/100 | Training Loss : 0.4150 | Training Accuracy : 79.60 % | Validation Loss : 0.4124 | Validation Accuracy : 79.24 %\nEpoch 21/100 | Training Loss : 0.4211 | Training Accuracy : 78.16 % | Validation Loss : 0.4117 | Validation Accuracy : 79.07 %\nEpoch 22/100 | Training Loss : 0.4091 | Training Accuracy : 79.53 % | Validation Loss : 0.4059 | Validation Accuracy : 79.24 %\nEpoch 23/100 | Training Loss : 0.4079 | Training Accuracy : 79.72 % | Validation Loss : 0.4058 | Validation Accuracy : 80.62 %\nEpoch 24/100 | Training Loss : 0.4077 | Training Accuracy : 79.24 % | Validation Loss : 0.4230 | Validation Accuracy : 78.03 %\nEpoch 25/100 | Training Loss : 0.4096 | Training Accuracy : 78.95 % | Validation Loss : 0.4041 | Validation Accuracy : 79.58 %\nEpoch 26/100 | Training Loss : 0.4018 | Training Accuracy : 79.55 % | Validation Loss : 0.4054 | Validation Accuracy : 79.41 %\nEpoch 27/100 | Training Loss : 0.3997 | Training Accuracy : 79.70 % | Validation Loss : 0.4051 | Validation Accuracy : 79.58 %\nEpoch 28/100 | Training Loss : 0.4029 | Training Accuracy : 79.64 % | Validation Loss : 0.4069 | Validation Accuracy : 79.24 %\nEpoch 29/100 | Training Loss : 0.3971 | Training Accuracy : 80.16 % | Validation Loss : 0.4098 | Validation Accuracy : 79.41 %\nEpoch 30/100 | Training Loss : 0.4085 | Training Accuracy : 79.62 % | Validation Loss : 0.4122 | Validation Accuracy : 79.41 %\nEpoch 31/100 | Training Loss : 0.3883 | Training Accuracy : 80.26 % | Validation Loss : 0.4078 | Validation Accuracy : 78.37 %\nEpoch 32/100 | Training Loss : 0.4005 | Training Accuracy : 79.85 % | Validation Loss : 0.4048 | Validation Accuracy : 80.45 %\nEpoch 33/100 | Training Loss : 0.3902 | Training Accuracy : 80.33 % | Validation Loss : 0.4117 | Validation Accuracy : 79.24 %\nEpoch 34/100 | Training Loss : 0.3963 | Training Accuracy : 79.55 % | Validation Loss : 0.4096 | Validation Accuracy : 79.24 %\nEpoch 35/100 | Training Loss : 0.3934 | Training Accuracy : 80.26 % | Validation Loss : 0.4081 | Validation Accuracy : 79.41 %\nEpoch 36/100 | Training Loss : 0.4008 | Training Accuracy : 79.10 % | Validation Loss : 0.4059 | Validation Accuracy : 79.93 %\nEpoch 37/100 | Training Loss : 0.3989 | Training Accuracy : 80.12 % | Validation Loss : 0.4065 | Validation Accuracy : 79.07 %\nEpoch 38/100 | Training Loss : 0.3985 | Training Accuracy : 79.60 % | Validation Loss : 0.4068 | Validation Accuracy : 79.07 %\nEpoch 39/100 | Training Loss : 0.3993 | Training Accuracy : 79.85 % | Validation Loss : 0.4033 | Validation Accuracy : 79.93 %\nEpoch 40/100 | Training Loss : 0.3938 | Training Accuracy : 80.53 % | Validation Loss : 0.4131 | Validation Accuracy : 78.72 %\nEpoch 41/100 | Training Loss : 0.3967 | Training Accuracy : 80.30 % | Validation Loss : 0.4072 | Validation Accuracy : 79.76 %\nEpoch 42/100 | Training Loss : 0.4006 | Training Accuracy : 80.28 % | Validation Loss : 0.4051 | Validation Accuracy : 79.24 %\nEpoch 43/100 | Training Loss : 0.3983 | Training Accuracy : 79.89 % | Validation Loss : 0.4099 | Validation Accuracy : 79.58 %\nEpoch 44/100 | Training Loss : 0.3922 | Training Accuracy : 79.74 % | Validation Loss : 0.4038 | Validation Accuracy : 79.24 %\nEpoch 45/100 | Training Loss : 0.3953 | Training Accuracy : 80.39 % | Validation Loss : 0.4025 | Validation Accuracy : 79.93 %\nEpoch 46/100 | Training Loss : 0.3913 | Training Accuracy : 81.01 % | Validation Loss : 0.4033 | Validation Accuracy : 80.45 %\nEpoch 47/100 | Training Loss : 0.3956 | Training Accuracy : 80.39 % | Validation Loss : 0.4079 | Validation Accuracy : 79.58 %\nEpoch 48/100 | Training Loss : 0.3908 | Training Accuracy : 80.37 % | Validation Loss : 0.4104 | Validation Accuracy : 79.24 %\nEpoch 49/100 | Training Loss : 0.3947 | Training Accuracy : 79.82 % | Validation Loss : 0.4032 | Validation Accuracy : 80.28 %\nEpoch 50/100 | Training Loss : 0.3916 | Training Accuracy : 80.57 % | Validation Loss : 0.4082 | Validation Accuracy : 79.58 %\nEpoch 51/100 | Training Loss : 0.3917 | Training Accuracy : 80.18 % | Validation Loss : 0.4083 | Validation Accuracy : 79.41 %\nEpoch 52/100 | Training Loss : 0.3936 | Training Accuracy : 80.07 % | Validation Loss : 0.4105 | Validation Accuracy : 79.24 %\nEpoch 53/100 | Training Loss : 0.3848 | Training Accuracy : 80.49 % | Validation Loss : 0.4073 | Validation Accuracy : 79.76 %\nEpoch 54/100 | Training Loss : 0.3990 | Training Accuracy : 79.58 % | Validation Loss : 0.4046 | Validation Accuracy : 79.41 %\nEpoch 55/100 | Training Loss : 0.3976 | Training Accuracy : 80.08 % | Validation Loss : 0.4050 | Validation Accuracy : 79.93 %\nEpoch 56/100 | Training Loss : 0.3912 | Training Accuracy : 79.97 % | Validation Loss : 0.4034 | Validation Accuracy : 79.58 %\nEpoch 57/100 | Training Loss : 0.3969 | Training Accuracy : 80.47 % | Validation Loss : 0.4074 | Validation Accuracy : 79.41 %\nEpoch 58/100 | Training Loss : 0.3870 | Training Accuracy : 80.87 % | Validation Loss : 0.4025 | Validation Accuracy : 80.28 %\nEpoch 59/100 | Training Loss : 0.3946 | Training Accuracy : 79.87 % | Validation Loss : 0.4060 | Validation Accuracy : 79.76 %\nEpoch 60/100 | Training Loss : 0.3913 | Training Accuracy : 80.03 % | Validation Loss : 0.4026 | Validation Accuracy : 79.93 %\nEpoch 61/100 | Training Loss : 0.3950 | Training Accuracy : 79.76 % | Validation Loss : 0.4094 | Validation Accuracy : 79.41 %\nEpoch 62/100 | Training Loss : 0.3926 | Training Accuracy : 80.41 % | Validation Loss : 0.4065 | Validation Accuracy : 79.58 %\nEpoch 63/100 | Training Loss : 0.3873 | Training Accuracy : 80.41 % | Validation Loss : 0.4113 | Validation Accuracy : 78.89 %\nEpoch 64/100 | Training Loss : 0.3847 | Training Accuracy : 81.06 % | Validation Loss : 0.4054 | Validation Accuracy : 79.93 %\nEpoch 65/100 | Training Loss : 0.3915 | Training Accuracy : 80.08 % | Validation Loss : 0.4070 | Validation Accuracy : 79.07 %\nEpoch 66/100 | Training Loss : 0.3953 | Training Accuracy : 80.24 % | Validation Loss : 0.4097 | Validation Accuracy : 79.41 %\nEpoch 67/100 | Training Loss : 0.3961 | Training Accuracy : 80.64 % | Validation Loss : 0.4144 | Validation Accuracy : 78.72 %\nEpoch 68/100 | Training Loss : 0.4016 | Training Accuracy : 79.99 % | Validation Loss : 0.4050 | Validation Accuracy : 79.93 %\nEpoch 69/100 | Training Loss : 0.3859 | Training Accuracy : 80.07 % | Validation Loss : 0.4061 | Validation Accuracy : 79.76 %\nEpoch 70/100 | Training Loss : 0.3860 | Training Accuracy : 80.49 % | Validation Loss : 0.4040 | Validation Accuracy : 79.93 %\nEpoch 71/100 | Training Loss : 0.3903 | Training Accuracy : 80.35 % | Validation Loss : 0.4064 | Validation Accuracy : 79.76 %\nEpoch 72/100 | Training Loss : 0.3944 | Training Accuracy : 79.68 % | Validation Loss : 0.4057 | Validation Accuracy : 79.76 %\nEpoch 73/100 | Training Loss : 0.3918 | Training Accuracy : 80.01 % | Validation Loss : 0.4076 | Validation Accuracy : 79.24 %\nEpoch 74/100 | Training Loss : 0.3904 | Training Accuracy : 80.47 % | Validation Loss : 0.4094 | Validation Accuracy : 79.76 %\nEpoch 75/100 | Training Loss : 0.3929 | Training Accuracy : 80.28 % | Validation Loss : 0.4033 | Validation Accuracy : 80.62 %\nEpoch 76/100 | Training Loss : 0.3990 | Training Accuracy : 79.51 % | Validation Loss : 0.4062 | Validation Accuracy : 79.58 %\nEpoch 77/100 | Training Loss : 0.3928 | Training Accuracy : 80.26 % | Validation Loss : 0.4056 | Validation Accuracy : 80.10 %\nEpoch 78/100 | Training Loss : 0.3911 | Training Accuracy : 79.85 % | Validation Loss : 0.4070 | Validation Accuracy : 80.10 %\nEpoch 79/100 | Training Loss : 0.3946 | Training Accuracy : 79.72 % | Validation Loss : 0.4042 | Validation Accuracy : 79.41 %\nEpoch 80/100 | Training Loss : 0.3953 | Training Accuracy : 79.78 % | Validation Loss : 0.4086 | Validation Accuracy : 79.24 %\nEpoch 81/100 | Training Loss : 0.3936 | Training Accuracy : 80.18 % | Validation Loss : 0.4084 | Validation Accuracy : 79.58 %\nEpoch 82/100 | Training Loss : 0.3871 | Training Accuracy : 81.03 % | Validation Loss : 0.4046 | Validation Accuracy : 79.58 %\nEpoch 83/100 | Training Loss : 0.3945 | Training Accuracy : 80.58 % | Validation Loss : 0.4077 | Validation Accuracy : 79.93 %\nEpoch 84/100 | Training Loss : 0.3929 | Training Accuracy : 80.58 % | Validation Loss : 0.4076 | Validation Accuracy : 78.89 %\nEpoch 85/100 | Training Loss : 0.3872 | Training Accuracy : 80.43 % | Validation Loss : 0.4054 | Validation Accuracy : 79.93 %\nEpoch 86/100 | Training Loss : 0.3896 | Training Accuracy : 80.55 % | Validation Loss : 0.4031 | Validation Accuracy : 80.10 %\nEpoch 87/100 | Training Loss : 0.3915 | Training Accuracy : 79.39 % | Validation Loss : 0.4061 | Validation Accuracy : 79.24 %\nEpoch 88/100 | Training Loss : 0.3881 | Training Accuracy : 80.07 % | Validation Loss : 0.4090 | Validation Accuracy : 78.72 %\nEpoch 89/100 | Training Loss : 0.3916 | Training Accuracy : 80.14 % | Validation Loss : 0.4050 | Validation Accuracy : 80.10 %\nEpoch 90/100 | Training Loss : 0.3959 | Training Accuracy : 80.03 % | Validation Loss : 0.4015 | Validation Accuracy : 80.10 %\nEpoch 91/100 | Training Loss : 0.3982 | Training Accuracy : 79.83 % | Validation Loss : 0.4102 | Validation Accuracy : 79.24 %\nEpoch 92/100 | Training Loss : 0.3937 | Training Accuracy : 80.57 % | Validation Loss : 0.4080 | Validation Accuracy : 79.41 %\nEpoch 93/100 | Training Loss : 0.3946 | Training Accuracy : 79.91 % | Validation Loss : 0.4090 | Validation Accuracy : 78.89 %\nEpoch 94/100 | Training Loss : 0.3904 | Training Accuracy : 80.57 % | Validation Loss : 0.4065 | Validation Accuracy : 79.76 %\nEpoch 95/100 | Training Loss : 0.3921 | Training Accuracy : 80.76 % | Validation Loss : 0.4051 | Validation Accuracy : 80.45 %\nEpoch 96/100 | Training Loss : 0.3909 | Training Accuracy : 79.93 % | Validation Loss : 0.4040 | Validation Accuracy : 80.10 %\nEpoch 97/100 | Training Loss : 0.3910 | Training Accuracy : 79.97 % | Validation Loss : 0.4089 | Validation Accuracy : 79.41 %\nEpoch 98/100 | Training Loss : 0.3860 | Training Accuracy : 81.12 % | Validation Loss : 0.4052 | Validation Accuracy : 79.41 %\nEpoch 99/100 | Training Loss : 0.3954 | Training Accuracy : 79.82 % | Validation Loss : 0.4044 | Validation Accuracy : 79.76 %\nEpoch 100/100 | Training Loss : 0.3848 | Training Accuracy : 80.91 % | Validation Loss : 0.4094 | Validation Accuracy : 79.24 %\nTest Subject : 5 | Test Loss : 0.3821 | Test Accuracy : 83.11 %\n\n################################\n\n===== Fold 6\n===== Seed : 48\n===== Training Subject : 1, 2, 3, 4, 5, 7, 8, 9\n===== Test Subject : 6\n\nEpoch 1/100 | Training Loss : 0.4246 | Training Accuracy : 78.12 % | Validation Loss : 0.3789 | Validation Accuracy : 83.28 %\nEpoch 2/100 | Training Loss : 0.4192 | Training Accuracy : 78.43 % | Validation Loss : 0.3711 | Validation Accuracy : 81.21 %\nEpoch 3/100 | Training Loss : 0.4127 | Training Accuracy : 78.97 % | Validation Loss : 0.3667 | Validation Accuracy : 81.38 %\nEpoch 4/100 | Training Loss : 0.4197 | Training Accuracy : 78.30 % | Validation Loss : 0.3761 | Validation Accuracy : 80.52 %\nEpoch 5/100 | Training Loss : 0.4022 | Training Accuracy : 78.77 % | Validation Loss : 0.3950 | Validation Accuracy : 79.14 %\nEpoch 6/100 | Training Loss : 0.4080 | Training Accuracy : 78.18 % | Validation Loss : 0.3875 | Validation Accuracy : 80.52 %\nEpoch 7/100 | Training Loss : 0.4093 | Training Accuracy : 78.75 % | Validation Loss : 0.3823 | Validation Accuracy : 81.03 %\nEpoch 8/100 | Training Loss : 0.4055 | Training Accuracy : 79.39 % | Validation Loss : 0.3903 | Validation Accuracy : 79.83 %\nEpoch 9/100 | Training Loss : 0.4063 | Training Accuracy : 78.58 % | Validation Loss : 0.4124 | Validation Accuracy : 80.52 %\nEpoch 10/100 | Training Loss : 0.4104 | Training Accuracy : 79.10 % | Validation Loss : 0.3768 | Validation Accuracy : 81.55 %\nEpoch 11/100 | Training Loss : 0.3926 | Training Accuracy : 80.31 % | Validation Loss : 0.3777 | Validation Accuracy : 81.21 %\nEpoch 12/100 | Training Loss : 0.3937 | Training Accuracy : 79.79 % | Validation Loss : 0.3807 | Validation Accuracy : 81.38 %\nEpoch 13/100 | Training Loss : 0.3932 | Training Accuracy : 80.34 % | Validation Loss : 0.3679 | Validation Accuracy : 81.55 %\nEpoch 14/100 | Training Loss : 0.3943 | Training Accuracy : 79.75 % | Validation Loss : 0.3791 | Validation Accuracy : 80.34 %\nEpoch 15/100 | Training Loss : 0.3868 | Training Accuracy : 80.11 % | Validation Loss : 0.3883 | Validation Accuracy : 79.83 %\nEpoch 16/100 | Training Loss : 0.3929 | Training Accuracy : 79.43 % | Validation Loss : 0.3745 | Validation Accuracy : 81.03 %\nEpoch 17/100 | Training Loss : 0.3916 | Training Accuracy : 80.23 % | Validation Loss : 0.3743 | Validation Accuracy : 80.69 %\nEpoch 18/100 | Training Loss : 0.3907 | Training Accuracy : 79.66 % | Validation Loss : 0.3671 | Validation Accuracy : 81.03 %\nEpoch 19/100 | Training Loss : 0.3803 | Training Accuracy : 80.61 % | Validation Loss : 0.3742 | Validation Accuracy : 81.38 %\nEpoch 20/100 | Training Loss : 0.3864 | Training Accuracy : 80.31 % | Validation Loss : 0.3697 | Validation Accuracy : 81.21 %\nEpoch 21/100 | Training Loss : 0.3811 | Training Accuracy : 80.40 % | Validation Loss : 0.3757 | Validation Accuracy : 80.34 %\nEpoch 22/100 | Training Loss : 0.3790 | Training Accuracy : 81.09 % | Validation Loss : 0.3675 | Validation Accuracy : 81.21 %\nEpoch 23/100 | Training Loss : 0.3776 | Training Accuracy : 80.48 % | Validation Loss : 0.3715 | Validation Accuracy : 80.69 %\nEpoch 24/100 | Training Loss : 0.3813 | Training Accuracy : 80.08 % | Validation Loss : 0.3696 | Validation Accuracy : 81.38 %\nEpoch 25/100 | Training Loss : 0.3813 | Training Accuracy : 80.88 % | Validation Loss : 0.3663 | Validation Accuracy : 81.21 %\nEpoch 26/100 | Training Loss : 0.3869 | Training Accuracy : 79.89 % | Validation Loss : 0.3692 | Validation Accuracy : 81.38 %\nEpoch 27/100 | Training Loss : 0.3799 | Training Accuracy : 80.71 % | Validation Loss : 0.3703 | Validation Accuracy : 81.38 %\nEpoch 28/100 | Training Loss : 0.3833 | Training Accuracy : 80.77 % | Validation Loss : 0.3726 | Validation Accuracy : 81.21 %\nEpoch 29/100 | Training Loss : 0.3775 | Training Accuracy : 80.88 % | Validation Loss : 0.3702 | Validation Accuracy : 80.86 %\nEpoch 30/100 | Training Loss : 0.3785 | Training Accuracy : 80.65 % | Validation Loss : 0.3660 | Validation Accuracy : 80.69 %\nEpoch 31/100 | Training Loss : 0.3770 | Training Accuracy : 81.23 % | Validation Loss : 0.3690 | Validation Accuracy : 80.17 %\nEpoch 32/100 | Training Loss : 0.3820 | Training Accuracy : 79.96 % | Validation Loss : 0.3709 | Validation Accuracy : 81.38 %\nEpoch 33/100 | Training Loss : 0.3881 | Training Accuracy : 80.17 % | Validation Loss : 0.3711 | Validation Accuracy : 80.52 %\nEpoch 34/100 | Training Loss : 0.3768 | Training Accuracy : 80.82 % | Validation Loss : 0.3710 | Validation Accuracy : 81.55 %\nEpoch 35/100 | Training Loss : 0.3842 | Training Accuracy : 80.15 % | Validation Loss : 0.3722 | Validation Accuracy : 82.24 %\nEpoch 36/100 | Training Loss : 0.3771 | Training Accuracy : 81.05 % | Validation Loss : 0.3720 | Validation Accuracy : 80.69 %\nEpoch 37/100 | Training Loss : 0.3754 | Training Accuracy : 81.07 % | Validation Loss : 0.3726 | Validation Accuracy : 80.69 %\nEpoch 38/100 | Training Loss : 0.3797 | Training Accuracy : 80.46 % | Validation Loss : 0.3679 | Validation Accuracy : 80.34 %\nEpoch 39/100 | Training Loss : 0.3757 | Training Accuracy : 80.86 % | Validation Loss : 0.3706 | Validation Accuracy : 80.86 %\nEpoch 40/100 | Training Loss : 0.3764 | Training Accuracy : 80.50 % | Validation Loss : 0.3706 | Validation Accuracy : 80.86 %\nEpoch 41/100 | Training Loss : 0.3730 | Training Accuracy : 80.88 % | Validation Loss : 0.3697 | Validation Accuracy : 82.07 %\nEpoch 42/100 | Training Loss : 0.3818 | Training Accuracy : 81.03 % | Validation Loss : 0.3697 | Validation Accuracy : 79.83 %\nEpoch 43/100 | Training Loss : 0.3837 | Training Accuracy : 80.46 % | Validation Loss : 0.3710 | Validation Accuracy : 80.34 %\nEpoch 44/100 | Training Loss : 0.3757 | Training Accuracy : 81.09 % | Validation Loss : 0.3686 | Validation Accuracy : 80.69 %\nEpoch 45/100 | Training Loss : 0.3742 | Training Accuracy : 81.13 % | Validation Loss : 0.3669 | Validation Accuracy : 81.90 %\nEpoch 46/100 | Training Loss : 0.3795 | Training Accuracy : 80.69 % | Validation Loss : 0.3696 | Validation Accuracy : 80.52 %\nEpoch 47/100 | Training Loss : 0.3720 | Training Accuracy : 80.52 % | Validation Loss : 0.3679 | Validation Accuracy : 81.21 %\nEpoch 48/100 | Training Loss : 0.3744 | Training Accuracy : 80.50 % | Validation Loss : 0.3696 | Validation Accuracy : 80.52 %\nEpoch 49/100 | Training Loss : 0.3808 | Training Accuracy : 81.07 % | Validation Loss : 0.3673 | Validation Accuracy : 80.52 %\nEpoch 50/100 | Training Loss : 0.3759 | Training Accuracy : 80.90 % | Validation Loss : 0.3706 | Validation Accuracy : 79.83 %\nEpoch 51/100 | Training Loss : 0.3709 | Training Accuracy : 81.30 % | Validation Loss : 0.3696 | Validation Accuracy : 80.17 %\nEpoch 52/100 | Training Loss : 0.3738 | Training Accuracy : 81.02 % | Validation Loss : 0.3678 | Validation Accuracy : 80.69 %\nEpoch 53/100 | Training Loss : 0.3771 | Training Accuracy : 80.48 % | Validation Loss : 0.3678 | Validation Accuracy : 80.86 %\nEpoch 54/100 | Training Loss : 0.3765 | Training Accuracy : 81.36 % | Validation Loss : 0.3678 | Validation Accuracy : 81.55 %\nEpoch 55/100 | Training Loss : 0.3759 | Training Accuracy : 81.05 % | Validation Loss : 0.3686 | Validation Accuracy : 80.69 %\nEpoch 56/100 | Training Loss : 0.3770 | Training Accuracy : 80.42 % | Validation Loss : 0.3677 | Validation Accuracy : 80.52 %\nEpoch 57/100 | Training Loss : 0.3746 | Training Accuracy : 81.34 % | Validation Loss : 0.3682 | Validation Accuracy : 80.17 %\nEpoch 58/100 | Training Loss : 0.3800 | Training Accuracy : 80.36 % | Validation Loss : 0.3695 | Validation Accuracy : 80.86 %\nEpoch 59/100 | Training Loss : 0.3828 | Training Accuracy : 80.40 % | Validation Loss : 0.3700 | Validation Accuracy : 80.52 %\nEpoch 60/100 | Training Loss : 0.3748 | Training Accuracy : 80.71 % | Validation Loss : 0.3692 | Validation Accuracy : 82.07 %\nEpoch 61/100 | Training Loss : 0.3839 | Training Accuracy : 80.75 % | Validation Loss : 0.3677 | Validation Accuracy : 80.69 %\nEpoch 62/100 | Training Loss : 0.3721 | Training Accuracy : 80.98 % | Validation Loss : 0.3688 | Validation Accuracy : 81.03 %\nEpoch 63/100 | Training Loss : 0.3744 | Training Accuracy : 80.56 % | Validation Loss : 0.3694 | Validation Accuracy : 80.34 %\nEpoch 64/100 | Training Loss : 0.3748 | Training Accuracy : 80.79 % | Validation Loss : 0.3685 | Validation Accuracy : 80.34 %\nEpoch 65/100 | Training Loss : 0.3750 | Training Accuracy : 80.59 % | Validation Loss : 0.3691 | Validation Accuracy : 80.17 %\nEpoch 66/100 | Training Loss : 0.3810 | Training Accuracy : 80.36 % | Validation Loss : 0.3707 | Validation Accuracy : 80.52 %\nEpoch 67/100 | Training Loss : 0.3755 | Training Accuracy : 80.84 % | Validation Loss : 0.3697 | Validation Accuracy : 81.03 %\nEpoch 68/100 | Training Loss : 0.3719 | Training Accuracy : 81.11 % | Validation Loss : 0.3697 | Validation Accuracy : 80.86 %\nEpoch 69/100 | Training Loss : 0.3718 | Training Accuracy : 81.13 % | Validation Loss : 0.3681 | Validation Accuracy : 81.03 %\nEpoch 70/100 | Training Loss : 0.3838 | Training Accuracy : 80.54 % | Validation Loss : 0.3684 | Validation Accuracy : 81.21 %\nEpoch 71/100 | Training Loss : 0.3732 | Training Accuracy : 80.92 % | Validation Loss : 0.3683 | Validation Accuracy : 80.69 %\nEpoch 72/100 | Training Loss : 0.3810 | Training Accuracy : 80.44 % | Validation Loss : 0.3688 | Validation Accuracy : 81.03 %\nEpoch 73/100 | Training Loss : 0.3764 | Training Accuracy : 80.63 % | Validation Loss : 0.3685 | Validation Accuracy : 80.17 %\nEpoch 74/100 | Training Loss : 0.3789 | Training Accuracy : 80.82 % | Validation Loss : 0.3684 | Validation Accuracy : 80.52 %\nEpoch 75/100 | Training Loss : 0.3726 | Training Accuracy : 80.80 % | Validation Loss : 0.3686 | Validation Accuracy : 80.34 %\nEpoch 76/100 | Training Loss : 0.3813 | Training Accuracy : 80.31 % | Validation Loss : 0.3692 | Validation Accuracy : 80.69 %\nEpoch 77/100 | Training Loss : 0.3720 | Training Accuracy : 81.48 % | Validation Loss : 0.3719 | Validation Accuracy : 82.24 %\nEpoch 78/100 | Training Loss : 0.3828 | Training Accuracy : 80.27 % | Validation Loss : 0.3693 | Validation Accuracy : 80.52 %\nEpoch 79/100 | Training Loss : 0.3795 | Training Accuracy : 80.50 % | Validation Loss : 0.3691 | Validation Accuracy : 80.00 %\nEpoch 80/100 | Training Loss : 0.3733 | Training Accuracy : 80.71 % | Validation Loss : 0.3667 | Validation Accuracy : 80.69 %\nEpoch 81/100 | Training Loss : 0.3732 | Training Accuracy : 80.71 % | Validation Loss : 0.3683 | Validation Accuracy : 80.69 %\nEpoch 82/100 | Training Loss : 0.3816 | Training Accuracy : 80.52 % | Validation Loss : 0.3691 | Validation Accuracy : 80.52 %\nEpoch 83/100 | Training Loss : 0.3771 | Training Accuracy : 80.11 % | Validation Loss : 0.3684 | Validation Accuracy : 80.86 %\nEpoch 84/100 | Training Loss : 0.3784 | Training Accuracy : 80.90 % | Validation Loss : 0.3702 | Validation Accuracy : 80.52 %\nEpoch 85/100 | Training Loss : 0.3738 | Training Accuracy : 81.42 % | Validation Loss : 0.3688 | Validation Accuracy : 81.72 %\nEpoch 86/100 | Training Loss : 0.3717 | Training Accuracy : 81.13 % | Validation Loss : 0.3682 | Validation Accuracy : 80.00 %\nEpoch 87/100 | Training Loss : 0.3744 | Training Accuracy : 80.92 % | Validation Loss : 0.3695 | Validation Accuracy : 80.69 %\nEpoch 88/100 | Training Loss : 0.3751 | Training Accuracy : 80.50 % | Validation Loss : 0.3693 | Validation Accuracy : 80.69 %\nEpoch 89/100 | Training Loss : 0.3758 | Training Accuracy : 81.21 % | Validation Loss : 0.3699 | Validation Accuracy : 80.69 %\nEpoch 90/100 | Training Loss : 0.3860 | Training Accuracy : 79.79 % | Validation Loss : 0.3690 | Validation Accuracy : 80.52 %\nEpoch 91/100 | Training Loss : 0.3773 | Training Accuracy : 80.29 % | Validation Loss : 0.3689 | Validation Accuracy : 81.38 %\nEpoch 92/100 | Training Loss : 0.3702 | Training Accuracy : 81.25 % | Validation Loss : 0.3694 | Validation Accuracy : 81.21 %\nEpoch 93/100 | Training Loss : 0.3760 | Training Accuracy : 80.94 % | Validation Loss : 0.3724 | Validation Accuracy : 81.55 %\nEpoch 94/100 | Training Loss : 0.3798 | Training Accuracy : 80.92 % | Validation Loss : 0.3684 | Validation Accuracy : 80.69 %\nEpoch 95/100 | Training Loss : 0.3774 | Training Accuracy : 80.63 % | Validation Loss : 0.3717 | Validation Accuracy : 80.00 %\nEpoch 96/100 | Training Loss : 0.3792 | Training Accuracy : 80.38 % | Validation Loss : 0.3681 | Validation Accuracy : 80.69 %\nEpoch 97/100 | Training Loss : 0.3749 | Training Accuracy : 81.02 % | Validation Loss : 0.3716 | Validation Accuracy : 80.17 %\nEpoch 98/100 | Training Loss : 0.3769 | Training Accuracy : 80.94 % | Validation Loss : 0.3702 | Validation Accuracy : 81.21 %\nEpoch 99/100 | Training Loss : 0.3752 | Training Accuracy : 81.25 % | Validation Loss : 0.3716 | Validation Accuracy : 82.41 %\nEpoch 100/100 | Training Loss : 0.3814 | Training Accuracy : 80.96 % | Validation Loss : 0.3706 | Validation Accuracy : 80.52 %\nTest Subject : 6 | Test Loss : 0.4575 | Test Accuracy : 79.17 %\n\n################################\n\n===== Fold 7\n===== Seed : 49\n===== Training Subject : 1, 2, 3, 4, 5, 6, 8, 9\n===== Test Subject : 7\n\nEpoch 1/100 | Training Loss : 0.4034 | Training Accuracy : 79.23 % | Validation Loss : 0.3494 | Validation Accuracy : 83.45 %\nEpoch 2/100 | Training Loss : 0.4041 | Training Accuracy : 80.13 % | Validation Loss : 0.3605 | Validation Accuracy : 81.03 %\nEpoch 3/100 | Training Loss : 0.3977 | Training Accuracy : 79.62 % | Validation Loss : 0.3547 | Validation Accuracy : 83.97 %\nEpoch 4/100 | Training Loss : 0.4067 | Training Accuracy : 78.98 % | Validation Loss : 0.3669 | Validation Accuracy : 81.72 %\nEpoch 5/100 | Training Loss : 0.4076 | Training Accuracy : 79.71 % | Validation Loss : 0.3723 | Validation Accuracy : 81.55 %\nEpoch 6/100 | Training Loss : 0.3900 | Training Accuracy : 80.10 % | Validation Loss : 0.3643 | Validation Accuracy : 82.07 %\nEpoch 7/100 | Training Loss : 0.3905 | Training Accuracy : 80.27 % | Validation Loss : 0.3649 | Validation Accuracy : 81.21 %\nEpoch 8/100 | Training Loss : 0.3843 | Training Accuracy : 81.17 % | Validation Loss : 0.3590 | Validation Accuracy : 82.41 %\nEpoch 9/100 | Training Loss : 0.3846 | Training Accuracy : 80.57 % | Validation Loss : 0.3630 | Validation Accuracy : 82.76 %\nEpoch 10/100 | Training Loss : 0.3778 | Training Accuracy : 79.87 % | Validation Loss : 0.3587 | Validation Accuracy : 82.93 %\nEpoch 11/100 | Training Loss : 0.3868 | Training Accuracy : 79.94 % | Validation Loss : 0.3656 | Validation Accuracy : 81.55 %\nEpoch 12/100 | Training Loss : 0.3882 | Training Accuracy : 80.42 % | Validation Loss : 0.3624 | Validation Accuracy : 81.72 %\nEpoch 13/100 | Training Loss : 0.3775 | Training Accuracy : 81.09 % | Validation Loss : 0.3650 | Validation Accuracy : 81.03 %\nEpoch 14/100 | Training Loss : 0.3718 | Training Accuracy : 81.00 % | Validation Loss : 0.3614 | Validation Accuracy : 82.76 %\nEpoch 15/100 | Training Loss : 0.3852 | Training Accuracy : 80.04 % | Validation Loss : 0.3666 | Validation Accuracy : 82.07 %\nEpoch 16/100 | Training Loss : 0.3775 | Training Accuracy : 80.75 % | Validation Loss : 0.3610 | Validation Accuracy : 82.41 %\nEpoch 17/100 | Training Loss : 0.3702 | Training Accuracy : 80.96 % | Validation Loss : 0.3614 | Validation Accuracy : 82.76 %\nEpoch 18/100 | Training Loss : 0.3699 | Training Accuracy : 81.48 % | Validation Loss : 0.3594 | Validation Accuracy : 82.07 %\nEpoch 19/100 | Training Loss : 0.3708 | Training Accuracy : 80.71 % | Validation Loss : 0.3628 | Validation Accuracy : 81.72 %\nEpoch 20/100 | Training Loss : 0.3694 | Training Accuracy : 81.26 % | Validation Loss : 0.3595 | Validation Accuracy : 81.72 %\nEpoch 21/100 | Training Loss : 0.3670 | Training Accuracy : 81.17 % | Validation Loss : 0.3583 | Validation Accuracy : 82.41 %\nEpoch 22/100 | Training Loss : 0.3632 | Training Accuracy : 81.72 % | Validation Loss : 0.3622 | Validation Accuracy : 82.76 %\nEpoch 23/100 | Training Loss : 0.3681 | Training Accuracy : 81.49 % | Validation Loss : 0.3600 | Validation Accuracy : 82.24 %\nEpoch 24/100 | Training Loss : 0.3649 | Training Accuracy : 81.42 % | Validation Loss : 0.3580 | Validation Accuracy : 82.24 %\nEpoch 25/100 | Training Loss : 0.3683 | Training Accuracy : 81.59 % | Validation Loss : 0.3606 | Validation Accuracy : 82.59 %\nEpoch 26/100 | Training Loss : 0.3671 | Training Accuracy : 81.48 % | Validation Loss : 0.3580 | Validation Accuracy : 82.59 %\nEpoch 27/100 | Training Loss : 0.3637 | Training Accuracy : 82.07 % | Validation Loss : 0.3581 | Validation Accuracy : 81.72 %\nEpoch 28/100 | Training Loss : 0.3667 | Training Accuracy : 81.63 % | Validation Loss : 0.3587 | Validation Accuracy : 82.59 %\nEpoch 29/100 | Training Loss : 0.3640 | Training Accuracy : 81.11 % | Validation Loss : 0.3621 | Validation Accuracy : 81.90 %\nEpoch 30/100 | Training Loss : 0.3686 | Training Accuracy : 81.49 % | Validation Loss : 0.3576 | Validation Accuracy : 81.55 %\nEpoch 31/100 | Training Loss : 0.3666 | Training Accuracy : 81.36 % | Validation Loss : 0.3593 | Validation Accuracy : 81.72 %\nEpoch 32/100 | Training Loss : 0.3705 | Training Accuracy : 80.86 % | Validation Loss : 0.3619 | Validation Accuracy : 82.24 %\nEpoch 33/100 | Training Loss : 0.3641 | Training Accuracy : 81.57 % | Validation Loss : 0.3604 | Validation Accuracy : 81.55 %\nEpoch 34/100 | Training Loss : 0.3674 | Training Accuracy : 81.49 % | Validation Loss : 0.3617 | Validation Accuracy : 82.07 %\nEpoch 35/100 | Training Loss : 0.3645 | Training Accuracy : 81.55 % | Validation Loss : 0.3599 | Validation Accuracy : 81.72 %\nEpoch 36/100 | Training Loss : 0.3560 | Training Accuracy : 81.59 % | Validation Loss : 0.3614 | Validation Accuracy : 82.07 %\nEpoch 37/100 | Training Loss : 0.3647 | Training Accuracy : 81.99 % | Validation Loss : 0.3613 | Validation Accuracy : 81.72 %\nEpoch 38/100 | Training Loss : 0.3714 | Training Accuracy : 81.42 % | Validation Loss : 0.3626 | Validation Accuracy : 81.90 %\nEpoch 39/100 | Training Loss : 0.3646 | Training Accuracy : 81.02 % | Validation Loss : 0.3632 | Validation Accuracy : 82.24 %\nEpoch 40/100 | Training Loss : 0.3563 | Training Accuracy : 82.20 % | Validation Loss : 0.3620 | Validation Accuracy : 82.59 %\nEpoch 41/100 | Training Loss : 0.3659 | Training Accuracy : 81.30 % | Validation Loss : 0.3610 | Validation Accuracy : 82.07 %\nEpoch 42/100 | Training Loss : 0.3686 | Training Accuracy : 81.36 % | Validation Loss : 0.3607 | Validation Accuracy : 82.07 %\nEpoch 43/100 | Training Loss : 0.3649 | Training Accuracy : 81.90 % | Validation Loss : 0.3601 | Validation Accuracy : 81.90 %\nEpoch 44/100 | Training Loss : 0.3635 | Training Accuracy : 81.59 % | Validation Loss : 0.3600 | Validation Accuracy : 81.90 %\nEpoch 45/100 | Training Loss : 0.3590 | Training Accuracy : 82.03 % | Validation Loss : 0.3605 | Validation Accuracy : 81.55 %\nEpoch 46/100 | Training Loss : 0.3596 | Training Accuracy : 82.01 % | Validation Loss : 0.3624 | Validation Accuracy : 82.24 %\nEpoch 47/100 | Training Loss : 0.3683 | Training Accuracy : 81.97 % | Validation Loss : 0.3610 | Validation Accuracy : 81.55 %\nEpoch 48/100 | Training Loss : 0.3691 | Training Accuracy : 81.19 % | Validation Loss : 0.3633 | Validation Accuracy : 82.24 %\nEpoch 49/100 | Training Loss : 0.3638 | Training Accuracy : 81.65 % | Validation Loss : 0.3603 | Validation Accuracy : 81.55 %\nEpoch 50/100 | Training Loss : 0.3625 | Training Accuracy : 82.41 % | Validation Loss : 0.3622 | Validation Accuracy : 81.90 %\nEpoch 51/100 | Training Loss : 0.3655 | Training Accuracy : 80.96 % | Validation Loss : 0.3607 | Validation Accuracy : 81.55 %\nEpoch 52/100 | Training Loss : 0.3682 | Training Accuracy : 81.49 % | Validation Loss : 0.3634 | Validation Accuracy : 82.07 %\nEpoch 53/100 | Training Loss : 0.3685 | Training Accuracy : 81.30 % | Validation Loss : 0.3611 | Validation Accuracy : 81.72 %\nEpoch 54/100 | Training Loss : 0.3646 | Training Accuracy : 81.17 % | Validation Loss : 0.3623 | Validation Accuracy : 82.24 %\nEpoch 55/100 | Training Loss : 0.3732 | Training Accuracy : 81.40 % | Validation Loss : 0.3642 | Validation Accuracy : 81.72 %\nEpoch 56/100 | Training Loss : 0.3642 | Training Accuracy : 82.05 % | Validation Loss : 0.3602 | Validation Accuracy : 81.21 %\nEpoch 57/100 | Training Loss : 0.3682 | Training Accuracy : 81.44 % | Validation Loss : 0.3616 | Validation Accuracy : 81.90 %\nEpoch 58/100 | Training Loss : 0.3669 | Training Accuracy : 81.28 % | Validation Loss : 0.3606 | Validation Accuracy : 81.72 %\nEpoch 59/100 | Training Loss : 0.3645 | Training Accuracy : 81.70 % | Validation Loss : 0.3605 | Validation Accuracy : 81.90 %\nEpoch 60/100 | Training Loss : 0.3629 | Training Accuracy : 81.69 % | Validation Loss : 0.3614 | Validation Accuracy : 82.41 %\nEpoch 61/100 | Training Loss : 0.3668 | Training Accuracy : 81.03 % | Validation Loss : 0.3604 | Validation Accuracy : 81.55 %\nEpoch 62/100 | Training Loss : 0.3738 | Training Accuracy : 81.32 % | Validation Loss : 0.3653 | Validation Accuracy : 82.24 %\nEpoch 63/100 | Training Loss : 0.3583 | Training Accuracy : 82.16 % | Validation Loss : 0.3608 | Validation Accuracy : 81.90 %\nEpoch 64/100 | Training Loss : 0.3745 | Training Accuracy : 80.84 % | Validation Loss : 0.3620 | Validation Accuracy : 82.07 %\nEpoch 65/100 | Training Loss : 0.3670 | Training Accuracy : 81.30 % | Validation Loss : 0.3606 | Validation Accuracy : 81.21 %\nEpoch 66/100 | Training Loss : 0.3668 | Training Accuracy : 81.25 % | Validation Loss : 0.3632 | Validation Accuracy : 81.90 %\nEpoch 67/100 | Training Loss : 0.3640 | Training Accuracy : 81.46 % | Validation Loss : 0.3601 | Validation Accuracy : 81.38 %\nEpoch 68/100 | Training Loss : 0.3673 | Training Accuracy : 81.84 % | Validation Loss : 0.3608 | Validation Accuracy : 82.24 %\nEpoch 69/100 | Training Loss : 0.3592 | Training Accuracy : 82.11 % | Validation Loss : 0.3612 | Validation Accuracy : 81.55 %\nEpoch 70/100 | Training Loss : 0.3745 | Training Accuracy : 81.25 % | Validation Loss : 0.3611 | Validation Accuracy : 82.41 %\nEpoch 71/100 | Training Loss : 0.3645 | Training Accuracy : 80.86 % | Validation Loss : 0.3599 | Validation Accuracy : 81.72 %\nEpoch 72/100 | Training Loss : 0.3663 | Training Accuracy : 81.84 % | Validation Loss : 0.3600 | Validation Accuracy : 81.72 %\nEpoch 73/100 | Training Loss : 0.3619 | Training Accuracy : 81.42 % | Validation Loss : 0.3607 | Validation Accuracy : 81.90 %\nEpoch 74/100 | Training Loss : 0.3628 | Training Accuracy : 81.53 % | Validation Loss : 0.3598 | Validation Accuracy : 81.90 %\nEpoch 75/100 | Training Loss : 0.3670 | Training Accuracy : 81.30 % | Validation Loss : 0.3596 | Validation Accuracy : 81.21 %\nEpoch 76/100 | Training Loss : 0.3670 | Training Accuracy : 82.01 % | Validation Loss : 0.3623 | Validation Accuracy : 82.59 %\nEpoch 77/100 | Training Loss : 0.3616 | Training Accuracy : 81.93 % | Validation Loss : 0.3609 | Validation Accuracy : 82.59 %\nEpoch 78/100 | Training Loss : 0.3657 | Training Accuracy : 81.74 % | Validation Loss : 0.3607 | Validation Accuracy : 82.41 %\nEpoch 79/100 | Training Loss : 0.3706 | Training Accuracy : 81.51 % | Validation Loss : 0.3630 | Validation Accuracy : 82.07 %\nEpoch 80/100 | Training Loss : 0.3596 | Training Accuracy : 82.62 % | Validation Loss : 0.3613 | Validation Accuracy : 82.41 %\nEpoch 81/100 | Training Loss : 0.3648 | Training Accuracy : 81.86 % | Validation Loss : 0.3608 | Validation Accuracy : 81.72 %\nEpoch 82/100 | Training Loss : 0.3723 | Training Accuracy : 81.28 % | Validation Loss : 0.3619 | Validation Accuracy : 82.24 %\nEpoch 83/100 | Training Loss : 0.3622 | Training Accuracy : 81.78 % | Validation Loss : 0.3624 | Validation Accuracy : 82.07 %\nEpoch 84/100 | Training Loss : 0.3588 | Training Accuracy : 81.88 % | Validation Loss : 0.3610 | Validation Accuracy : 81.90 %\nEpoch 85/100 | Training Loss : 0.3630 | Training Accuracy : 82.22 % | Validation Loss : 0.3607 | Validation Accuracy : 81.90 %\nEpoch 86/100 | Training Loss : 0.3663 | Training Accuracy : 81.76 % | Validation Loss : 0.3605 | Validation Accuracy : 81.55 %\nEpoch 87/100 | Training Loss : 0.3655 | Training Accuracy : 81.59 % | Validation Loss : 0.3600 | Validation Accuracy : 81.55 %\nEpoch 88/100 | Training Loss : 0.3660 | Training Accuracy : 81.90 % | Validation Loss : 0.3616 | Validation Accuracy : 81.90 %\nEpoch 89/100 | Training Loss : 0.3641 | Training Accuracy : 81.57 % | Validation Loss : 0.3600 | Validation Accuracy : 81.55 %\nEpoch 90/100 | Training Loss : 0.3701 | Training Accuracy : 81.57 % | Validation Loss : 0.3602 | Validation Accuracy : 81.55 %\nEpoch 91/100 | Training Loss : 0.3598 | Training Accuracy : 81.78 % | Validation Loss : 0.3632 | Validation Accuracy : 82.59 %\nEpoch 92/100 | Training Loss : 0.3706 | Training Accuracy : 81.44 % | Validation Loss : 0.3600 | Validation Accuracy : 81.90 %\nEpoch 93/100 | Training Loss : 0.3643 | Training Accuracy : 81.40 % | Validation Loss : 0.3623 | Validation Accuracy : 82.07 %\nEpoch 94/100 | Training Loss : 0.3630 | Training Accuracy : 81.74 % | Validation Loss : 0.3615 | Validation Accuracy : 82.41 %\nEpoch 95/100 | Training Loss : 0.3641 | Training Accuracy : 82.05 % | Validation Loss : 0.3609 | Validation Accuracy : 81.90 %\nEpoch 96/100 | Training Loss : 0.3622 | Training Accuracy : 82.09 % | Validation Loss : 0.3622 | Validation Accuracy : 82.41 %\nEpoch 97/100 | Training Loss : 0.3663 | Training Accuracy : 81.99 % | Validation Loss : 0.3610 | Validation Accuracy : 81.90 %\nEpoch 98/100 | Training Loss : 0.3642 | Training Accuracy : 81.67 % | Validation Loss : 0.3646 | Validation Accuracy : 82.07 %\nEpoch 99/100 | Training Loss : 0.3665 | Training Accuracy : 81.28 % | Validation Loss : 0.3617 | Validation Accuracy : 81.90 %\nEpoch 100/100 | Training Loss : 0.3645 | Training Accuracy : 81.74 % | Validation Loss : 0.3595 | Validation Accuracy : 81.55 %\nTest Subject : 7 | Test Loss : 0.4479 | Test Accuracy : 81.25 %\n\n################################\n\n===== Fold 8\n===== Seed : 50\n===== Training Subject : 1, 2, 3, 4, 5, 6, 7, 9\n===== Test Subject : 8\n\nEpoch 1/100 | Training Loss : 0.3965 | Training Accuracy : 80.25 % | Validation Loss : 0.3783 | Validation Accuracy : 77.78 %\nEpoch 2/100 | Training Loss : 0.3957 | Training Accuracy : 80.29 % | Validation Loss : 0.3746 | Validation Accuracy : 78.65 %\nEpoch 3/100 | Training Loss : 0.3860 | Training Accuracy : 80.84 % | Validation Loss : 0.3711 | Validation Accuracy : 80.03 %\nEpoch 4/100 | Training Loss : 0.3891 | Training Accuracy : 79.94 % | Validation Loss : 0.3725 | Validation Accuracy : 78.47 %\nEpoch 5/100 | Training Loss : 0.3844 | Training Accuracy : 80.56 % | Validation Loss : 0.3881 | Validation Accuracy : 78.99 %\nEpoch 6/100 | Training Loss : 0.3890 | Training Accuracy : 79.98 % | Validation Loss : 0.3987 | Validation Accuracy : 79.51 %\nEpoch 7/100 | Training Loss : 0.3923 | Training Accuracy : 80.03 % | Validation Loss : 0.3844 | Validation Accuracy : 77.60 %\nEpoch 8/100 | Training Loss : 0.3829 | Training Accuracy : 79.86 % | Validation Loss : 0.3803 | Validation Accuracy : 78.82 %\nEpoch 9/100 | Training Loss : 0.3781 | Training Accuracy : 80.90 % | Validation Loss : 0.3788 | Validation Accuracy : 77.78 %\nEpoch 10/100 | Training Loss : 0.3738 | Training Accuracy : 80.90 % | Validation Loss : 0.3677 | Validation Accuracy : 78.65 %\nEpoch 11/100 | Training Loss : 0.3700 | Training Accuracy : 82.54 % | Validation Loss : 0.3736 | Validation Accuracy : 78.65 %\nEpoch 12/100 | Training Loss : 0.3636 | Training Accuracy : 82.43 % | Validation Loss : 0.3721 | Validation Accuracy : 78.47 %\nEpoch 13/100 | Training Loss : 0.3644 | Training Accuracy : 81.96 % | Validation Loss : 0.3739 | Validation Accuracy : 80.03 %\nEpoch 14/100 | Training Loss : 0.3626 | Training Accuracy : 82.37 % | Validation Loss : 0.3708 | Validation Accuracy : 77.60 %\nEpoch 15/100 | Training Loss : 0.3606 | Training Accuracy : 82.10 % | Validation Loss : 0.3771 | Validation Accuracy : 78.82 %\nEpoch 16/100 | Training Loss : 0.3638 | Training Accuracy : 81.93 % | Validation Loss : 0.3821 | Validation Accuracy : 78.30 %\nEpoch 17/100 | Training Loss : 0.3535 | Training Accuracy : 82.04 % | Validation Loss : 0.3723 | Validation Accuracy : 78.12 %\nEpoch 18/100 | Training Loss : 0.3615 | Training Accuracy : 82.25 % | Validation Loss : 0.3748 | Validation Accuracy : 78.82 %\nEpoch 19/100 | Training Loss : 0.3617 | Training Accuracy : 82.02 % | Validation Loss : 0.3755 | Validation Accuracy : 77.78 %\nEpoch 20/100 | Training Loss : 0.3605 | Training Accuracy : 81.66 % | Validation Loss : 0.3704 | Validation Accuracy : 77.95 %\nEpoch 21/100 | Training Loss : 0.3643 | Training Accuracy : 82.37 % | Validation Loss : 0.3784 | Validation Accuracy : 78.12 %\nEpoch 22/100 | Training Loss : 0.3575 | Training Accuracy : 82.37 % | Validation Loss : 0.3704 | Validation Accuracy : 77.78 %\nEpoch 23/100 | Training Loss : 0.3604 | Training Accuracy : 82.27 % | Validation Loss : 0.3732 | Validation Accuracy : 78.12 %\nEpoch 24/100 | Training Loss : 0.3455 | Training Accuracy : 83.28 % | Validation Loss : 0.3743 | Validation Accuracy : 77.43 %\nEpoch 25/100 | Training Loss : 0.3545 | Training Accuracy : 82.50 % | Validation Loss : 0.3752 | Validation Accuracy : 77.78 %\nEpoch 26/100 | Training Loss : 0.3494 | Training Accuracy : 82.48 % | Validation Loss : 0.3744 | Validation Accuracy : 78.30 %\nEpoch 27/100 | Training Loss : 0.3536 | Training Accuracy : 82.00 % | Validation Loss : 0.3728 | Validation Accuracy : 78.30 %\nEpoch 28/100 | Training Loss : 0.3562 | Training Accuracy : 82.47 % | Validation Loss : 0.3741 | Validation Accuracy : 77.60 %\nEpoch 29/100 | Training Loss : 0.3551 | Training Accuracy : 82.91 % | Validation Loss : 0.3744 | Validation Accuracy : 78.30 %\nEpoch 30/100 | Training Loss : 0.3560 | Training Accuracy : 82.45 % | Validation Loss : 0.3757 | Validation Accuracy : 78.82 %\nEpoch 31/100 | Training Loss : 0.3512 | Training Accuracy : 82.47 % | Validation Loss : 0.3736 | Validation Accuracy : 77.95 %\nEpoch 32/100 | Training Loss : 0.3440 | Training Accuracy : 82.79 % | Validation Loss : 0.3727 | Validation Accuracy : 78.47 %\nEpoch 33/100 | Training Loss : 0.3492 | Training Accuracy : 82.54 % | Validation Loss : 0.3719 | Validation Accuracy : 77.78 %\nEpoch 34/100 | Training Loss : 0.3537 | Training Accuracy : 82.45 % | Validation Loss : 0.3749 | Validation Accuracy : 78.12 %\nEpoch 35/100 | Training Loss : 0.3475 | Training Accuracy : 83.16 % | Validation Loss : 0.3706 | Validation Accuracy : 77.78 %\nEpoch 36/100 | Training Loss : 0.3491 | Training Accuracy : 82.50 % | Validation Loss : 0.3733 | Validation Accuracy : 77.95 %\nEpoch 37/100 | Training Loss : 0.3527 | Training Accuracy : 82.83 % | Validation Loss : 0.3750 | Validation Accuracy : 78.30 %\nEpoch 38/100 | Training Loss : 0.3545 | Training Accuracy : 82.25 % | Validation Loss : 0.3739 | Validation Accuracy : 78.30 %\nEpoch 39/100 | Training Loss : 0.3485 | Training Accuracy : 82.95 % | Validation Loss : 0.3730 | Validation Accuracy : 78.30 %\nEpoch 40/100 | Training Loss : 0.3534 | Training Accuracy : 82.64 % | Validation Loss : 0.3746 | Validation Accuracy : 78.47 %\nEpoch 41/100 | Training Loss : 0.3505 | Training Accuracy : 82.54 % | Validation Loss : 0.3756 | Validation Accuracy : 78.47 %\nEpoch 42/100 | Training Loss : 0.3495 | Training Accuracy : 82.37 % | Validation Loss : 0.3757 | Validation Accuracy : 78.30 %\nEpoch 43/100 | Training Loss : 0.3482 | Training Accuracy : 82.58 % | Validation Loss : 0.3758 | Validation Accuracy : 78.12 %\nEpoch 44/100 | Training Loss : 0.3530 | Training Accuracy : 82.37 % | Validation Loss : 0.3746 | Validation Accuracy : 78.12 %\nEpoch 45/100 | Training Loss : 0.3531 | Training Accuracy : 82.31 % | Validation Loss : 0.3732 | Validation Accuracy : 77.95 %\nEpoch 46/100 | Training Loss : 0.3447 | Training Accuracy : 83.20 % | Validation Loss : 0.3771 | Validation Accuracy : 77.78 %\nEpoch 47/100 | Training Loss : 0.3614 | Training Accuracy : 82.08 % | Validation Loss : 0.3744 | Validation Accuracy : 77.78 %\nEpoch 48/100 | Training Loss : 0.3452 | Training Accuracy : 82.58 % | Validation Loss : 0.3755 | Validation Accuracy : 78.12 %\nEpoch 49/100 | Training Loss : 0.3557 | Training Accuracy : 82.10 % | Validation Loss : 0.3750 | Validation Accuracy : 78.12 %\nEpoch 50/100 | Training Loss : 0.3512 | Training Accuracy : 83.02 % | Validation Loss : 0.3752 | Validation Accuracy : 77.95 %\nEpoch 51/100 | Training Loss : 0.3491 | Training Accuracy : 82.56 % | Validation Loss : 0.3746 | Validation Accuracy : 77.95 %\nEpoch 52/100 | Training Loss : 0.3526 | Training Accuracy : 82.50 % | Validation Loss : 0.3745 | Validation Accuracy : 78.30 %\nEpoch 53/100 | Training Loss : 0.3507 | Training Accuracy : 82.14 % | Validation Loss : 0.3720 | Validation Accuracy : 78.30 %\nEpoch 54/100 | Training Loss : 0.3490 | Training Accuracy : 82.97 % | Validation Loss : 0.3734 | Validation Accuracy : 77.95 %\nEpoch 55/100 | Training Loss : 0.3489 | Training Accuracy : 83.28 % | Validation Loss : 0.3774 | Validation Accuracy : 78.30 %\nEpoch 56/100 | Training Loss : 0.3504 | Training Accuracy : 82.23 % | Validation Loss : 0.3753 | Validation Accuracy : 78.12 %\nEpoch 57/100 | Training Loss : 0.3526 | Training Accuracy : 82.43 % | Validation Loss : 0.3731 | Validation Accuracy : 78.12 %\nEpoch 58/100 | Training Loss : 0.3532 | Training Accuracy : 82.47 % | Validation Loss : 0.3748 | Validation Accuracy : 77.60 %\nEpoch 59/100 | Training Loss : 0.3389 | Training Accuracy : 82.97 % | Validation Loss : 0.3733 | Validation Accuracy : 78.47 %\nEpoch 60/100 | Training Loss : 0.3546 | Training Accuracy : 81.98 % | Validation Loss : 0.3747 | Validation Accuracy : 77.95 %\nEpoch 61/100 | Training Loss : 0.3519 | Training Accuracy : 82.64 % | Validation Loss : 0.3718 | Validation Accuracy : 77.78 %\nEpoch 62/100 | Training Loss : 0.3502 | Training Accuracy : 82.87 % | Validation Loss : 0.3733 | Validation Accuracy : 77.95 %\nEpoch 63/100 | Training Loss : 0.3515 | Training Accuracy : 82.45 % | Validation Loss : 0.3776 | Validation Accuracy : 77.78 %\nEpoch 64/100 | Training Loss : 0.3553 | Training Accuracy : 82.08 % | Validation Loss : 0.3730 | Validation Accuracy : 77.95 %\nEpoch 65/100 | Training Loss : 0.3485 | Training Accuracy : 82.75 % | Validation Loss : 0.3752 | Validation Accuracy : 77.95 %\nEpoch 66/100 | Training Loss : 0.3553 | Training Accuracy : 82.04 % | Validation Loss : 0.3722 | Validation Accuracy : 78.30 %\nEpoch 67/100 | Training Loss : 0.3438 | Training Accuracy : 83.55 % | Validation Loss : 0.3733 | Validation Accuracy : 77.60 %\nEpoch 68/100 | Training Loss : 0.3501 | Training Accuracy : 82.95 % | Validation Loss : 0.3742 | Validation Accuracy : 77.78 %\nEpoch 69/100 | Training Loss : 0.3518 | Training Accuracy : 82.41 % | Validation Loss : 0.3759 | Validation Accuracy : 77.95 %\nEpoch 70/100 | Training Loss : 0.3479 | Training Accuracy : 83.06 % | Validation Loss : 0.3745 | Validation Accuracy : 78.12 %\nEpoch 71/100 | Training Loss : 0.3500 | Training Accuracy : 82.89 % | Validation Loss : 0.3744 | Validation Accuracy : 77.95 %\nEpoch 72/100 | Training Loss : 0.3524 | Training Accuracy : 82.33 % | Validation Loss : 0.3740 | Validation Accuracy : 77.95 %\nEpoch 73/100 | Training Loss : 0.3469 | Training Accuracy : 82.74 % | Validation Loss : 0.3743 | Validation Accuracy : 77.95 %\nEpoch 74/100 | Training Loss : 0.3524 | Training Accuracy : 82.70 % | Validation Loss : 0.3778 | Validation Accuracy : 77.95 %\nEpoch 75/100 | Training Loss : 0.3498 | Training Accuracy : 83.18 % | Validation Loss : 0.3742 | Validation Accuracy : 77.78 %\nEpoch 76/100 | Training Loss : 0.3607 | Training Accuracy : 82.04 % | Validation Loss : 0.3779 | Validation Accuracy : 77.43 %\nEpoch 77/100 | Training Loss : 0.3519 | Training Accuracy : 82.74 % | Validation Loss : 0.3741 | Validation Accuracy : 77.78 %\nEpoch 78/100 | Training Loss : 0.3534 | Training Accuracy : 82.18 % | Validation Loss : 0.3766 | Validation Accuracy : 77.78 %\nEpoch 79/100 | Training Loss : 0.3401 | Training Accuracy : 83.41 % | Validation Loss : 0.3729 | Validation Accuracy : 78.30 %\nEpoch 80/100 | Training Loss : 0.3517 | Training Accuracy : 82.93 % | Validation Loss : 0.3768 | Validation Accuracy : 77.78 %\nEpoch 81/100 | Training Loss : 0.3501 | Training Accuracy : 83.24 % | Validation Loss : 0.3712 | Validation Accuracy : 78.12 %\nEpoch 82/100 | Training Loss : 0.3451 | Training Accuracy : 82.95 % | Validation Loss : 0.3731 | Validation Accuracy : 78.12 %\nEpoch 83/100 | Training Loss : 0.3545 | Training Accuracy : 82.00 % | Validation Loss : 0.3754 | Validation Accuracy : 77.78 %\nEpoch 84/100 | Training Loss : 0.3513 | Training Accuracy : 82.91 % | Validation Loss : 0.3733 | Validation Accuracy : 77.95 %\nEpoch 85/100 | Training Loss : 0.3472 | Training Accuracy : 82.23 % | Validation Loss : 0.3738 | Validation Accuracy : 78.12 %\nEpoch 86/100 | Training Loss : 0.3440 | Training Accuracy : 82.72 % | Validation Loss : 0.3753 | Validation Accuracy : 78.12 %\nEpoch 87/100 | Training Loss : 0.3535 | Training Accuracy : 82.06 % | Validation Loss : 0.3722 | Validation Accuracy : 77.95 %\nEpoch 88/100 | Training Loss : 0.3554 | Training Accuracy : 82.20 % | Validation Loss : 0.3743 | Validation Accuracy : 77.60 %\nEpoch 89/100 | Training Loss : 0.3500 | Training Accuracy : 82.14 % | Validation Loss : 0.3747 | Validation Accuracy : 78.12 %\nEpoch 90/100 | Training Loss : 0.3495 | Training Accuracy : 82.16 % | Validation Loss : 0.3728 | Validation Accuracy : 77.78 %\nEpoch 91/100 | Training Loss : 0.3464 | Training Accuracy : 82.20 % | Validation Loss : 0.3760 | Validation Accuracy : 77.78 %\nEpoch 92/100 | Training Loss : 0.3483 | Training Accuracy : 82.45 % | Validation Loss : 0.3740 | Validation Accuracy : 77.78 %\nEpoch 93/100 | Training Loss : 0.3527 | Training Accuracy : 82.21 % | Validation Loss : 0.3735 | Validation Accuracy : 77.95 %\nEpoch 94/100 | Training Loss : 0.3534 | Training Accuracy : 82.14 % | Validation Loss : 0.3762 | Validation Accuracy : 77.95 %\nEpoch 95/100 | Training Loss : 0.3605 | Training Accuracy : 81.52 % | Validation Loss : 0.3764 | Validation Accuracy : 78.12 %\nEpoch 96/100 | Training Loss : 0.3590 | Training Accuracy : 82.04 % | Validation Loss : 0.3763 | Validation Accuracy : 78.30 %\nEpoch 97/100 | Training Loss : 0.3574 | Training Accuracy : 82.31 % | Validation Loss : 0.3757 | Validation Accuracy : 77.78 %\nEpoch 98/100 | Training Loss : 0.3516 | Training Accuracy : 82.62 % | Validation Loss : 0.3713 | Validation Accuracy : 78.30 %\nEpoch 99/100 | Training Loss : 0.3512 | Training Accuracy : 82.45 % | Validation Loss : 0.3772 | Validation Accuracy : 77.60 %\nEpoch 100/100 | Training Loss : 0.3497 | Training Accuracy : 82.75 % | Validation Loss : 0.3750 | Validation Accuracy : 77.78 %\nTest Subject : 8 | Test Loss : 0.5571 | Test Accuracy : 79.08 %\n\n################################\n\n===== Fold 9\n===== Seed : 51\n===== Training Subject : 1, 2, 3, 4, 5, 6, 7, 8\n===== Test Subject : 9\n\nEpoch 1/100 | Training Loss : 0.4014 | Training Accuracy : 79.60 % | Validation Loss : 0.3561 | Validation Accuracy : 84.48 %\nEpoch 2/100 | Training Loss : 0.4011 | Training Accuracy : 79.16 % | Validation Loss : 0.3745 | Validation Accuracy : 81.21 %\nEpoch 3/100 | Training Loss : 0.4036 | Training Accuracy : 79.41 % | Validation Loss : 0.3683 | Validation Accuracy : 82.76 %\nEpoch 4/100 | Training Loss : 0.3890 | Training Accuracy : 80.13 % | Validation Loss : 0.3838 | Validation Accuracy : 81.72 %\nEpoch 5/100 | Training Loss : 0.3878 | Training Accuracy : 80.54 % | Validation Loss : 0.3737 | Validation Accuracy : 82.59 %\nEpoch 6/100 | Training Loss : 0.3958 | Training Accuracy : 81.00 % | Validation Loss : 0.4069 | Validation Accuracy : 78.97 %\nEpoch 7/100 | Training Loss : 0.3892 | Training Accuracy : 80.02 % | Validation Loss : 0.3837 | Validation Accuracy : 80.00 %\nEpoch 8/100 | Training Loss : 0.3778 | Training Accuracy : 80.84 % | Validation Loss : 0.3749 | Validation Accuracy : 81.38 %\nEpoch 9/100 | Training Loss : 0.3810 | Training Accuracy : 80.73 % | Validation Loss : 0.3781 | Validation Accuracy : 81.21 %\nEpoch 10/100 | Training Loss : 0.3750 | Training Accuracy : 81.42 % | Validation Loss : 0.3810 | Validation Accuracy : 81.21 %\nEpoch 11/100 | Training Loss : 0.3751 | Training Accuracy : 81.46 % | Validation Loss : 0.3804 | Validation Accuracy : 82.07 %\nEpoch 12/100 | Training Loss : 0.3763 | Training Accuracy : 80.75 % | Validation Loss : 0.3885 | Validation Accuracy : 81.03 %\nEpoch 13/100 | Training Loss : 0.3751 | Training Accuracy : 81.17 % | Validation Loss : 0.3842 | Validation Accuracy : 80.86 %\nEpoch 14/100 | Training Loss : 0.3703 | Training Accuracy : 81.07 % | Validation Loss : 0.3854 | Validation Accuracy : 80.52 %\nEpoch 15/100 | Training Loss : 0.3735 | Training Accuracy : 80.82 % | Validation Loss : 0.3791 | Validation Accuracy : 80.34 %\nEpoch 16/100 | Training Loss : 0.3663 | Training Accuracy : 81.67 % | Validation Loss : 0.3824 | Validation Accuracy : 80.69 %\nEpoch 17/100 | Training Loss : 0.3710 | Training Accuracy : 81.07 % | Validation Loss : 0.3815 | Validation Accuracy : 82.24 %\nEpoch 18/100 | Training Loss : 0.3685 | Training Accuracy : 81.48 % | Validation Loss : 0.3764 | Validation Accuracy : 80.69 %\nEpoch 19/100 | Training Loss : 0.3711 | Training Accuracy : 81.40 % | Validation Loss : 0.3823 | Validation Accuracy : 79.66 %\nEpoch 20/100 | Training Loss : 0.3638 | Training Accuracy : 81.92 % | Validation Loss : 0.3804 | Validation Accuracy : 80.00 %\nEpoch 21/100 | Training Loss : 0.3617 | Training Accuracy : 81.70 % | Validation Loss : 0.3814 | Validation Accuracy : 79.83 %\nEpoch 22/100 | Training Loss : 0.3668 | Training Accuracy : 81.88 % | Validation Loss : 0.3809 | Validation Accuracy : 80.34 %\nEpoch 23/100 | Training Loss : 0.3570 | Training Accuracy : 81.70 % | Validation Loss : 0.3865 | Validation Accuracy : 79.66 %\nEpoch 24/100 | Training Loss : 0.3622 | Training Accuracy : 81.78 % | Validation Loss : 0.3887 | Validation Accuracy : 80.34 %\nEpoch 25/100 | Training Loss : 0.3659 | Training Accuracy : 81.86 % | Validation Loss : 0.3793 | Validation Accuracy : 81.38 %\nEpoch 26/100 | Training Loss : 0.3557 | Training Accuracy : 82.68 % | Validation Loss : 0.3846 | Validation Accuracy : 81.21 %\nEpoch 27/100 | Training Loss : 0.3617 | Training Accuracy : 81.97 % | Validation Loss : 0.3792 | Validation Accuracy : 80.52 %\nEpoch 28/100 | Training Loss : 0.3653 | Training Accuracy : 81.59 % | Validation Loss : 0.3814 | Validation Accuracy : 81.55 %\nEpoch 29/100 | Training Loss : 0.3579 | Training Accuracy : 82.30 % | Validation Loss : 0.3806 | Validation Accuracy : 80.52 %\nEpoch 30/100 | Training Loss : 0.3598 | Training Accuracy : 81.72 % | Validation Loss : 0.3802 | Validation Accuracy : 80.86 %\nEpoch 31/100 | Training Loss : 0.3546 | Training Accuracy : 82.49 % | Validation Loss : 0.3803 | Validation Accuracy : 80.86 %\nEpoch 32/100 | Training Loss : 0.3668 | Training Accuracy : 81.53 % | Validation Loss : 0.3798 | Validation Accuracy : 81.38 %\nEpoch 33/100 | Training Loss : 0.3550 | Training Accuracy : 82.55 % | Validation Loss : 0.3821 | Validation Accuracy : 81.55 %\nEpoch 34/100 | Training Loss : 0.3606 | Training Accuracy : 82.07 % | Validation Loss : 0.3788 | Validation Accuracy : 81.38 %\nEpoch 35/100 | Training Loss : 0.3498 | Training Accuracy : 82.51 % | Validation Loss : 0.3801 | Validation Accuracy : 81.03 %\nEpoch 36/100 | Training Loss : 0.3583 | Training Accuracy : 82.18 % | Validation Loss : 0.3828 | Validation Accuracy : 80.69 %\nEpoch 37/100 | Training Loss : 0.3477 | Training Accuracy : 82.95 % | Validation Loss : 0.3805 | Validation Accuracy : 81.03 %\nEpoch 38/100 | Training Loss : 0.3538 | Training Accuracy : 81.97 % | Validation Loss : 0.3809 | Validation Accuracy : 81.21 %\nEpoch 39/100 | Training Loss : 0.3558 | Training Accuracy : 82.36 % | Validation Loss : 0.3823 | Validation Accuracy : 81.03 %\nEpoch 40/100 | Training Loss : 0.3599 | Training Accuracy : 81.82 % | Validation Loss : 0.3801 | Validation Accuracy : 80.86 %\nEpoch 41/100 | Training Loss : 0.3540 | Training Accuracy : 82.07 % | Validation Loss : 0.3823 | Validation Accuracy : 80.69 %\nEpoch 42/100 | Training Loss : 0.3554 | Training Accuracy : 82.62 % | Validation Loss : 0.3787 | Validation Accuracy : 80.52 %\nEpoch 43/100 | Training Loss : 0.3493 | Training Accuracy : 81.90 % | Validation Loss : 0.3818 | Validation Accuracy : 80.86 %\nEpoch 44/100 | Training Loss : 0.3618 | Training Accuracy : 82.11 % | Validation Loss : 0.3794 | Validation Accuracy : 81.21 %\nEpoch 45/100 | Training Loss : 0.3563 | Training Accuracy : 81.88 % | Validation Loss : 0.3814 | Validation Accuracy : 80.52 %\nEpoch 46/100 | Training Loss : 0.3574 | Training Accuracy : 81.95 % | Validation Loss : 0.3809 | Validation Accuracy : 80.69 %\nEpoch 47/100 | Training Loss : 0.3581 | Training Accuracy : 81.74 % | Validation Loss : 0.3830 | Validation Accuracy : 81.21 %\nEpoch 48/100 | Training Loss : 0.3572 | Training Accuracy : 82.41 % | Validation Loss : 0.3819 | Validation Accuracy : 81.38 %\nEpoch 49/100 | Training Loss : 0.3634 | Training Accuracy : 81.82 % | Validation Loss : 0.3809 | Validation Accuracy : 80.34 %\nEpoch 50/100 | Training Loss : 0.3537 | Training Accuracy : 81.95 % | Validation Loss : 0.3805 | Validation Accuracy : 80.69 %\nEpoch 51/100 | Training Loss : 0.3654 | Training Accuracy : 81.40 % | Validation Loss : 0.3877 | Validation Accuracy : 80.34 %\nEpoch 52/100 | Training Loss : 0.3575 | Training Accuracy : 82.28 % | Validation Loss : 0.3817 | Validation Accuracy : 80.69 %\nEpoch 53/100 | Training Loss : 0.3481 | Training Accuracy : 82.51 % | Validation Loss : 0.3790 | Validation Accuracy : 81.55 %\nEpoch 54/100 | Training Loss : 0.3526 | Training Accuracy : 82.41 % | Validation Loss : 0.3816 | Validation Accuracy : 80.69 %\nEpoch 55/100 | Training Loss : 0.3550 | Training Accuracy : 81.67 % | Validation Loss : 0.3803 | Validation Accuracy : 81.03 %\nEpoch 56/100 | Training Loss : 0.3632 | Training Accuracy : 81.86 % | Validation Loss : 0.3810 | Validation Accuracy : 81.38 %\nEpoch 57/100 | Training Loss : 0.3603 | Training Accuracy : 81.74 % | Validation Loss : 0.3833 | Validation Accuracy : 80.69 %\nEpoch 58/100 | Training Loss : 0.3539 | Training Accuracy : 82.47 % | Validation Loss : 0.3810 | Validation Accuracy : 80.86 %\nEpoch 59/100 | Training Loss : 0.3523 | Training Accuracy : 81.95 % | Validation Loss : 0.3821 | Validation Accuracy : 80.52 %\nEpoch 60/100 | Training Loss : 0.3515 | Training Accuracy : 82.01 % | Validation Loss : 0.3795 | Validation Accuracy : 81.38 %\nEpoch 61/100 | Training Loss : 0.3578 | Training Accuracy : 81.61 % | Validation Loss : 0.3818 | Validation Accuracy : 80.52 %\nEpoch 62/100 | Training Loss : 0.3431 | Training Accuracy : 83.22 % | Validation Loss : 0.3783 | Validation Accuracy : 81.90 %\nEpoch 63/100 | Training Loss : 0.3561 | Training Accuracy : 82.36 % | Validation Loss : 0.3800 | Validation Accuracy : 80.69 %\nEpoch 64/100 | Training Loss : 0.3596 | Training Accuracy : 81.93 % | Validation Loss : 0.3793 | Validation Accuracy : 81.38 %\nEpoch 65/100 | Training Loss : 0.3517 | Training Accuracy : 82.24 % | Validation Loss : 0.3796 | Validation Accuracy : 80.69 %\nEpoch 66/100 | Training Loss : 0.3660 | Training Accuracy : 81.99 % | Validation Loss : 0.3794 | Validation Accuracy : 81.21 %\nEpoch 67/100 | Training Loss : 0.3541 | Training Accuracy : 82.49 % | Validation Loss : 0.3798 | Validation Accuracy : 81.38 %\nEpoch 68/100 | Training Loss : 0.3557 | Training Accuracy : 82.76 % | Validation Loss : 0.3796 | Validation Accuracy : 80.86 %\nEpoch 69/100 | Training Loss : 0.3607 | Training Accuracy : 81.88 % | Validation Loss : 0.3860 | Validation Accuracy : 80.34 %\nEpoch 70/100 | Training Loss : 0.3556 | Training Accuracy : 82.22 % | Validation Loss : 0.3797 | Validation Accuracy : 81.38 %\nEpoch 71/100 | Training Loss : 0.3618 | Training Accuracy : 81.21 % | Validation Loss : 0.3813 | Validation Accuracy : 81.21 %\nEpoch 72/100 | Training Loss : 0.3617 | Training Accuracy : 81.36 % | Validation Loss : 0.3776 | Validation Accuracy : 81.55 %\nEpoch 73/100 | Training Loss : 0.3547 | Training Accuracy : 82.70 % | Validation Loss : 0.3799 | Validation Accuracy : 81.90 %\nEpoch 74/100 | Training Loss : 0.3548 | Training Accuracy : 82.30 % | Validation Loss : 0.3832 | Validation Accuracy : 81.38 %\nEpoch 75/100 | Training Loss : 0.3529 | Training Accuracy : 82.47 % | Validation Loss : 0.3810 | Validation Accuracy : 80.52 %\nEpoch 76/100 | Training Loss : 0.3537 | Training Accuracy : 82.20 % | Validation Loss : 0.3794 | Validation Accuracy : 80.69 %\nEpoch 77/100 | Training Loss : 0.3502 | Training Accuracy : 82.55 % | Validation Loss : 0.3816 | Validation Accuracy : 80.69 %\nEpoch 78/100 | Training Loss : 0.3478 | Training Accuracy : 83.14 % | Validation Loss : 0.3828 | Validation Accuracy : 80.86 %\nEpoch 79/100 | Training Loss : 0.3596 | Training Accuracy : 82.07 % | Validation Loss : 0.3847 | Validation Accuracy : 80.86 %\nEpoch 80/100 | Training Loss : 0.3604 | Training Accuracy : 81.57 % | Validation Loss : 0.3811 | Validation Accuracy : 81.03 %\nEpoch 81/100 | Training Loss : 0.3550 | Training Accuracy : 81.86 % | Validation Loss : 0.3791 | Validation Accuracy : 81.38 %\nEpoch 82/100 | Training Loss : 0.3544 | Training Accuracy : 82.30 % | Validation Loss : 0.3821 | Validation Accuracy : 81.38 %\nEpoch 83/100 | Training Loss : 0.3508 | Training Accuracy : 82.76 % | Validation Loss : 0.3819 | Validation Accuracy : 81.38 %\nEpoch 84/100 | Training Loss : 0.3564 | Training Accuracy : 82.36 % | Validation Loss : 0.3809 | Validation Accuracy : 81.21 %\nEpoch 85/100 | Training Loss : 0.3485 | Training Accuracy : 82.26 % | Validation Loss : 0.3819 | Validation Accuracy : 81.55 %\nEpoch 86/100 | Training Loss : 0.3602 | Training Accuracy : 81.92 % | Validation Loss : 0.3799 | Validation Accuracy : 81.03 %\nEpoch 87/100 | Training Loss : 0.3568 | Training Accuracy : 82.28 % | Validation Loss : 0.3804 | Validation Accuracy : 80.34 %\nEpoch 88/100 | Training Loss : 0.3582 | Training Accuracy : 81.97 % | Validation Loss : 0.3794 | Validation Accuracy : 81.72 %\nEpoch 89/100 | Training Loss : 0.3557 | Training Accuracy : 82.36 % | Validation Loss : 0.3832 | Validation Accuracy : 80.69 %\nEpoch 90/100 | Training Loss : 0.3496 | Training Accuracy : 82.22 % | Validation Loss : 0.3886 | Validation Accuracy : 80.86 %\nEpoch 91/100 | Training Loss : 0.3665 | Training Accuracy : 81.53 % | Validation Loss : 0.3815 | Validation Accuracy : 80.17 %\nEpoch 92/100 | Training Loss : 0.3643 | Training Accuracy : 81.80 % | Validation Loss : 0.3829 | Validation Accuracy : 81.38 %\nEpoch 93/100 | Training Loss : 0.3586 | Training Accuracy : 82.03 % | Validation Loss : 0.3799 | Validation Accuracy : 81.21 %\nEpoch 94/100 | Training Loss : 0.3524 | Training Accuracy : 82.91 % | Validation Loss : 0.3805 | Validation Accuracy : 81.38 %\nEpoch 95/100 | Training Loss : 0.3641 | Training Accuracy : 80.94 % | Validation Loss : 0.3806 | Validation Accuracy : 81.38 %\nEpoch 96/100 | Training Loss : 0.3574 | Training Accuracy : 82.16 % | Validation Loss : 0.3786 | Validation Accuracy : 80.86 %\nEpoch 97/100 | Training Loss : 0.3575 | Training Accuracy : 81.92 % | Validation Loss : 0.3818 | Validation Accuracy : 81.21 %\nEpoch 98/100 | Training Loss : 0.3607 | Training Accuracy : 81.59 % | Validation Loss : 0.3812 | Validation Accuracy : 80.52 %\nEpoch 99/100 | Training Loss : 0.3624 | Training Accuracy : 82.28 % | Validation Loss : 0.3797 | Validation Accuracy : 81.55 %\nEpoch 100/100 | Training Loss : 0.3549 | Training Accuracy : 81.99 % | Validation Loss : 0.3773 | Validation Accuracy : 81.72 %\nTest Subject : 9 | Test Loss : 0.3020 | Test Accuracy : 86.25 %\n\n################################\n\n================\nLOSO Summary\n================\nSubject 0 -> Test Accuracy : 70.14 %\nSubject 1 -> Test Accuracy : 58.53 %\nSubject 2 -> Test Accuracy : 58.47 %\nSubject 3 -> Test Accuracy : 86.08 %\nSubject 4 -> Test Accuracy : 83.11 %\nSubject 5 -> Test Accuracy : 79.17 %\nSubject 6 -> Test Accuracy : 81.25 %\nSubject 7 -> Test Accuracy : 79.08 %\nSubject 8 -> Test Accuracy : 86.25 %\n\n-----------------------------\nAverage Test Accuracy : 75.79 %\n-----------------------------\n","output_type":"stream"}],"execution_count":12}]}