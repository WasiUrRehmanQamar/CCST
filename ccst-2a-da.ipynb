{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13734721,"sourceType":"datasetVersion","datasetId":7205743}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Use GPU**","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:09:56.259432Z","iopub.execute_input":"2025-11-15T10:09:56.259759Z","iopub.status.idle":"2025-11-15T10:09:56.264945Z","shell.execute_reply.started":"2025-11-15T10:09:56.259733Z","shell.execute_reply":"2025-11-15T10:09:56.263948Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Hide Warnings**","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:09:56.584669Z","iopub.execute_input":"2025-11-15T10:09:56.584957Z","iopub.status.idle":"2025-11-15T10:09:56.588589Z","shell.execute_reply.started":"2025-11-15T10:09:56.584936Z","shell.execute_reply":"2025-11-15T10:09:56.587733Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# **Install Libraries**","metadata":{}},{"cell_type":"code","source":"pip install torch torchvision torchaudio torchinfo einops scikit-learn pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:09:56.904030Z","iopub.execute_input":"2025-11-15T10:09:56.904309Z","iopub.status.idle":"2025-11-15T10:10:00.221047Z","shell.execute_reply.started":"2025-11-15T10:09:56.904288Z","shell.execute_reply":"2025-11-15T10:10:00.219943Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install -U mne==1.0.0 scipy==1.13.1 numpy==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:13:11.669875Z","iopub.execute_input":"2025-11-15T10:13:11.670179Z","iopub.status.idle":"2025-11-15T10:13:15.869930Z","shell.execute_reply.started":"2025-11-15T10:13:11.670156Z","shell.execute_reply":"2025-11-15T10:13:15.869003Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mne==1.0.0 in /usr/local/lib/python3.10/dist-packages (1.0.0)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (3.7.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (4.67.1)\nRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (1.8.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (4.4.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (24.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne==1.0.0) (3.1.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne==1.0.0) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne==1.0.0) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne==1.0.0) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mne==1.0.0) (2.8.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mne==1.0.0) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne==1.0.0) (2024.12.14)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport mne\nimport time\nimport math\nimport pickle\nimport random\nimport datetime\nimport scipy.io\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom einops import rearrange\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom einops.layers.torch import Rearrange, Reduce\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import LeaveOneGroupOut, train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:21:16.806679Z","iopub.execute_input":"2025-11-15T10:21:16.807016Z","iopub.status.idle":"2025-11-15T10:21:16.812179Z","shell.execute_reply.started":"2025-11-15T10:21:16.806989Z","shell.execute_reply":"2025-11-15T10:21:16.811278Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"###############################################################################\n# 1) Set seeds for reproducibility\n###############################################################################\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n###############################################################################\n# 2) Load & preprocess data from the pickle file\n###############################################################################\ndef pad_and_resize_eeg(eeg_2d, target_h=22, target_w=321):\n    \"\"\"\n    Zero-pad or resize EEG data to ensure consistent dimensions\n    \n    Args:\n        eeg_2d (np.ndarray): EEG data of shape (channels, time_samples)\n        target_h (int): Target number of EEG channels\n        target_w (int): Target number of time samples\n    \n    Returns:\n        np.ndarray: Zero-padded EEG data of shape (target_h, target_w)\n    \"\"\"\n    out = np.zeros((target_h, target_w), dtype=eeg_2d.dtype)\n    h, w = eeg_2d.shape\n    used_h = min(h, target_h)\n    used_w = min(w, target_w)\n    out[:used_h, :used_w] = eeg_2d[:used_h, :used_w]\n    return out\n\ndef load_and_preprocess_data(pickle_path, target_h=22, target_w=321, l_freq=7, h_freq=30):\n    \"\"\"\n    Loads and preprocesses EEG data from a pickle file\n\n    Args:\n        pickle_path (str): Path to the pickle file\n        target_h (int): Target number of EEG channels\n        target_w (int): Target number of time samples\n        l_freq (float): Low cutoff frequency for band-pass filter\n        h_freq (float): High cutoff frequency for band-pass filter\n\n    Returns:\n        all_X (np.ndarray): Preprocessed EEG data of shape (trials, 1, channels, time_samples)\n        all_y (np.ndarray): Labels of shape (trials,)\n        all_subjects (np.ndarray): Subject indices for each trial\n    \"\"\"\n    with open(pickle_path, 'rb') as f:\n        raw_data = pickle.load(f)  # List of MNE Epochs, one per subject\n\n    all_X = []\n    all_y = []\n    all_subjects = []\n\n    for subj_idx, epochs in enumerate(raw_data):\n        # Apply band-pass filter\n        epochs = epochs.copy().filter(l_freq=l_freq, h_freq=h_freq)\n        X = epochs.get_data()  # (trials, channels, time_samples)\n        y = epochs.events[:, 2]  # (trials,)\n\n        y = y - 1  # Map 1->0, 2->1, 3->2, 4->3\n\n        # Resize each trial\n        padded = [pad_and_resize_eeg(trial, target_h, target_w) for trial in X]\n        padded_array = np.array(padded)  # (trials, channels, time_samples)\n\n        # Expand dimensions to match (B, 1, C, T)\n        padded_array = np.expand_dims(padded_array, axis=1)  # (trials, 1, channels, time_samples)\n\n        all_X.append(padded_array)\n        all_y.append(y)\n        all_subjects += [subj_idx] * len(y)\n\n    all_X = np.concatenate(all_X, axis=0)  # (total_trials, 1, channels, time_samples)\n    all_y = np.concatenate(all_y, axis=0)  # (total_trials,)\n    all_subjects = np.array(all_subjects)   # (total_trials,)\n\n    # Standardize\n    scaler = StandardScaler()\n    # Reshape to (trials, channels * time_samples) for scaling\n    all_X_reshaped = all_X.reshape(all_X.shape[0], -1)\n    all_X_scaled = scaler.fit_transform(all_X_reshaped)\n    all_X = all_X_scaled.reshape(all_X.shape[0], 1, target_h, target_w).astype(np.float32)\n\n    return all_X, all_y, all_subjects\n\n###############################################################################\n# 3) Dataset with optional augmentation\n###############################################################################\nclass MultiSubjectBCIDataset(Dataset):\n    def __init__(self, X, y, augment=False):\n        \"\"\"\n        Args:\n            X (np.ndarray): EEG data of shape (trials, 1, channels, time_samples)\n            y (np.ndarray): Labels of shape (trials,)\n            augment (bool): Whether to apply data augmentation\n        \"\"\"\n        self.X = X\n        self.y = y\n        self.augment = augment\n\n        self.max_shift = 10     # shift range\n        self.noise_amp = 0.01   # noise amplitude\n        self.dropout_rate = 0.05\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x_np = self.X[idx]  # shape: (1, channels, time_samples)\n        y = self.y[idx]\n\n        # Convert to torch\n        x_t = torch.from_numpy(x_np).float()  # (1, channels, time_samples)\n\n        if self.augment:\n            # Random shift in time\n            shift = random.randint(-self.max_shift, self.max_shift)\n            if shift > 0:\n                x_t = torch.cat([x_t[:, :, shift:], \n                                 torch.zeros((1, x_t.size(1), shift))], dim=2)\n            elif shift < 0:\n                x_t = torch.cat([torch.zeros((1, x_t.size(1), -shift)),\n                                 x_t[:, :, :shift]], dim=2)\n\n            # Add small Gaussian noise\n            noise = torch.randn_like(x_t) * self.noise_amp\n            x_t += noise\n\n            # Random electrode dropout\n            num_drop = int(x_t.size(1) * self.dropout_rate)\n            drop_indices = torch.randperm(x_t.size(1))[:num_drop]\n            x_t[:, drop_indices, :] = 0\n\n        return x_t, torch.tensor(y).long()\n\n###############################################################################\n# 4) Window-partitioning & shift for Swin-1D\n###############################################################################\ndef pad_sequence_1d(x, window_size):\n    \"\"\"\n    x shape: (B, L, C)\n    Zero-pad or resize length to multiple of window_size\n    \"\"\"\n    B, L, C = x.shape\n    remainder = L % window_size\n    if remainder == 0:\n        return x, L, 0\n    pad_len = window_size - remainder\n    pad_vec = torch.zeros(B, pad_len, C, dtype=x.dtype, device=x.device)\n    x_padded = torch.cat([x, pad_vec], dim=1)\n    return x_padded, L, pad_len\n\ndef window_partition_1d(x, window_size):\n    \"\"\"\n    x shape: (B, L, C) => (B*nW, window_size, C)\n    \"\"\"\n    B, L, C = x.shape\n    x_padded, orig_L, pad_len = pad_sequence_1d(x, window_size)\n    Bp, Lp, Cp = x_padded.shape\n    num_windows = Lp // window_size\n    x_padded = x_padded.view(Bp, num_windows, window_size, Cp)\n    x_windows = x_padded.reshape(Bp * num_windows, window_size, Cp)\n    return x_windows, (orig_L, pad_len, num_windows)\n\ndef window_reverse_1d(x_windows, window_size, pad_info):\n    \"\"\"\n    Reconstruct from (B*nW, window_size, C) => (B, L, C)\n    \"\"\"\n    orig_L, pad_len, num_windows = pad_info\n    BnW, WS, C = x_windows.shape\n    B = BnW // num_windows\n    x_reshaped = x_windows.view(B, num_windows, WS, C)\n    x_merged = x_reshaped.reshape(B, num_windows * WS, C)\n    if pad_len > 0:\n        x_merged = x_merged[:, :orig_L, :]\n    return x_merged\n\ndef cyclic_shift_1d(x, shift_size):\n    \"\"\"\n    Negative roll along dimension=1\n    \"\"\"\n    return torch.roll(x, shifts=-shift_size, dims=1)\n\ndef cyclic_shift_back_1d(x, shift_size):\n    return torch.roll(x, shifts=shift_size, dims=1)\n\n###############################################################################\n# 5) Custom QKV Attention & MLP\n###############################################################################\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads, attn_dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.attn_drop = nn.Dropout(attn_dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(attn_dropout)\n\n    def forward(self, x):\n        B, L, C = x.shape\n        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, L, head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, num_heads, L, L)\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        out = (attn @ v).transpose(1, 2).reshape(B, L, C)  # (B, L, C)\n        out = self.proj(out)\n        out = self.proj_drop(out)\n        return out\n\nclass MLP(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n###############################################################################\n# 6) Swin1DBlock & Swin1DTransformer\n###############################################################################\nclass Swin1DBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        window_size=4,\n        shift_size=2,\n        mlp_hidden=128,\n        attn_dropout=0.1\n    ):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n        # LayerNorm & Attention\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim=dim, num_heads=num_heads, attn_dropout=attn_dropout)\n\n        # LayerNorm & MLP\n        self.ln2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim=dim, hidden_dim=mlp_hidden, dropout=attn_dropout)\n\n    def forward(self, x):\n        # x: (B, L, dim)\n        if self.shift_size > 0:\n            x = cyclic_shift_1d(x, self.shift_size)\n\n        x_windows, pad_info = window_partition_1d(x, self.window_size)\n\n        # Attention + residual\n        shortcut = x_windows\n        x_windows = self.ln1(x_windows)\n        x_windows = self.attn(x_windows)\n        x_windows = shortcut + x_windows\n\n        # MLP + residual\n        shortcut = x_windows\n        x_windows = self.ln2(x_windows)\n        x_windows = self.mlp(x_windows)\n        x_windows = shortcut + x_windows\n\n        x_merged = window_reverse_1d(x_windows, self.window_size, pad_info)\n        if self.shift_size > 0:\n            x_merged = cyclic_shift_back_1d(x_merged, self.shift_size)\n        return x_merged\n\nclass Swin1DTransformer(nn.Module):\n    def __init__(\n        self,\n        dim=64,           # Embedding dimension\n        num_layers=3,     # Number of Swin1DBlock layers\n        num_heads=4,      # Number of attention heads\n        mlp_hidden=128,   # Hidden dimension in MLP\n        window_size=4,    # Window size for attention\n        attn_dropout=0.1, # Dropout rate for attention\n        fc_dropout=0.3    # Dropout rate before final FC\n    ):\n        super().__init__()\n        blocks = []\n        for i in range(num_layers):\n            shift = window_size // 2 if (i % 2 == 1) else 0\n            block = Swin1DBlock(\n                dim=dim,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=shift,\n                mlp_hidden=mlp_hidden,\n                attn_dropout=attn_dropout\n            )\n            blocks.append(block)\n\n        self.blocks = nn.ModuleList(blocks)\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        # x: (B, L, dim)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        # average pooling over the L dimension\n        x = x.mean(dim=1)  # (B, dim)\n        return x  # Feature Vector: (B, dim)\n\n###############################################################################\n# 7) Positional Embeddings: learnable, sine, or none\n###############################################################################\ndef create_positional_embedding(mode, seq_len, dim):\n    \"\"\"\n    Creates positional embeddings\n\n    Args:\n        mode (str): 'learnable', 'sine', or 'none'\n        seq_len (int): Sequence length\n        dim (int): Embedding dimension\n\n    Returns:\n        nn.Parameter or None: Positional embedding tensor\n    \"\"\"\n    if mode == 'none':\n        return None\n    elif mode == 'learnable':\n        pe = nn.Parameter(torch.zeros(1, seq_len, dim))\n        nn.init.trunc_normal_(pe, std=0.02)\n        return pe\n    elif mode == 'sine':\n        # Classic sinusoidal\n        pe_np = np.zeros((seq_len, dim))\n        for pos in range(seq_len):\n            for i in range(0, dim, 2):\n                theta = pos / (10000 ** ((2 * i) / dim))\n                pe_np[pos, i]   = np.sin(theta)\n                if i+1 < dim:\n                    pe_np[pos, i+1] = np.cos(theta)\n        pe = torch.from_numpy(pe_np).float().unsqueeze(0)  # shape (1, seq_len, dim)\n        return nn.Parameter(pe, requires_grad=False)\n    else:\n        raise ValueError(f\"Unsupported positional embedding mode: {mode}\")\n\n###############################################################################\n# 8) Patch Embedding\n###############################################################################\nclass PatchEmbedding(nn.Module):\n    def __init__(self, emb_size=40):\n        super().__init__()\n\n        self.shallownet = nn.Sequential(\n            nn.Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1)),  # Temporal convolution\n            nn.Conv2d(40, 40, kernel_size=(22, 1), stride=(1, 1)),  # Spatial convolution across electrodes\n            nn.BatchNorm2d(40),\n            nn.ELU(),\n            nn.AvgPool2d(kernel_size=(1, 75), stride=(1, 15)),  # Downsample temporal dimension\n            nn.Dropout(p=0.5),\n            nn.Conv2d(40, emb_size, kernel_size=(1,1), stride=(1,1))  # Projection to emb_size\n        )\n        self.rearrange = Rearrange('b e (h) (w) -> b (h w) e')  # Reshape to (B, seq_len, emb_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.shallownet(x)           # [B, emb_size, 1, w]\n        x = self.rearrange(x)            # [B, seq_len, emb_size]\n        return x\n\n###############################################################################\n# 9) Classification Head\n###############################################################################\nclass ClassificationHead(nn.Module):\n    def __init__(self, input_dim=64, num_classes=4):\n        super(ClassificationHead, self).__init__()\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):\n        x = self.dropout(x)            # [B, dim]\n        logits = self.linear(x)        # [B, num_classes]\n        return logits\n\n\n###############################################################################\n# 10) CCST\n###############################################################################\nclass CCST(nn.Module):\n    def __init__(\n        self,\n        emb_size=40,\n        swin_embedding_dim=64,\n        num_swin_layers=3,\n        num_heads=4,\n        mlp_size=128,\n        fc_dropout=0.3,\n        pos_emb_mode='learnable',\n        num_classes=4\n    ):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(emb_size=emb_size)  # [B, seq_len, emb_size]\n        self.embedding_projection = nn.Linear(emb_size, swin_embedding_dim)  # [B, seq_len, swin_embedding_dim]\n        self.pos_encoding = create_positional_embedding(pos_emb_mode, seq_len=15, dim=swin_embedding_dim)\n        self.transformer = Swin1DTransformer(\n            dim=swin_embedding_dim,\n            num_layers=num_swin_layers,\n            num_heads=num_heads,\n            mlp_hidden=mlp_size,\n            window_size=4,\n            attn_dropout=0.1\n        )\n        self.classification_head = ClassificationHead(input_dim=swin_embedding_dim, num_classes=num_classes)  # [B, num_classes]\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor of shape (B, 1, 22, 321)\n        \n        Returns:\n            torch.Tensor: Logits of shape (B, num_classes)\n        \"\"\"\n        x = self.patch_embedding(x)         # [B, seq_len, emb_size]\n        x = self.embedding_projection(x)    # [B, seq_len, swin_embedding_dim]\n        if self.pos_encoding is not None:\n            x = x + self.pos_encoding[:, :x.size(1), :]\n        features = self.transformer(x)      # [B, swin_embedding_dim]\n        logits = self.classification_head(features)  # [B, num_classes]\n        return logits\n\n###############################################################################\n# 11) Training and Evaluation\n###############################################################################\nclass HybridExP():\n    def __init__(self, nsub, pickle_path='/kaggle/input/bcic-iv/BCIIV_2a.pkl', device='cuda:0'):\n        super(HybridExP, self).__init__()\n        self.batch_size = 72\n        self.n_epochs = 100\n        self.patience = 10\n        self.c_dim = 4  # Number of classes (Left Hand, Right Hand)\n        self.lr = 3e-4\n        self.betas = (0.9, 0.999)\n        self.nSub = nsub\n        self.pickle_path = pickle_path\n        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n\n        self.criterion_cls = nn.CrossEntropyLoss().to(self.device)\n\n        self.model = CCST(num_classes=self.c_dim).to(self.device)\n        self.model = nn.DataParallel(self.model)\n        self.model = self.model.to(self.device)\n\n    # Data augmentation method\n    def interaug(self, timg, label):\n        \"\"\"\n        Placeholder for additional data augmentation if required\n        \"\"\"\n        return timg, label\n\n    # Load data\n    def get_source_data(self):\n        # Load and preprocess data\n        all_X, all_y, all_subjects = load_and_preprocess_data(\n            pickle_path=self.pickle_path,\n            target_h=22,\n            target_w=321,\n            l_freq=7,\n            h_freq=30\n        )\n\n        return all_X, all_y, all_subjects\n\n    # Training method for one fold\n    def train_fold(self, train_indices, val_indices, test_indices, train_labels, test_labels):\n        # Split data into training and validation\n        X_train = self.X[train_indices]\n        y_train = self.y[train_indices]\n        X_val = self.X[val_indices]\n        y_val = self.y[val_indices]\n        X_test = self.X[test_indices]\n        y_test = self.y[test_indices]\n\n        # Create datasets\n        train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=True)\n        val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n        test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n\n        # Create dataloaders\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n        test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n\n        # Define optimizer and scheduler\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                         factor=0.5, patience=5, verbose=True)\n\n        best_val_acc = 0.0\n        patience_counter = 0\n        best_model_state = None\n\n        for epoch in range(self.n_epochs):\n            # Training Phase\n            self.model.train()\n            running_loss = 0.0\n            correct = 0\n            total = 0\n\n            for img, label in train_loader:\n                img = img.to(self.device)  # (B, 1, 22, 321)\n                label = label.to(self.device)  # (B,)\n\n                optimizer.zero_grad()\n                outputs = self.model(img)  # (B, num_classes)\n                loss = self.criterion_cls(outputs, label)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * img.size(0)\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == label).sum().item()\n                total += label.size(0)\n\n            train_loss = running_loss / total\n            train_acc = correct / total\n\n            # Validation Phase\n            self.model.eval()\n            val_loss = 0.0\n            val_correct = 0\n            val_total = 0\n\n            with torch.no_grad():\n                for img, label in val_loader:\n                    img = img.to(self.device)\n                    label = label.to(self.device)\n\n                    outputs = self.model(img)\n                    loss = self.criterion_cls(outputs, label)\n\n                    val_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    val_correct += (preds == label).sum().item()\n                    val_total += label.size(0)\n\n            val_loss /= val_total\n            val_acc = val_correct / val_total\n\n            # Step scheduler\n            scheduler.step(val_loss)\n\n            print(f'Epoch {epoch+1}/{self.n_epochs} | '\n                  f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | '\n                  f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%')\n\n            # Early Stopping\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                best_model_state = self.model.state_dict()\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                # if patience_counter >= self.patience:\n                #     print(f\"Early stopping triggered at epoch {epoch+1}\")\n                #     break\n\n        # Load best model\n        if best_model_state is not None:\n            self.model.load_state_dict(best_model_state)\n\n        # Testing Phase\n        self.model.eval()\n        test_loss = 0.0\n        test_correct = 0\n        test_total = 0\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for img, label in test_loader:\n                img = img.to(self.device)\n                label = label.to(self.device)\n\n                outputs = self.model(img)\n                loss = self.criterion_cls(outputs, label)\n\n                test_loss += loss.item() * img.size(0)\n                _, preds = torch.max(outputs, 1)\n                test_correct += (preds == label).sum().item()\n                test_total += label.size(0)\n\n                all_preds.append(preds.cpu().numpy())\n                all_labels.append(label.cpu().numpy())\n\n        test_loss /= test_total\n        test_acc = test_correct / test_total\n\n        all_preds = np.concatenate(all_preds)\n        all_labels = np.concatenate(all_labels)\n\n        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%')\n\n        return test_acc, all_labels, all_preds\n\n    # Training method with LOSO cross-validation\n    def train_loso(self):\n        # Load data\n        self.X, self.y, self.subjects = self.get_source_data()\n\n        logo = LeaveOneGroupOut()\n        device = self.device\n\n        test_accuracies = []\n        subject_ids = []\n        fold_count = 0\n\n        print(\"\\n####################\")\n        print(\"Traininig Started\")\n        print(\"####################\")\n        \n        for train_idx, test_idx in logo.split(self.X, self.y, groups=self.subjects):\n            fold_count += 1\n            heldout_subj = self.subjects[test_idx[0]]\n            training_subjects = np.unique(self.subjects[train_idx])\n            # Convert to 1-based indexing for display\n            training_subjects_1based = [s + 1 for s in training_subjects]\n            test_subj_1based = heldout_subj + 1\n            print(f\"\\n===== Fold {fold_count}\")\n            print(f\"===== Seed : {42+fold_count}\")\n            print(f\"===== Training Subject : {', '.join(map(str, training_subjects_1based))}\")\n            print(f\"===== Test Subject : {test_subj_1based}\\n\")\n\n            # Re-seed per fold for reproducibility\n            set_seed(42 + fold_count)\n\n            X_train_full, X_test = self.X[train_idx], self.X[test_idx]\n            y_train_full, y_test = self.y[train_idx], self.y[test_idx]\n\n            # Further split training data into training and validation (e.g., 90-10)\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_full, y_train_full, test_size=0.1,\n                stratify=y_train_full, random_state=42 + fold_count\n            )\n\n            # Create datasets\n            train_ds = MultiSubjectBCIDataset(X_train, y_train, augment=True)\n            val_ds = MultiSubjectBCIDataset(X_val, y_val, augment=False)\n            test_ds = MultiSubjectBCIDataset(X_test, y_test, augment=False)\n\n            # Create dataloaders\n            train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n            test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\n\n            # Define optimizer and scheduler\n            optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=self.betas)\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                             factor=0.5, patience=5, verbose=True)\n\n            best_val_acc = 0.0\n            patience_counter = 0\n            best_model_state = None\n\n            for epoch in range(self.n_epochs):\n                # Training Phase\n                self.model.train()\n                running_loss = 0.0\n                correct = 0\n                total = 0\n\n                for img, label in train_loader:\n                    img = img.to(self.device)  # (B, 1, 22, 321)\n                    label = label.to(self.device)  # (B,)\n\n                    optimizer.zero_grad()\n                    outputs = self.model(img)  # (B, num_classes)\n                    loss = self.criterion_cls(outputs, label)\n                    loss.backward()\n                    optimizer.step()\n\n                    running_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    correct += (preds == label).sum().item()\n                    total += label.size(0)\n\n                train_loss = running_loss / total\n                train_acc = correct / total\n\n                # Validation Phase\n                self.model.eval()\n                val_loss = 0.0\n                val_correct = 0\n                val_total = 0\n\n                with torch.no_grad():\n                    for img, label in val_loader:\n                        img = img.to(self.device)\n                        label = label.to(self.device)\n\n                        outputs = self.model(img)\n                        loss = self.criterion_cls(outputs, label)\n\n                        val_loss += loss.item() * img.size(0)\n                        _, preds = torch.max(outputs, 1)\n                        val_correct += (preds == label).sum().item()\n                        val_total += label.size(0)\n\n                val_loss /= val_total\n                val_acc = val_correct / val_total\n\n                # Step scheduler\n                scheduler.step(val_loss)\n\n                print(f'Epoch {epoch+1}/{self.n_epochs} | '\n                      f'Training Loss : {train_loss:.4f} | Training Accuracy : {train_acc*100:.2f} % | '\n                      f'Validation Loss : {val_loss:.4f} | Validation Accuracy : {val_acc*100:.2f} %')\n\n                # Early Stopping\n                if val_acc > best_val_acc:\n                    best_val_acc = val_acc\n                    best_model_state = self.model.state_dict()\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    # if patience_counter >= self.patience:\n                    #     print(f\"\\n=== Early Stopping Triggered at Epoch {epoch+1}\\n\")\n                    #     break\n\n            # Load best model\n            if best_model_state is not None:\n                self.model.load_state_dict(best_model_state)\n\n            # Testing Phase\n            self.model.eval()\n            test_loss = 0.0\n            test_correct = 0\n            test_total = 0\n            all_preds = []\n            all_labels = []\n\n            with torch.no_grad():\n                for img, label in test_loader:\n                    img = img.to(self.device)\n                    label = label.to(self.device)\n\n                    outputs = self.model(img)\n                    loss = self.criterion_cls(outputs, label)\n\n                    test_loss += loss.item() * img.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    test_correct += (preds == label).sum().item()\n                    test_total += label.size(0)\n\n                    all_preds.append(preds.cpu().numpy())\n                    all_labels.append(label.cpu().numpy())\n\n            test_loss /= test_total\n            test_acc = test_correct / test_total\n\n            all_preds = np.concatenate(all_preds)\n            all_labels = np.concatenate(all_labels)\n\n            print(f'Test Subject : {test_subj_1based} | Test Loss : {test_loss:.4f} | Test Accuracy : {test_acc*100:.2f} %')\n            print(\"\\n################################\")\n\n            test_accuracies.append(test_acc)\n            subject_ids.append(heldout_subj)\n\n        # Summary of LOSO\n        avg_test_acc = np.mean(test_accuracies) * 100\n        \n        print(\"\\n================\")\n        print(\"LOSO Summary\")\n        print(\"================\")\n        for i, sid in enumerate(subject_ids):\n            print(f\"Subject {sid} -> Test Accuracy : {test_accuracies[i]*100:.2f} %\")\n        print(\"\\n-----------------------------\")\n        print(f\"Average Test Accuracy : {avg_test_acc:.2f} %\")\n        print(\"-----------------------------\")\n\n###############################################################################\n# 12) Main Execution\n###############################################################################\ndef main():\n    best = 0\n    aver = 0\n\n    # Path to the pickle file\n    pickle_path = '/kaggle/input/bcic-iv/BCIIV_2a.pkl'\n\n    # Initialize and train the model\n    exp = HybridExP(nsub=1, pickle_path=pickle_path, device='cuda:0')  # 'nsub' will be managed by LOSO\n    exp.train_loso()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:31:05.229876Z","iopub.execute_input":"2025-11-15T10:31:05.230222Z","iopub.status.idle":"2025-11-15T10:31:05.643112Z","shell.execute_reply.started":"2025-11-15T10:31:05.230199Z","shell.execute_reply":"2025-11-15T10:31:05.641959Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-c049800462d2>\u001b[0m in \u001b[0;36m<cell line: 802>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-45-c049800462d2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;31m# Initialize and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridExP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 'nsub' will be managed by LOSO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-45-c049800462d2>\u001b[0m in \u001b[0;36mtrain_loso\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_loso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mlogo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeaveOneGroupOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-45-c049800462d2>\u001b[0m in \u001b[0;36mget_source_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_source_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         all_X, all_y, all_subjects = load_and_preprocess_data(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mpickle_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mtarget_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-45-c049800462d2>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(pickle_path, target_h, target_w, l_freq, h_freq)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubj_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Apply band-pass filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (trials, channels, time_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (trials,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'filter'"],"ename":"AttributeError","evalue":"'numpy.ndarray' object has no attribute 'filter'","output_type":"error"}],"execution_count":45}]}